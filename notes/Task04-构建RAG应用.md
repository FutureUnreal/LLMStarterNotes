# ä¸€ã€LLMæ¥å…¥LangChain
[LangChainå®˜æ–¹æ–‡æ¡£](https://python.langchain.com/docs/get_started/introduction)

## å®é™…æ“ä½œ
1. **è°ƒç”¨ LLM**:
   - é€šè¿‡ LangChainï¼Œå¯ä»¥æ–¹ä¾¿åœ°è°ƒç”¨ LLM å¹¶é›†æˆåˆ°ä¸ªäººåº”ç”¨ä¸­ã€‚

2. **Prompt æ¨¡æ¿**:
   - åœ¨å¼€å‘å¤§æ¨¡å‹åº”ç”¨æ—¶ï¼Œé€šå¸¸ä¸ä¼šç›´æ¥å°†ç”¨æˆ·è¾“å…¥ä¼ é€’ç»™LLMã€‚è€Œæ˜¯é€šè¿‡æç¤ºæ¨¡æ¿æ·»åŠ æ›´å¤šä¸Šä¸‹æ–‡ï¼Œè¿™äº›æ¨¡æ¿è½¬åŒ–ç”¨æˆ·è¾“å…¥ä¸ºå®Œå…¨æ ¼å¼åŒ–çš„æç¤ºï¼Œè¾…åŠ©æ¨¡å‹æ›´å¥½åœ°ç†è§£å’Œå“åº”ã€‚

3. **è¾“å‡ºè§£æå™¨**:
   - OutputParsers è´Ÿè´£å°†è¯­è¨€æ¨¡å‹çš„è¾“å‡ºè½¬æ¢ä¸ºå¯ç”¨æ ¼å¼ï¼Œæ¯”å¦‚ JSON æˆ–å­—ç¬¦ä¸²ï¼Œè¿™å¯¹äºæ•´åˆæ¨¡å‹è¾“å‡ºåˆ°åº”ç”¨ä¸­éå¸¸å…³é”®ã€‚

  
# äºŒã€æ„å»ºæ£€ç´¢é—®ç­”é“¾
![](../resources/imgs/C4-RAG.png)

# ä¸‰ã€éƒ¨ç½²çŸ¥è¯†åº“åŠ©æ‰‹
[streamlitå®˜æ–¹æ–‡æ¡£](https://docs.streamlit.io/)

ä¿®æ”¹UIï¼Œéƒ¨ç½²æˆåŠŸï¼
![](../resources/imgs/C4-sreamlit.png)

ä»£ç 

```python
import streamlit as st
from langchain_openai import ChatOpenAI
import os
from langchain_core.output_parsers import StrOutputParser
from langchain.prompts import PromptTemplate
from langchain.chains import RetrievalQA
import sys
sys.path.append("../C3_æ­å»ºçŸ¥è¯†åº“")
__import__('pysqlite3')
sys.modules['sqlite3'] = sys.modules.pop('pysqlite3')
from zhipuai_embedding import ZhipuAIEmbeddings
from langchain.vectorstores.chroma import Chroma
from langchain.memory import ConversationBufferMemory
from langchain.chains import ConversationalRetrievalChain
from dotenv import load_dotenv, find_dotenv
_ = load_dotenv(find_dotenv())

openai_api_key = os.environ["OPENAI_API_KEY"]
openai_base_url = os.environ["OPENAI_BASE_URL"]

def generate_response(input_text, openai_api_key, openai_api_base):
    st.write(f"ç”Ÿæˆå›åº”ï¼šè¾“å…¥æ–‡æœ¬={input_text}, APIé”®={openai_api_key}, APIåŸºå€={openai_api_base}")  # è°ƒè¯•è¾“å‡º
    llm = ChatOpenAI(temperature=0.7, openai_api_key=openai_api_key, openai_api_base=openai_api_base)
    output = llm.invoke(input_text)
    st.write(f"API è°ƒç”¨ç»“æœ={output}")  # è°ƒè¯•è¾“å‡º
    output_parser = StrOutputParser()
    output = output_parser.invoke(output)
    st.write(f"è§£æåçš„è¾“å‡º={output}")  # è°ƒè¯•è¾“å‡º
    return output

def get_vectordb():
    st.write("è·å–å‘é‡æ•°æ®åº“")  # è°ƒè¯•è¾“å‡º
    embedding = ZhipuAIEmbeddings()
    persist_directory = '../C3_æ­å»ºçŸ¥è¯†åº“/data_base/vector_db/chroma'
    vectordb = Chroma(persist_directory=persist_directory, embedding_function=embedding)
    st.write(f"å·²åŠ è½½å‘é‡æ•°æ®åº“ï¼šæŒä¹…åŒ–ç›®å½•={persist_directory}")  # è°ƒè¯•è¾“å‡º
    return vectordb

def get_chat_qa_chain(question, openai_api_key, openai_api_base):
    st.write(f"è·å–å¸¦å†å²è®°å½•çš„é—®ç­”é“¾ï¼šé—®é¢˜={question}")  # è°ƒè¯•è¾“å‡º
    vectordb = get_vectordb()
    llm = ChatOpenAI(model_name="gpt-3.5-turbo", temperature=0, openai_api_key=openai_api_key, openai_api_base=openai_api_base)
    if 'memory' not in st.session_state:
        st.session_state.memory = ConversationBufferMemory(memory_key="chat_history", return_messages=True)
        print(1)
    retriever = vectordb.as_retriever()
    qa = ConversationalRetrievalChain.from_llm(llm, retriever=retriever, memory=st.session_state.memory)
    result = qa.invoke({"question": question})
    st.write(f"é—®ç­”é“¾ç»“æœ={result}")  # è°ƒè¯•è¾“å‡º
    return result['answer']

def get_qa_chain(question, openai_api_key, openai_api_base):
    st.write(f"è·å–é—®ç­”é“¾ï¼šé—®é¢˜={question}")  # è°ƒè¯•è¾“å‡º
    vectordb = get_vectordb()
    llm = ChatOpenAI(model_name="gpt-3.5-turbo", temperature=0, openai_api_key=openai_api_key, openai_api_base=openai_api_base)
    template = """ä½¿ç”¨ä»¥ä¸‹ä¸Šä¸‹æ–‡æ¥å›ç­”æœ€åçš„é—®é¢˜ã€‚å¦‚æœä½ ä¸çŸ¥é“ç­”æ¡ˆï¼Œå°±è¯´ä½ ä¸çŸ¥é“ï¼Œä¸è¦è¯•å›¾ç¼–é€ ç­”
        æ¡ˆã€‚æœ€å¤šä½¿ç”¨ä¸‰å¥è¯ã€‚å°½é‡ä½¿ç­”æ¡ˆç®€æ˜æ‰¼è¦ã€‚æ€»æ˜¯åœ¨å›ç­”çš„æœ€åè¯´â€œè°¢è°¢ä½ çš„æé—®ï¼â€ã€‚
        {context}
        é—®é¢˜: {question}
        """
    QA_CHAIN_PROMPT = PromptTemplate(input_variables=["context","question"],
                                 template=template)
    qa_chain = RetrievalQA.from_chain_type(llm,
                                       retriever=vectordb.as_retriever(),
                                       return_source_documents=True,
                                       chain_type_kwargs={"prompt":QA_CHAIN_PROMPT})
    result = qa_chain.invoke({"query": question})
    st.write(f"é—®ç­”é“¾è°ƒç”¨ç»“æœ={result}")  # è°ƒè¯•è¾“å‡º
    return result["result"]


def main():
    st.set_page_config(page_title="å¤§æ¨¡å‹åº”ç”¨å¼€å‘", layout="wide")
    st.title('ğŸ¦œğŸ”— åŠ¨æ‰‹å­¦å¤§æ¨¡å‹åº”ç”¨å¼€å‘')

    if 'messages' not in st.session_state:
        st.session_state.messages = []
        st.write("åˆå§‹åŒ–å¯¹è¯å†å²")  # è°ƒè¯•è¾“å‡º

    openai_api_key = st.sidebar.text_input('OpenAI API key', type='password')
    openai_base_url = st.sidebar.text_input('OpenAI base URL', type='password')
    selected_method = st.sidebar.radio(
        "é€‰æ‹©å¯¹è¯æ¨¡å¼",
        ["æ— ", "é—®ç­”é“¾", "å¸¦å†å²è®°å½•çš„é—®ç­”é“¾"],
        help="é€‰æ‹©æ‚¨å¸Œæœ›çš„é—®ç­”æ¨¡å¼ã€‚"
    )

    prompt = st.text_input("è¯·è¾“å…¥æ‚¨çš„é—®é¢˜ï¼š", key="user_input")
    st.write(f"å½“å‰è¾“å…¥: {prompt}")  # è°ƒè¯•è¾“å‡º

    if prompt:
        if not any(m['text'] == prompt for m in st.session_state.messages if m['role'] == 'user'):
            st.session_state.messages.append({"role": "user", "text": prompt})
            if selected_method == "æ— ":
                answer = generate_response(prompt, openai_api_key, openai_base_url)
            elif selected_method == "é—®ç­”é“¾":
                answer = get_qa_chain(prompt, openai_api_key, openai_base_url)
            elif selected_method == "å¸¦å†å²è®°å½•çš„é—®ç­”é“¾":
                answer = get_chat_qa_chain(prompt, openai_api_key, openai_base_url)

            if answer:
                st.session_state.messages.append({"role": "assistant", "text": answer})
        else:
            st.write("è¾“å…¥é‡å¤ï¼Œæœªæ·»åŠ åˆ°å†å²")  # è°ƒè¯•è¾“å‡º

    st.write("å½“å‰å¯¹è¯å†å²ï¼š")  # è°ƒè¯•è¾“å‡º
    for message in st.session_state.messages:
        st.write(f"{message['role']}: {message['text']}")  # è°ƒè¯•è¾“å‡º

    messages_container = st.container()
    with messages_container:
        for message in st.session_state.messages:
            if message["role"] == "user":
                st.info(f"ğŸ‘¤: {message['text']}")
            elif message["role"] == "assistant":
                st.success(f"ğŸ¤–: {message['text']}")

if __name__ == "__main__":
    main()

```