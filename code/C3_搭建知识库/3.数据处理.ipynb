{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 数据处理\n",
    "\n",
    "为构建我们的本地知识库，我们需要对以多种类型存储的本地文档进行处理，读取本地文档并通过前文描述的 Embedding 方法将本地文档的内容转化为词向量来构建向量数据库。在本节中，我们以一些实际示例入手，来讲解如何对本地文档进行处理。\n",
    "## 一、源文档选取\n",
    "我们选用 Datawhale 一些经典开源课程作为示例，具体包括：\n",
    "* [《机器学习公式详解》PDF版本](https://github.com/datawhalechina/pumpkin-book/releases)\n",
    "* [《面向开发者的LLM入门教程、第一部分Prompt Engineering》md版本](https://github.com/datawhalechina/llm-cookbook)  \n",
    "我们将知识库源数据放置在../../data 目录下。\n",
    "## 二、数据读取\n",
    "### 1. PDF 文档\n",
    "我们可以使用 LangChain 的 PyMuPDFLoader 来读取知识库的 PDF 文件。PyMuPDFLoader 是 PDF 解析器中速度最快的一种，结果会包含 PDF 及其页面的详细元数据，并且每页返回一个文档。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders.pdf import PyMuPDFLoader\n",
    "\n",
    "# 创建一个 PyMuPDFLoader Class 实例，输入为待加载的 pdf 文档路径\n",
    "loader = PyMuPDFLoader(\"../../resources/data/pumpkin_book.pdf\")\n",
    "\n",
    "# 调用 PyMuPDFLoader Class 的函数 load 对 pdf 文件进行加载\n",
    "pdf_pages = loader.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "文档加载后储存在 `pages` 变量中:\n",
    "- `page` 的变量类型为 `List`\n",
    "- 打印 `pages` 的长度可以看到 pdf 一共包含多少页"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "载入后的变量类型为：<class 'list'>， 该 PDF 一共包含 196 页\n"
     ]
    }
   ],
   "source": [
    "print(f\"载入后的变量类型为：{type(pdf_pages)}，\",  f\"该 PDF 一共包含 {len(pdf_pages)} 页\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`page` 中的每一元素为一个文档，变量类型为 `langchain_core.documents.base.Document`, 文档变量类型包含两个属性\n",
    "- `page_content` 包含该文档的内容。\n",
    "- `meta_data` 为文档相关的描述性数据。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "每一个元素的类型：<class 'langchain_core.documents.base.Document'>.\n",
      "------\n",
      "该文档的描述性数据：{'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 1, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}\n",
      "------\n",
      "查看该文档的内容:\n",
      "前言\n",
      "“周志华老师的《机器学习》\n",
      "（西瓜书）是机器学习领域的经典入门教材之一，周老师为了使尽可能多的读\n",
      "者通过西瓜书对机器学习有所了解, 所以在书中对部分公式的推导细节没有详述，但是这对那些想深究公式推\n",
      "导细节的读者来说可能“不太友好”\n",
      "，本书旨在对西瓜书里比较难理解的公式加以解析，以及对部分公式补充\n",
      "具体的推导细节。\n",
      "”\n",
      "读到这里，大家可能会疑问为啥前面这段话加了引号，因为这只是我们最初的遐想，后来我们了解到，周\n",
      "老师之所以省去这些推导细节的真实原因是，他本尊认为“理工科数学基础扎实点的大二下学生应该对西瓜书\n",
      "中的推导细节无困难吧，要点在书里都有了，略去的细节应能脑补或做练习”\n",
      "。所以...... 本南瓜书只能算是我\n",
      "等数学渣渣在自学的时候记下来的笔记，希望能够帮助大家都成为一名合格的“理工科数学基础扎实点的大二\n",
      "下学生”\n",
      "。\n",
      "使用说明\n",
      "• 南瓜书的所有内容都是以西瓜书的内容为前置知识进行表述的，所以南瓜书的最佳使用方法是以西瓜书\n",
      "为主线，遇到自己推导不出来或者看不懂的公式时再来查阅南瓜书；\n",
      "• 对于初学机器学习的小白，西瓜书第1 章和第2 章的公式强烈不建议深究，简单过一下即可，等你学得\n",
      "有点飘的时候再回来啃都来得及；\n",
      "• 每个公式的解析和推导我们都力(zhi) 争(neng) 以本科数学基础的视角进行讲解，所以超纲的数学知识\n",
      "我们通常都会以附录和参考文献的形式给出，感兴趣的同学可以继续沿着我们给的资料进行深入学习；\n",
      "• 若南瓜书里没有你想要查阅的公式，\n",
      "或者你发现南瓜书哪个地方有错误，\n",
      "请毫不犹豫地去我们GitHub 的\n",
      "Issues（地址：https://github.com/datawhalechina/pumpkin-book/issues）进行反馈，在对应版块\n",
      "提交你希望补充的公式编号或者勘误信息，我们通常会在24 小时以内给您回复，超过24 小时未回复的\n",
      "话可以微信联系我们（微信号：at-Sm1les）\n",
      "；\n",
      "配套视频教程：https://www.bilibili.com/video/BV1Mh411e7VU\n",
      "在线阅读地址：https://datawhalechina.github.io/pumpkin-book（仅供第1 版）\n",
      "最新版PDF 获取地址：https://github.com/datawhalechina/pumpkin-book/releases\n",
      "编委会\n",
      "主编：Sm1les、archwalker、jbb0523\n",
      "编委：juxiao、Majingmin、MrBigFan、shanry、Ye980226\n",
      "封面设计：构思-Sm1les、创作-林王茂盛\n",
      "致谢\n",
      "特别感谢awyd234、\n",
      "feijuan、\n",
      "Ggmatch、\n",
      "Heitao5200、\n",
      "huaqing89、\n",
      "LongJH、\n",
      "LilRachel、\n",
      "LeoLRH、\n",
      "Nono17、\n",
      "spareribs、sunchaothu、StevenLzq 在最早期的时候对南瓜书所做的贡献。\n",
      "扫描下方二维码，然后回复关键词“南瓜书”\n",
      "，即可加入“南瓜书读者交流群”\n",
      "版权声明\n",
      "本作品采用知识共享署名-非商业性使用-相同方式共享4.0 国际许可协议进行许可。\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pdf_page = pdf_pages[1]\n",
    "print(f\"每一个元素的类型：{type(pdf_page)}.\", \n",
    "    f\"该文档的描述性数据：{pdf_page.metadata}\", \n",
    "    f\"查看该文档的内容:\\n{pdf_page.page_content}\", \n",
    "    sep=\"\\n------\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. MD 文档\n",
    "我们可以以几乎完全一致的方式读入 markdown 文档："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders.markdown import UnstructuredMarkdownLoader\n",
    "\n",
    "loader = UnstructuredMarkdownLoader(\"../../notes/Task01-大模型简介.md\")\n",
    "md_pages = loader.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "读取的对象和 PDF 文档读取出来是完全一致的："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "载入后的变量类型为：<class 'list'>， 该 Markdown 一共包含 1 页\n"
     ]
    }
   ],
   "source": [
    "print(f\"载入后的变量类型为：{type(md_pages)}，\",  f\"该 Markdown 一共包含 {len(md_pages)} 页\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "每一个元素的类型：<class 'langchain_core.documents.base.Document'>.\n",
      "------\n",
      "该文档的描述性数据：{'source': '../../notes/Task01-大模型简介.md'}\n",
      "------\n",
      "查看该文档的内容:\n",
      "一、大型语言模型（LLM）理论简介\n",
      "\n",
      "大型语言模型（LLM）是旨在理解和生成人类语言的人工智能模型。它们包含数百亿（或更多）参数，并在海量文本数据上进行训练，以深层次地理解语言。知名的 LLM 包括 GPT-3.5、GPT-4、PaLM、Claude 和 LLaMA 等。这些模型展现了在解决复杂任务时的惊人潜力，尤其是 ChatGPT 展示出了与人类交流的自然流畅性。\n",
      "\n",
      "什么是大型语言模型（LLM\n"
     ]
    }
   ],
   "source": [
    "md_page = md_pages[0]\n",
    "print(f\"每一个元素的类型：{type(md_page)}.\", \n",
    "    f\"该文档的描述性数据：{md_page.metadata}\", \n",
    "    f\"查看该文档的内容:\\n{md_page.page_content[0:][:200]}\", \n",
    "    sep=\"\\n------\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 三、数据清洗\n",
    "我们期望知识库的数据尽量是有序的、优质的、精简的，因此我们要删除低质量的、甚至影响理解的文本数据。  \n",
    "可以看到上文中读取的pdf文件不仅将一句话按照原文的分行添加了换行符`\\n`，也在原本两个符号中间插入了`\\n`，我们可以使用正则表达式匹配并删除掉`\\n`。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "前言\n",
      "“周志华老师的《机器学习》（西瓜书）是机器学习领域的经典入门教材之一，周老师为了使尽可能多的读\n",
      "者通过西瓜书对机器学习有所了解, 所以在书中对部分公式的推导细节没有详述，但是这对那些想深究公式推\n",
      "导细节的读者来说可能“不太友好”，本书旨在对西瓜书里比较难理解的公式加以解析，以及对部分公式补充\n",
      "具体的推导细节。”\n",
      "读到这里，大家可能会疑问为啥前面这段话加了引号，因为这只是我们最初的遐想，后来我们了解到，周\n",
      "老师之所以省去这些推导细节的真实原因是，他本尊认为“理工科数学基础扎实点的大二下学生应该对西瓜书\n",
      "中的推导细节无困难吧，要点在书里都有了，略去的细节应能脑补或做练习”。所以...... 本南瓜书只能算是我\n",
      "等数学渣渣在自学的时候记下来的笔记，希望能够帮助大家都成为一名合格的“理工科数学基础扎实点的大二\n",
      "下学生”。\n",
      "使用说明\n",
      "• 南瓜书的所有内容都是以西瓜书的内容为前置知识进行表述的，所以南瓜书的最佳使用方法是以西瓜书\n",
      "为主线，遇到自己推导不出来或者看不懂的公式时再来查阅南瓜书；• 对于初学机器学习的小白，西瓜书第1 章和第2 章的公式强烈不建议深究，简单过一下即可，等你学得\n",
      "有点飘的时候再回来啃都来得及；• 每个公式的解析和推导我们都力(zhi) 争(neng) 以本科数学基础的视角进行讲解，所以超纲的数学知识\n",
      "我们通常都会以附录和参考文献的形式给出，感兴趣的同学可以继续沿着我们给的资料进行深入学习；• 若南瓜书里没有你想要查阅的公式，\n",
      "或者你发现南瓜书哪个地方有错误，\n",
      "请毫不犹豫地去我们GitHub 的\n",
      "Issues（地址：https://github.com/datawhalechina/pumpkin-book/issues）进行反馈，在对应版块\n",
      "提交你希望补充的公式编号或者勘误信息，我们通常会在24 小时以内给您回复，超过24 小时未回复的\n",
      "话可以微信联系我们（微信号：at-Sm1les）；\n",
      "配套视频教程：https://www.bilibili.com/video/BV1Mh411e7VU\n",
      "在线阅读地址：https://datawhalechina.github.io/pumpkin-book（仅供第1 版）\n",
      "最新版PDF 获取地址：https://github.com/datawhalechina/pumpkin-book/releases\n",
      "编委会\n",
      "主编：Sm1les、archwalker、jbb0523\n",
      "编委：juxiao、Majingmin、MrBigFan、shanry、Ye980226\n",
      "封面设计：构思-Sm1les、创作-林王茂盛\n",
      "致谢\n",
      "特别感谢awyd234、feijuan、Ggmatch、Heitao5200、huaqing89、LongJH、LilRachel、LeoLRH、Nono17、spareribs、sunchaothu、StevenLzq 在最早期的时候对南瓜书所做的贡献。\n",
      "扫描下方二维码，然后回复关键词“南瓜书”，即可加入“南瓜书读者交流群”\n",
      "版权声明\n",
      "本作品采用知识共享署名-非商业性使用-相同方式共享4.0 国际许可协议进行许可。\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "pattern = re.compile(r'[^\\u4e00-\\u9fff](\\n)[^\\u4e00-\\u9fff]', re.DOTALL)\n",
    "pdf_page.page_content = re.sub(pattern, lambda match: match.group(0).replace('\\n', ''), pdf_page.page_content)\n",
    "print(pdf_page.page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "进一步分析数据，我们发现数据中还有不少的`•`和空格，我们的简单实用replace方法即可。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "前言\n",
      "“周志华老师的《机器学习》（西瓜书）是机器学习领域的经典入门教材之一，周老师为了使尽可能多的读\n",
      "者通过西瓜书对机器学习有所了解,所以在书中对部分公式的推导细节没有详述，但是这对那些想深究公式推\n",
      "导细节的读者来说可能“不太友好”，本书旨在对西瓜书里比较难理解的公式加以解析，以及对部分公式补充\n",
      "具体的推导细节。”\n",
      "读到这里，大家可能会疑问为啥前面这段话加了引号，因为这只是我们最初的遐想，后来我们了解到，周\n",
      "老师之所以省去这些推导细节的真实原因是，他本尊认为“理工科数学基础扎实点的大二下学生应该对西瓜书\n",
      "中的推导细节无困难吧，要点在书里都有了，略去的细节应能脑补或做练习”。所以......本南瓜书只能算是我\n",
      "等数学渣渣在自学的时候记下来的笔记，希望能够帮助大家都成为一名合格的“理工科数学基础扎实点的大二\n",
      "下学生”。\n",
      "使用说明\n",
      "南瓜书的所有内容都是以西瓜书的内容为前置知识进行表述的，所以南瓜书的最佳使用方法是以西瓜书\n",
      "为主线，遇到自己推导不出来或者看不懂的公式时再来查阅南瓜书；对于初学机器学习的小白，西瓜书第1章和第2章的公式强烈不建议深究，简单过一下即可，等你学得\n",
      "有点飘的时候再回来啃都来得及；每个公式的解析和推导我们都力(zhi)争(neng)以本科数学基础的视角进行讲解，所以超纲的数学知识\n",
      "我们通常都会以附录和参考文献的形式给出，感兴趣的同学可以继续沿着我们给的资料进行深入学习；若南瓜书里没有你想要查阅的公式，\n",
      "或者你发现南瓜书哪个地方有错误，\n",
      "请毫不犹豫地去我们GitHub的\n",
      "Issues（地址：https://github.com/datawhalechina/pumpkin-book/issues）进行反馈，在对应版块\n",
      "提交你希望补充的公式编号或者勘误信息，我们通常会在24小时以内给您回复，超过24小时未回复的\n",
      "话可以微信联系我们（微信号：at-Sm1les）；\n",
      "配套视频教程：https://www.bilibili.com/video/BV1Mh411e7VU\n",
      "在线阅读地址：https://datawhalechina.github.io/pumpkin-book（仅供第1版）\n",
      "最新版PDF获取地址：https://github.com/datawhalechina/pumpkin-book/releases\n",
      "编委会\n",
      "主编：Sm1les、archwalker、jbb0523\n",
      "编委：juxiao、Majingmin、MrBigFan、shanry、Ye980226\n",
      "封面设计：构思-Sm1les、创作-林王茂盛\n",
      "致谢\n",
      "特别感谢awyd234、feijuan、Ggmatch、Heitao5200、huaqing89、LongJH、LilRachel、LeoLRH、Nono17、spareribs、sunchaothu、StevenLzq在最早期的时候对南瓜书所做的贡献。\n",
      "扫描下方二维码，然后回复关键词“南瓜书”，即可加入“南瓜书读者交流群”\n",
      "版权声明\n",
      "本作品采用知识共享署名-非商业性使用-相同方式共享4.0国际许可协议进行许可。\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pdf_page.page_content = pdf_page.page_content.replace('•', '')\n",
    "pdf_page.page_content = pdf_page.page_content.replace(' ', '')\n",
    "print(pdf_page.page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "上文中读取的md文件每一段中间隔了一个换行符，我们同样可以使用replace方法去除。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "一、大型语言模型（LLM）理论简介\n",
      "大型语言模型（LLM）是旨在理解和生成人类语言的人工智能模型。它们包含数百亿（或更多）参数，并在海量文本数据上进行训练，以深层次地理解语言。知名的 LLM 包括 GPT-3.5、GPT-4、PaLM、Claude 和 LLaMA 等。这些模型展现了在解决复杂任务时的惊人潜力，尤其是 ChatGPT 展示出了与人类交流的自然流畅性。\n",
      "什么是大型语言模型（LLM）\n",
      "概念\n",
      "LLM 通常指拥有数百亿参数的语言模型。\n",
      "在大量文本数据上训练，以获得对语言的深层理解。\n",
      "发展历程\n",
      "研究可追溯至 20 世纪 90 年代，最初集中在统计学习方法。\n",
      "2003 年，深度学习被引入语言模型，标志性的是 Bengio 的论文。\n",
      "Transformer 架构的崛起（2018 年左右）极大提升了模型性能。\n",
      "模型规模的扩大展现了惊人的“涌现能力”。\n",
      "常见模型\n",
      "GPT 系列：由 OpenAI 提出，特点是通过语言建模压缩世界知识。\n",
      "Claude 系列：由 Anthropic 公司开发，致力于提升模型的理解和生成能力。\n",
      "PaLM/Gemini 系列：由 Google 开发，PaLM 是早期模型，Gemini 是后续升级版本。\n",
      "文心一言：基于百度文心大模型，中文能力强。\n",
      "星火大模型：科大讯飞发布，支持多种自然语言处理任务。\n",
      "LLaMA 系列：Meta 开源的基础语言模型，强调使用公开数据集训练。\n",
      "通义千问：阿里巴巴基于“通义”大模型研发的开源模型。\n",
      "（该图来源于Awesome-Multimodal-Large-Language-Models）\n",
      "LLM 的能力与特点\n",
      "涌现能力：大型模型在解决复杂任务时表现出的潜力。\n",
      "对话式应用：如 ChatGPT，展现出自然和流畅的交流能力。\n",
      "多模态理解：如 GPT-4，扩展到理解多种形式的输入。\n",
      "安全性和可靠性：通过迭代训练和安全奖励信号提高响应的安全性。\n",
      "LLM 的应用与影响\n",
      "知识探索与生成：通过深层理解语言，LLM 能生成内容，解决问题。\n",
      "自然语言交互：提升人机交互的自然度和流畅性。\n",
      "教育和研究：辅助学习和研究，提供丰富的信息和解答。\n",
      "商业应用：从文本生成到客户服务，LLM 正在变革多个行业。\n",
      "问题&思考❓\n",
      "大型语言模型（LLM）作为概率模型是否能实现通用人工智能（AGI）？\n",
      "AGI 的定义和追求：AGI 旨在创建能够在广泛认知任务上与人类相当或更优的人工智能系统。目前，多个研究机构和公司正在朝这一目标努力。Wikipedia. Artificial general intelligence\n",
      "涌现能力：一个复杂系统（如AGI）可能在微观层面上充满不确定性和概率性，但在宏观层面上展现出明确的智能和逻辑。LLM在处理统计归纳和因果推理时展现出的涌现能力，展现了量变到质变的过程，以及在足够复杂度和数据量下，模型可能会“顿悟”出超越其训练数据的本质规律。\n",
      "大型语言模型与智能系统特征：智能行为有许多特征。例如，理解世界的能力、理解物理世界的能力、记忆和检索事物的能力、持久性记忆、推理能力和计划能力。这是智能系统或实体、人类、动物的四个基本特征。LLM 无法做到这些，或者只能以非常原始的方式做到这些，而且并不真正了解物理界。LLM 并没有真正的持久记忆，无法真正推理，当然也无法计划。因此，如果你期望系统变得智能，但却无法做这些事情，那么你就犯了一个错误。这并不是说自回归 LLM 没有用。它们当然有用，但它们并不有趣，我们无法围绕它们构建整个应用程序生态系统。但作为迈向人类水平智能的通行证，它们缺少必要的组成部分。\n",
      "通过感官输入，我们看到的信息比通过语言看到的信息多得多，尽管我们有直觉，但我们学到的大部分内容和知识都是通过我们的观察和与现实世界的互动，而不是通过通过语言。我们在生命最初几年学到的一切，当然还有动物学到的一切，都与语言无关。\n",
      "Yann Lecun: Meta AI, Open Source, Limits of LLMs, AGI & the Future of AI | Lex Fridman Podcast\n",
      "多语言能力：对于同样的问题，如果 LLM 的前置回复显示其已经完全理解问题，会发现使用英文提问时其给出的回复较优。而如果是人类理解了一个问题后，那么不管提问的语言是什么，都会给出类似的回复。当然也不排除这是因为目前 LLM 的参数数量无法达到人脑水平。\n",
      "二、检索增强生成（RAG）简介\n",
      "三、LangChain简介\n",
      "利用 LangChain 框架，可以轻松地构建如下所示的 RAG 应用（图片来源）\n",
      "四、开发 LLM 应用的整体流程\n",
      "1. 大模型开发简介\n",
      "大模型开发是指开发以大语言模型(LLM)为功能核心的应用，通过大语言模型的理解能力和生成能力，结合特定的数据或业务逻辑来提供独特功能。技术核心点在于通过调用API或开源模型实现理解与生成，通过Prompt Engineering实现大语言模型的控制。\n",
      "大模型开发要素\n",
      "指令遵循与文本生成： 提供了替代复杂业务逻辑的简单平替方案。\n",
      "Prompt Engineering： 用于替代传统AI开发中的子模型训练，简化业务逻辑实现。\n",
      "评估思路： 大模型开发流程更灵活，从业务需求出发构造小批量验证集，设计合理Prompt，进行优化。\n",
      "2. 大模型开发的一般流程\n",
      "确定目标： 明确开发目标，包括应用场景、目标人群、核心价值。\n",
      "设计功能： 设计应用所提供的功能及每个功能的实现逻辑。\n",
      "搭建整体架构： 基于特定数据库+Prompt+通用大模型的架构，实现从用户输入到应用输出的全流程贯通。\n",
      "搭建数据库： 构建个性化数据库以支持大模型应用，包括数据收集、预处理、向量化存储等。\n",
      "Prompt Engineering： 构建优质的Prompt，逐步迭代优化以提升应用性能。\n",
      "验证迭代： 通过不断发现Bad Case并优化Prompt Engineering来提升系统效果。\n",
      "前后端搭建： 完成核心功能后，进行前后端开发，设计产品页面。\n",
      "体验优化： 应用上线后进行用户体验跟踪，记录反馈并进行优化。\n",
      "3. 搭建 LLM 项目的流程简析（以知识库助手为例）\n",
      "项目规划与需求分析： 明确项目目标、核心功能、技术架构等。\n",
      "数据准备与向量知识库构建： 包括文档收集、向量化、知识库索引建立等。\n",
      "大模型集成与API连接： 集成不同大模型，配置API。\n",
      "核心功能实现： 构建Prompt Engineering，实现大模型回答功能。\n",
      "核心功能迭代优化： 收集Bad Case，迭代优化核心功能。\n",
      "前端与用户交互界面开发： 使用Gradio和Streamlit搭建前端界面，设计用户界面。\n",
      "部署测试与上线： 部署应用到服务器或云平台，确保稳定性，进行上线。\n",
      "维护与持续改进： 监测系统性能和用户反馈，定期更新知识库和系统改进。\n",
      "五、开发环境配置(已提交至课程项目)\n",
      "参考链接\n"
     ]
    }
   ],
   "source": [
    "md_page.page_content = md_page.page_content.replace('\\n\\n', '\\n')\n",
    "print(md_page.page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 四、文档分割\n",
    "\n",
    "由于单个文档的长度往往会超过模型支持的上下文，导致检索得到的知识太长超出模型的处理能力，因此，在构建向量知识库的过程中，我们往往需要对文档进行分割，将单个文档按长度或者按固定的规则分割成若干个 chunk，然后将每个 chunk 转化为词向量，存储到向量数据库中。\n",
    "\n",
    "在检索时，我们会以 chunk 作为检索的元单位，也就是每一次检索到 k 个 chunk 作为模型可以参考来回答用户问题的知识，这个 k 是我们可以自由设定的。\n",
    "\n",
    "Langchain 中文本分割器都根据 `chunk_size` (块大小)和 `chunk_overlap` (块与块之间的重叠大小)进行分割。\n",
    "\n",
    "![image.png](../figures/C3-3-example-splitter.png)\n",
    "\n",
    "* chunk_size 指每个块包含的字符或 Token （如单词、句子等）的数量\n",
    "\n",
    "* chunk_overlap 指两个块之间共享的字符数量，用于保持上下文的连贯性，避免分割丢失上下文信息"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Langchain 提供多种文档分割方式，区别在怎么确定块与块之间的边界、块由哪些字符/token组成、以及如何测量块大小\n",
    "\n",
    "- RecursiveCharacterTextSplitter(): 按字符串分割文本，递归地尝试按不同的分隔符进行分割文本。\n",
    "- CharacterTextSplitter(): 按字符来分割文本。\n",
    "- MarkdownHeaderTextSplitter(): 基于指定的标题来分割markdown 文件。\n",
    "- TokenTextSplitter(): 按token来分割文本。\n",
    "- SentenceTransformersTokenTextSplitter(): 按token来分割文本\n",
    "- Language(): 用于 CPP、Python、Ruby、Markdown 等。\n",
    "- NLTKTextSplitter(): 使用 NLTK（自然语言工具包）按句子分割文本。\n",
    "- SpacyTextSplitter(): 使用 Spacy按句子的切割文本。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' \n",
    "* RecursiveCharacterTextSplitter 递归字符文本分割\n",
    "RecursiveCharacterTextSplitter 将按不同的字符递归地分割(按照这个优先级[\"\\n\\n\", \"\\n\", \" \", \"\"])，\n",
    "    这样就能尽量把所有和语义相关的内容尽可能长时间地保留在同一位置\n",
    "RecursiveCharacterTextSplitter需要关注的是4个参数：\n",
    "\n",
    "* separators - 分隔符字符串数组\n",
    "* chunk_size - 每个文档的字符数量限制\n",
    "* chunk_overlap - 两份文档重叠区域的长度\n",
    "* length_function - 长度计算函数\n",
    "'''\n",
    "#导入文本分割器\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 知识库中单段文本长度\n",
    "CHUNK_SIZE = 500\n",
    "\n",
    "# 知识库中相邻文本重合长度\n",
    "OVERLAP_SIZE = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['前言\\n“周志华老师的《机器学习》（西瓜书）是机器学习领域的经典入门教材之一，周老师为了使尽可能多的读\\n者通过西瓜书对机器学习有所了解,所以在书中对部分公式的推导细节没有详述，但是这对那些想深究公式推\\n导细节的读者来说可能“不太友好”，本书旨在对西瓜书里比较难理解的公式加以解析，以及对部分公式补充\\n具体的推导细节。”\\n读到这里，大家可能会疑问为啥前面这段话加了引号，因为这只是我们最初的遐想，后来我们了解到，周\\n老师之所以省去这些推导细节的真实原因是，他本尊认为“理工科数学基础扎实点的大二下学生应该对西瓜书\\n中的推导细节无困难吧，要点在书里都有了，略去的细节应能脑补或做练习”。所以......本南瓜书只能算是我\\n等数学渣渣在自学的时候记下来的笔记，希望能够帮助大家都成为一名合格的“理工科数学基础扎实点的大二\\n下学生”。\\n使用说明\\n南瓜书的所有内容都是以西瓜书的内容为前置知识进行表述的，所以南瓜书的最佳使用方法是以西瓜书\\n为主线，遇到自己推导不出来或者看不懂的公式时再来查阅南瓜书；对于初学机器学习的小白，西瓜书第1章和第2章的公式强烈不建议深究，简单过一下即可，等你学得',\n",
       " '有点飘的时候再回来啃都来得及；每个公式的解析和推导我们都力(zhi)争(neng)以本科数学基础的视角进行讲解，所以超纲的数学知识\\n我们通常都会以附录和参考文献的形式给出，感兴趣的同学可以继续沿着我们给的资料进行深入学习；若南瓜书里没有你想要查阅的公式，\\n或者你发现南瓜书哪个地方有错误，\\n请毫不犹豫地去我们GitHub的\\nIssues（地址：https://github.com/datawhalechina/pumpkin-book/issues）进行反馈，在对应版块\\n提交你希望补充的公式编号或者勘误信息，我们通常会在24小时以内给您回复，超过24小时未回复的\\n话可以微信联系我们（微信号：at-Sm1les）；\\n配套视频教程：https://www.bilibili.com/video/BV1Mh411e7VU\\n在线阅读地址：https://datawhalechina.github.io/pumpkin-book（仅供第1版）\\n最新版PDF获取地址：https://github.com/datawhalechina/pumpkin-book/releases\\n编委会',\n",
       " '编委会\\n主编：Sm1les、archwalk']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 使用递归字符文本分割器\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=CHUNK_SIZE,\n",
    "    chunk_overlap=OVERLAP_SIZE\n",
    ")\n",
    "text_splitter.split_text(pdf_page.page_content[0:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "切分后的文件数量：720\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Document(page_content='\\x01本\\x03:2.0.0\\n发布日期:2023.11\\n南  ⽠  书\\nPUMPKIN\\nB  O  O  K\\n谢\\t睿 \\x0b州 贾彬彬', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 0, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='前言\\n“周志华老师的《机器学习》（西瓜书）是机器学习领域的经典入门教材之一，周老师为了使尽可能多的读\\n者通过西瓜书对机器学习有所了解,所以在书中对部分公式的推导细节没有详述，但是这对那些想深究公式推\\n导细节的读者来说可能“不太友好”，本书旨在对西瓜书里比较难理解的公式加以解析，以及对部分公式补充\\n具体的推导细节。”\\n读到这里，大家可能会疑问为啥前面这段话加了引号，因为这只是我们最初的遐想，后来我们了解到，周\\n老师之所以省去这些推导细节的真实原因是，他本尊认为“理工科数学基础扎实点的大二下学生应该对西瓜书\\n中的推导细节无困难吧，要点在书里都有了，略去的细节应能脑补或做练习”。所以......本南瓜书只能算是我\\n等数学渣渣在自学的时候记下来的笔记，希望能够帮助大家都成为一名合格的“理工科数学基础扎实点的大二\\n下学生”。\\n使用说明\\n南瓜书的所有内容都是以西瓜书的内容为前置知识进行表述的，所以南瓜书的最佳使用方法是以西瓜书\\n为主线，遇到自己推导不出来或者看不懂的公式时再来查阅南瓜书；对于初学机器学习的小白，西瓜书第1章和第2章的公式强烈不建议深究，简单过一下即可，等你学得', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 1, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='有点飘的时候再回来啃都来得及；每个公式的解析和推导我们都力(zhi)争(neng)以本科数学基础的视角进行讲解，所以超纲的数学知识\\n我们通常都会以附录和参考文献的形式给出，感兴趣的同学可以继续沿着我们给的资料进行深入学习；若南瓜书里没有你想要查阅的公式，\\n或者你发现南瓜书哪个地方有错误，\\n请毫不犹豫地去我们GitHub的\\nIssues（地址：https://github.com/datawhalechina/pumpkin-book/issues）进行反馈，在对应版块\\n提交你希望补充的公式编号或者勘误信息，我们通常会在24小时以内给您回复，超过24小时未回复的\\n话可以微信联系我们（微信号：at-Sm1les）；\\n配套视频教程：https://www.bilibili.com/video/BV1Mh411e7VU\\n在线阅读地址：https://datawhalechina.github.io/pumpkin-book（仅供第1版）\\n最新版PDF获取地址：https://github.com/datawhalechina/pumpkin-book/releases\\n编委会', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 1, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='编委会\\n主编：Sm1les、archwalker、jbb0523\\n编委：juxiao、Majingmin、MrBigFan、shanry、Ye980226\\n封面设计：构思-Sm1les、创作-林王茂盛\\n致谢\\n特别感谢awyd234、feijuan、Ggmatch、Heitao5200、huaqing89、LongJH、LilRachel、LeoLRH、Nono17、spareribs、sunchaothu、StevenLzq在最早期的时候对南瓜书所做的贡献。\\n扫描下方二维码，然后回复关键词“南瓜书”，即可加入“南瓜书读者交流群”\\n版权声明\\n本作品采用知识共享署名-非商业性使用-相同方式共享4.0国际许可协议进行许可。', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 1, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='→_→\\n欢迎去各大电商平台选购纸质版南瓜书《机器学习公式详解第2 版》\\n←_←\\n目录\\n第1 章绪论\\n1\\n1.1\\n引言. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n1\\n1.2\\n基本术语\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n1\\n1.3\\n假设空间\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n3\\n1.4\\n归纳偏好\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n3\\n1.4.1', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 2, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='3\\n1.4.1\\n式(1.1) 和式(1.2) 的解释. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n4\\n第2 章模型评估与选择\\n5\\n2.1\\n经验误差与过拟合\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n5\\n2.2\\n评估方法\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n5\\n2.2.1\\n算法参数（超参数）与模型参数. . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n6\\n2.2.2\\n验证集. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n6\\n2.3\\n性能度量', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 2, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='6\\n2.3\\n性能度量\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n6\\n2.3.1\\n式(2.2) 到式(2.7) 的解释. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n6\\n2.3.2\\n式(2.8) 和式(2.9) 的解释. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n6\\n2.3.3\\n图2.3 的解释. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n6\\n2.3.4\\n式(2.10) 的推导\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n7\\n2.3.5\\n式(2.11) 的解释', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 2, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='7\\n2.3.5\\n式(2.11) 的解释\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n7\\n2.3.6\\n式(2.12) 到式(2.17) 的解释. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n7\\n2.3.7\\n式(2.18) 和式(2.19) 的解释. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n8\\n2.3.8\\n式(2.20) 的推导\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n8\\n2.3.9\\n式(2.21) 和式(2.22) 的推导. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n9\\n2.3.10 式(2.23) 的解释', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 2, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='9\\n2.3.10 式(2.23) 的解释\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n10\\n2.3.11 式(2.24) 的解释\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n11\\n2.3.12 式(2.25) 的解释\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n12\\n2.4\\n比较检验\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n13\\n2.4.1\\n式(2.26) 的解释\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n13\\n2.4.2', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 2, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='13\\n2.4.2\\n式(2.27) 的推导\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n14\\n2.5\\n偏差与方差. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n15\\n2.5.1\\n式(2.37) 到式(2.42) 的推导. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n15\\n第3 章线性模型\\n18\\n3.1\\n基本形式\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n18\\n3.2\\n线性回归', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 2, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='18\\n3.2\\n线性回归\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n18\\n3.2.1\\n属性数值化. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n18\\n3.2.2\\n式(3.4) 的解释. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n18\\n3.2.3\\n式(3.5) 的推导. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n19\\n3.2.4\\n式(3.6) 的推导. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n19\\n3.2.5', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 2, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='19\\n3.2.5\\n式(3.7) 的推导. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n19\\n3.2.6\\n式(3.9) 的推导. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n21\\n3.2.7\\n式(3.10) 的推导\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n21\\n3.2.8\\n式(3.11) 的推导\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n22\\n3.3\\n对数几率回归. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n23\\n→_→', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 2, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='23\\n→_→\\n配套视频教程：https://www.bilibili.com/video/BV1Mh411e7VU\\n←_←', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 2, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='→_→\\n欢迎去各大电商平台选购纸质版南瓜书《机器学习公式详解第2 版》\\n←_←\\n3.3.1\\n式(3.27) 的推导\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n23\\n3.3.2\\n梯度下降法. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n24\\n3.3.3\\n牛顿法. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n25\\n3.3.4\\n式(3.29) 的解释\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n26\\n3.3.5\\n式(3.30) 的推导', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 3, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='26\\n3.3.5\\n式(3.30) 的推导\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n26\\n3.3.6\\n式(3.31) 的推导\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n27\\n3.4\\n线性判别分析. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n27\\n3.4.1\\n式(3.32) 的推导\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n28\\n3.4.2\\n式(3.37) 到式(3.39) 的推导. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n28\\n3.4.3\\n式(3.43) 的推导', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 3, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='28\\n3.4.3\\n式(3.43) 的推导\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n29\\n3.4.4\\n式(3.44) 的推导\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n29\\n3.4.5\\n式(3.45) 的推导\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n30\\n3.5\\n多分类学习. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n31\\n3.5.1\\n图3.5 的解释. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n31\\n3.6', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 3, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='31\\n3.6\\n类别不平衡问题. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n31\\n第4 章决策树\\n32\\n4.1\\n基本流程\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n32\\n4.2\\n划分选择\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n32\\n4.2.1\\n式(4.1) 的解释. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n32\\n4.2.2', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 3, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='32\\n4.2.2\\n式(4.2) 的解释. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n35\\n4.2.3\\n式(4.4) 的解释. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n35\\n4.2.4\\n式(4.5) 的推导. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n35\\n4.2.5\\n式(4.6) 的解释. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n36\\n4.3\\n剪枝处理\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n38\\n4.4', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 3, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='38\\n4.4\\n连续与缺失值. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n38\\n4.4.1\\n式(4.7) 的解释. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n38\\n4.4.2\\n式(4.8) 的解释. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n39\\n4.4.3\\n式(4.12) 的解释\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n39\\n4.5\\n多变量决策树. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n39\\n4.5.1', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 3, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='39\\n4.5.1\\n图(4.10) 的解释\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n39\\n4.5.2\\n图(4.11) 的解释\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n39\\n第5 章神经网络\\n41\\n5.1\\n神经元模型. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n41\\n5.2\\n感知机与多层网络\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n41\\n5.2.1\\n式(5.1) 和式(5.2) 的推导. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n41\\n5.2.2', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 3, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='41\\n5.2.2\\n图5.5 的解释. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n43\\n5.3\\n误差逆传播算法. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n43\\n5.3.1\\n式(5.10) 的推导\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n43\\n5.3.2\\n式(5.12) 的推导\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n43\\n5.3.3\\n式(5.13) 的推导\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n43\\n5.3.4\\n式(5.14) 的推导', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 3, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='43\\n5.3.4\\n式(5.14) 的推导\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n44\\n5.3.5\\n式(5.15) 的推导\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n45\\n5.4\\n全局最小与局部极小. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n45\\n→_→\\n配套视频教程：https://www.bilibili.com/video/BV1Mh411e7VU\\n←_←', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 3, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='→_→\\n欢迎去各大电商平台选购纸质版南瓜书《机器学习公式详解第2 版》\\n←_←\\n5.5\\n其他常见神经网络\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n45\\n5.5.1\\n式(5.18) 的解释\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n45\\n5.5.2\\n式(5.20) 的解释\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n45\\n5.5.3\\n式(5.22) 的解释\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n45\\n5.5.4\\n式(5.23) 的解释', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 4, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='45\\n5.5.4\\n式(5.23) 的解释\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n45\\n5.6\\n深度学习\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n46\\n5.6.1\\n什么是深度学习. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n46\\n5.6.2\\n深度学习的起源. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n46\\n5.6.3\\n怎么理解特征学习\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n46\\n第6 章支持向量机\\n47\\n6.1', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 4, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='46\\n第6 章支持向量机\\n47\\n6.1\\n间隔与支持向量. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n47\\n6.1.1\\n图6.1 的解释. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n47\\n6.1.2\\n式(6.1) 的解释. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n47\\n6.1.3\\n式(6.2) 的推导. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n47\\n6.1.4\\n式(6.3) 的推导. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n47\\n6.1.5', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 4, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='47\\n6.1.5\\n式(6.4) 的推导. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n48\\n6.1.6\\n式(6.5) 的解释. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n48\\n6.2\\n对偶问题\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n49\\n6.2.1\\n凸优化问题. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n49\\n6.2.2\\nKKT 条件. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n49\\n6.2.3', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 4, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='49\\n6.2.3\\n拉格朗日对偶函数\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n49\\n6.2.4\\n拉格朗日对偶问题\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n50\\n6.2.5\\n式(6.9) 和式(6.10) 的推导. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n52\\n6.2.6\\n式(6.11) 的推导\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n52\\n6.2.7\\n式(6.13) 的解释\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n53\\n6.3', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 4, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='53\\n6.3\\n核函数. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n53\\n6.3.1\\n式(6.22) 的解释\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n53\\n6.4\\n软间隔与正则化. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n53\\n6.4.1\\n式(6.35) 的推导\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n53\\n6.4.2\\n式(6.37) 和式(6.38) 的推导. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n53\\n6.4.3', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 4, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='53\\n6.4.3\\n式(6.39) 的推导\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n53\\n6.4.4\\n式(6.40) 的推导\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n54\\n6.4.5\\n对数几率回归与支持向量机的关系\\n. . . . . . . . . . . . . . . . . . . . . . . . . . .\\n54\\n6.4.6\\n式(6.41) 的解释\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n55\\n6.5\\n支持向量回归. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n55\\n6.5.1\\n式(6.43) 的解释', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 4, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='55\\n6.5.1\\n式(6.43) 的解释\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n55\\n6.5.2\\n式(6.45) 的推导\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n55\\n6.5.3\\n式(6.52) 的推导\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n56\\n6.6\\n核方法. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n56\\n6.6.1\\n式(6.57) 和式(6.58) 的解释. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n56\\n6.6.2', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 4, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='56\\n6.6.2\\n式(6.65) 的推导\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n57\\n6.6.3\\n式(6.66) 和式(6.67) 的解释. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n57\\n6.6.4\\n式(6.70) 的推导\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n58\\n6.6.5\\n核对数几率回归. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n60\\n→_→\\n配套视频教程：https://www.bilibili.com/video/BV1Mh411e7VU\\n←_←', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 4, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='→_→\\n欢迎去各大电商平台选购纸质版南瓜书《机器学习公式详解第2 版》\\n←_←\\n第7 章贝叶斯分类器\\n62\\n7.1\\n贝叶斯决策论. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n62\\n7.1.1\\n式(7.5) 的推导. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n62\\n7.1.2\\n式(7.6) 的推导. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n62\\n7.1.3\\n判别式模型与生成式模型\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n62\\n7.2', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 5, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='62\\n7.2\\n极大似然估计. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n62\\n7.2.1\\n式(7.12) 和(7.13) 的推导. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n62\\n7.3\\n朴素贝叶斯分类器\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n64\\n7.3.1\\n式(7.16) 和式(7.17) 的解释. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n64\\n7.3.2\\n式(7.18) 的解释\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n64\\n7.3.3\\n贝叶斯估计', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 5, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='64\\n7.3.3\\n贝叶斯估计\\n[1] . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n65\\n7.3.4\\nCategorical 分布. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n65\\n7.3.5\\nDirichlet 分布. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n65\\n7.3.6\\n式(7.19) 和式(7.20) 的推导. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n65\\n7.4\\n半朴素贝叶斯分类器. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n67\\n7.4.1\\n式(7.21) 的解释', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 5, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='67\\n7.4.1\\n式(7.21) 的解释\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n67\\n7.4.2\\n式(7.22) 的解释\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n68\\n7.4.3\\n式(7.23) 的推导\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n68\\n7.4.4\\n式(7.24) 和式(7.25) 的推导. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n68\\n7.5\\n贝叶斯网\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n69\\n7.5.1\\n式(7.27) 的解释', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 5, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='69\\n7.5.1\\n式(7.27) 的解释\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n69\\n7.6\\nEM 算法. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n69\\n7.6.1\\nJensen 不等式\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n69\\n7.6.2\\nEM 算法的推导. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n69\\n第8 章集成学习\\n76\\n8.1\\n个体与集成. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 5, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='77\\n8.1.1\\n式(8.1) 的解释. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n77\\n8.1.2\\n式(8.2) 的解释. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n77\\n8.1.3\\n式(8.3) 的推导. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n77\\n8.2\\nBoosting . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n78\\n8.2.1\\n式(8.4) 的解释. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n78\\n8.2.2', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 5, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='78\\n8.2.2\\n式(8.5) 的解释. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n78\\n8.2.3\\n式(8.6) 的推导. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n79\\n8.2.4\\n式(8.7) 的推导. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n79\\n8.2.5\\n式(8.8) 的推导. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n79\\n8.2.6\\n式(8.9) 的推导. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n80\\n8.2.7\\n式(8.10) 的解释', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 5, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='80\\n8.2.7\\n式(8.10) 的解释\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n80\\n8.2.8\\n式(8.11) 的推导\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n80\\n8.2.9\\n式(8.12) 的解释\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n81\\n8.2.10 式(8.13) 的推导\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n81\\n8.2.11 式(8.14) 的推导\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n82\\n8.2.12 式(8.16) 的推导', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 5, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='82\\n8.2.12 式(8.16) 的推导\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n82\\n8.2.13 式(8.17) 的推导\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n83\\n8.2.14 式(8.18) 的推导\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n83\\n→_→\\n配套视频教程：https://www.bilibili.com/video/BV1Mh411e7VU\\n←_←', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 5, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='→_→\\n欢迎去各大电商平台选购纸质版南瓜书《机器学习公式详解第2 版》\\n←_←\\n8.2.15 式(8.19) 的推导\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n83\\n8.2.16 AdaBoost 的个人推导. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n84\\n8.2.17 进一步理解权重更新公式\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n86\\n8.2.18 能够接受带权样本的基学习算法. . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n87\\n8.3\\nBagging 与随机森林\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n88\\n8.3.1', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 6, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='88\\n8.3.1\\n式(8.20) 的解释\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n88\\n8.3.2\\n式(8.21) 的推导\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n88\\n8.3.3\\n随机森林的解释. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n88\\n8.4\\n结合策略\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n88\\n8.4.1\\n式(8.22) 的解释\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n88\\n8.4.2\\n式(8.23) 的解释', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 6, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='88\\n8.4.2\\n式(8.23) 的解释\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n88\\n8.4.3\\n硬投票和软投票的解释. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n89\\n8.4.4\\n式(8.24) 的解释\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n89\\n8.4.5\\n式(8.25) 的解释\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n89\\n8.4.6\\n式(8.26) 的解释\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n89\\n8.4.7', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 6, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='89\\n8.4.7\\n元学习器(meta-learner) 的解释. . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n89\\n8.4.8\\nStacking 算法的解释. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n89\\n8.5\\n多样性. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n90\\n8.5.1\\n式(8.27) 的解释\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n90\\n8.5.2\\n式(8.28) 的解释\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n90\\n8.5.3\\n式(8.29) 的解释', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 6, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='90\\n8.5.3\\n式(8.29) 的解释\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n90\\n8.5.4\\n式(8.30) 的解释\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n90\\n8.5.5\\n式(8.31) 的推导\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n90\\n8.5.6\\n式(8.32) 的解释\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n91\\n8.5.7\\n式(8.33) 的解释\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n91\\n8.5.8\\n式(8.34) 的解释', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 6, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='91\\n8.5.8\\n式(8.34) 的解释\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n91\\n8.5.9\\n式(8.35) 的解释\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n91\\n8.5.10 式(8.36) 的解释\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n91\\n8.5.11 式(8.40) 的解释\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n92\\n8.5.12 式(8.41) 的解释\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n92\\n8.5.13 式(8.42) 的解释', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 6, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='92\\n8.5.13 式(8.42) 的解释\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n92\\n8.5.14 多样性增强的解释\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n92\\n8.6\\nGradient Boosting/GBDT/XGBoost 联系与区别. . . . . . . . . . . . . . . . . . . . . . .\\n92\\n8.6.1\\n梯度下降法. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n93\\n8.6.2\\n从梯度下降的角度解释AdaBoost . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n94\\n8.6.3', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 6, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='94\\n8.6.3\\n梯度提升(Gradient Boosting) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n96\\n8.6.4\\n梯度提升树(GBDT) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n97\\n8.6.5\\nXGBoost\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n97\\n第9 章聚类\\n98\\n9.1\\n聚类任务\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n98\\n9.2\\n性能度量\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n98', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 6, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='98\\n9.2.1\\n式(9.5) 的解释. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n98\\n9.2.2\\n式(9.6) 的解释. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n99\\n9.2.3\\n式(9.7) 的解释. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n99\\n→_→\\n配套视频教程：https://www.bilibili.com/video/BV1Mh411e7VU\\n←_←', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 6, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='→_→\\n欢迎去各大电商平台选购纸质版南瓜书《机器学习公式详解第2 版》\\n←_←\\n9.2.4\\n式(9.8) 的解释. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n99\\n9.2.5\\n式(9.12) 的解释\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n99\\n9.3\\n距离计算\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n99\\n9.3.1\\n式(9.21) 的解释\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n100\\n9.4\\n原型聚类', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 7, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='100\\n9.4\\n原型聚类\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n100\\n9.4.1\\n式(9.28) 的解释\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n100\\n9.4.2\\n式(9.29) 的解释\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n100\\n9.4.3\\n式(9.30) 的解释\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n101\\n9.4.4\\n式(9.31) 的解释\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n101\\n9.4.5', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 7, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='101\\n9.4.5\\n式(9.32) 的解释\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n101\\n9.4.6\\n式(9.33) 的推导\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n102\\n9.4.7\\n式(9.34) 的推导\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n102\\n9.4.8\\n式(9.35) 的推导\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n103\\n9.4.9\\n式(9.36) 的解释\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n104', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 7, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='104\\n9.4.10 式(9.37) 的推导\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n104\\n9.4.11 式(9.38) 的推导\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n105\\n9.4.12 图9.6 的解释. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n105\\n9.5\\n密度聚类\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n106\\n9.5.1\\n密度直达、密度可达与密度相连. . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n106\\n9.5.2', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 7, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='106\\n9.5.2\\n图9.9 的解释. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n107\\n9.6\\n层次聚类\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n107\\n第10 章降维与度量学习\\n108\\n10.1 预备知识\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n108\\n10.1.1 符号约定\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n108', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 7, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='108\\n10.1.2 矩阵与单位阵、向量的乘法. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n108\\n10.2 矩阵的F 范数与迹. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n108\\n10.3 k 近邻学习\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n110\\n10.3.1 式(10.1) 的解释\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n110\\n10.3.2 式(10.2) 的推导\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n110\\n10.4 低维嵌入', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 7, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='110\\n10.4 低维嵌入\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n111\\n10.4.1 图10.2 的解释\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n111\\n10.4.2 式(10.3) 的推导\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n111\\n10.4.3 式(10.4) 的推导\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n111\\n10.4.4 式(10.5) 的推导\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n112', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 7, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='112\\n10.4.5 式(10.6) 的推导\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n112\\n10.4.6 式(10.10) 的推导. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n113\\n10.4.7 式(10.11) 的解释. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n113\\n10.4.8 图10.3 关于MDS 算法的解释\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n113\\n10.5 主成分分析. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n114', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 7, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='114\\n10.5.1 式(10.14) 的推导. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n114\\n10.5.2 式(10.16) 的解释. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n116\\n10.5.3 式(10.17) 的推导. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n118\\n10.5.4 根据式(10.17) 求解式(10.16) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n119\\n10.6 核化线性降维. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n119\\n→_→', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 7, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='119\\n→_→\\n配套视频教程：https://www.bilibili.com/video/BV1Mh411e7VU\\n←_←', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 7, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='→_→\\n欢迎去各大电商平台选购纸质版南瓜书《机器学习公式详解第2 版》\\n←_←\\n10.6.1 式(10.19) 的解释. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n120\\n10.6.2 式(10.20) 的解释. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n120\\n10.6.3 式(10.21) 的解释. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n120\\n10.6.4 式(10.22) 的解释. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n120', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 8, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='120\\n10.6.5 式(10.24) 的推导. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n120\\n10.6.6 式(10.25) 的解释. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n121\\n10.7 流形学习\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n121\\n10.7.1 等度量映射(Isomap) 的解释\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n121\\n10.7.2 式(10.28) 的推导. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n121', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 8, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='121\\n10.7.3 式(10.31) 的推导. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n123\\n10.8 度量学习\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n124\\n10.8.1 式(10.34) 的解释. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n124\\n10.8.2 式(10.35) 的解释. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n125\\n10.8.3 式(10.36) 的解释. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n125', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 8, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='125\\n10.8.4 式(10.37) 的解释. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n125\\n10.8.5 式(10.38) 的解释. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n126\\n10.8.6 式(10.39) 的解释. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n126\\n第11 章特征选择与稀疏学习\\n127\\n11.1 子集搜索与评价. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n127\\n11.1.1 式(11.1) 的解释', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 8, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='127\\n11.1.1 式(11.1) 的解释\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n127\\n11.1.2 式(11.2) 的解释\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n127\\n11.2 过滤式选择. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n127\\n11.2.1 包裹式选择. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n127\\n11.3 嵌入式选择与L1 正则化. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n128', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 8, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='128\\n11.3.1 式(11.5) 的解释\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n128\\n11.3.2 式(11.6) 的解释\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n128\\n11.3.3 式(11.7) 的解释\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n128\\n11.3.4 式(11.8) 的解释\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n128\\n11.3.5 式(11.9) 的解释\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n129', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 8, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='129\\n11.3.6 式(11.10) 的推导. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n129\\n11.3.7 式(11.11) 的解释. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n129\\n11.3.8 式(11.12) 的解释. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n130\\n11.3.9 式(11.13) 的解释. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n130\\n11.3.10 式(11.14) 的推导. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n130', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 8, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='130\\n11.4 稀疏表示与字典学习. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n131\\n11.4.1 式(11.15) 的解释. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n132\\n11.4.2 式(11.16) 的解释. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n132\\n11.4.3 式(11.17) 的推导. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n132\\n11.4.4 式(11.18) 的推导. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n132', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 8, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='132\\n11.5 K-SVD 算法. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n133\\n11.6 压缩感知\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n135\\n11.6.1 式(11.21) 的解释. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n135\\n11.6.2 式(11.25) 的解释. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n135\\n→_→\\n配套视频教程：https://www.bilibili.com/video/BV1Mh411e7VU\\n←_←', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 8, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='→_→\\n欢迎去各大电商平台选购纸质版南瓜书《机器学习公式详解第2 版》\\n←_←\\n第12 章计算学习理论\\n136\\n12.1 基础知识\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n136\\n12.1.1 式(12.1) 的解释\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n136\\n12.1.2 式(12.2) 的解释\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n136\\n12.1.3 式(12.3) 的解释\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n136\\n12.1.4 式(12.4) 的解释', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 9, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='136\\n12.1.4 式(12.4) 的解释\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n136\\n12.1.5 式(12.5) 的解释\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n136\\n12.1.6 式(12.7) 的解释\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n137\\n12.2 PAC 学习. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n137\\n12.2.1 式(12.9) 的解释\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n138', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 9, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='138\\n12.3 有限假设空间. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n138\\n12.3.1 式(12.10) 的解释. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n138\\n12.3.2 式(12.11) 的解释. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n138\\n12.3.3 式(12.12) 的推导. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n138\\n12.3.4 式(12.13) 的解释. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n139', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 9, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='139\\n12.3.5 式(12.14) 的推导. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n139\\n12.3.6 引理12.1 的解释. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n139\\n12.3.7 式(12.18) 的推导. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n140\\n12.3.8 式(12.19) 的推导. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n140\\n12.3.9 式(12.20) 的解释. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n140', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 9, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='140\\n12.4 VC 维. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n141\\n12.4.1 式(12.21) 的解释. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n141\\n12.4.2 式(12.22) 的解释. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n141\\n12.4.3 式(12.23) 的解释. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n141\\n12.4.4 引理12.2 的解释. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n142', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 9, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='142\\n12.4.5 式(12.28) 的解释. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n143\\n12.4.6 式(12.29) 的解释. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n143\\n12.4.7 式(12.30) 的解释. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n144\\n12.4.8 定理12.4 的解释. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n144\\n12.5 Rademacher 复杂度\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n145', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 9, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='145\\n12.5.1 式(12.36) 的解释. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n145\\n12.5.2 式(12.37) 的解释. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n145\\n12.5.3 式(12.38) 的解释. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n145\\n12.5.4 式(12.39) 的解释. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n145\\n12.5.5 式(12.40) 的解释. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n146', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 9, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='146\\n12.5.6 式(12.41) 的解释. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n146\\n12.5.7 定理12.5 的解释. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n146\\n12.6 定理12.6 的解释. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n147\\n12.6.1 式(12.52) 的证明. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n148\\n12.6.2 式(12.53) 的推导. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n148', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 9, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='148\\n12.7 稳定性. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n148\\n12.7.1 泛化/经验/留一损失的解释. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n149\\n12.7.2 式(12.57) 的解释. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n149\\n12.7.3 定理12.8 的解释. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n149\\n→_→\\n配套视频教程：https://www.bilibili.com/video/BV1Mh411e7VU\\n←_←', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 9, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='→_→\\n欢迎去各大电商平台选购纸质版南瓜书《机器学习公式详解第2 版》\\n←_←\\n12.7.4 式(12.60) 的推导. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n149\\n12.7.5 经验损失最小化. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n149\\n12.7.6 定理(12.9) 的证明的解释. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n149\\n第13 章半监督学习\\n151\\n13.1 未标记样本. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n151', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 10, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='151\\n13.2 生成式方法. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n151\\n13.2.1 式(13.1) 的解释\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n151\\n13.2.2 式(13.2) 的推导\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n151\\n13.2.3 式(13.3) 的推导\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n152\\n13.2.4 式(13.4) 的推导\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n152', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 10, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='152\\n13.2.5 式(13.5) 的解释\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n152\\n13.2.6 式(13.6) 的解释\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n152\\n13.2.7 式(13.7) 的解释\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n153\\n13.2.8 式(13.8) 的解释\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n154\\n13.3 半监督SVM\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n156', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 10, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='156\\n13.3.1 图13.3 的解释\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n156\\n13.3.2 式(13.9) 的解释\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n156\\n13.3.3 图13.4 的解释\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n156\\n13.3.4 式(13.10) 的解释. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n158\\n13.4 图半监督学习. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n158', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 10, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='158\\n13.4.1 式(13.12) 的推导. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n158\\n13.4.2 式(13.13) 的推导. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n159\\n13.4.3 式(13.14) 的推导. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n159\\n13.4.4 式(13.15) 的推导. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n160\\n13.4.5 式(13.16) 的解释. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n160', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 10, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='160\\n13.4.6 式(13.17) 的推导. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n160\\n13.4.7 式(13.18) 的解释. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n160\\n13.4.8 式(13.20) 的解释. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n160\\n13.4.9 式(13.21) 的推导. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n161\\n13.5 基于分歧的方法. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n164', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 10, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='164\\n13.5.1 图13.6 的解释\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n164\\n13.6 半监督聚类. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n164\\n13.6.1 图13.7 的解释\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n164\\n13.6.2 图13.9 的解释\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n164\\n第14 章概率图模型\\n166', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 10, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='164\\n第14 章概率图模型\\n166\\n14.1 隐马尔可夫模型. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n166\\n14.1.1 生成式模型和判别式模型\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n166\\n14.1.2 式(14.1) 的推导\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n166\\n14.1.3 隐马尔可夫模型的三组参数. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n167\\n14.2 马尔可夫随机场. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n167', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 10, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='167\\n14.2.1 式(14.2) 和式(14.3) 的解释. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n167\\n14.2.2 式(14.4) 到式(14.7) 的推导. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n167\\n14.2.3 马尔可夫毯(Markov blanket) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n168\\n→_→\\n配套视频教程：https://www.bilibili.com/video/BV1Mh411e7VU\\n←_←', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 10, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='→_→\\n欢迎去各大电商平台选购纸质版南瓜书《机器学习公式详解第2 版》\\n←_←\\n14.2.4 势函数(potential function) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n168\\n14.2.5 式(14.8) 的解释\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n168\\n14.2.6 式(14.9) 的解释\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n168\\n14.3 条件随机场. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n168', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 11, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='168\\n14.3.1 式(14.10) 的解释. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n168\\n14.3.2 式(14.11) 的解释. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n169\\n14.3.3 学习与推断. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n169\\n14.3.4 式(14.14) 的推导. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n169\\n14.3.5 式(14.15) 和式(14.16) 的推导\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n169', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 11, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='169\\n14.3.6 式(14.17) 的解释. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n169\\n14.3.7 式(14.18) 的推导. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n170\\n14.3.8 式(14.19) 的解释. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n170\\n14.3.9 式(14.20) 的解释. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n170\\n14.3.10 式(14.22) 的推导. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n170', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 11, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='170\\n14.3.11 图14.8 的解释. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n171\\n14.4 近似推断\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n171\\n14.4.1 式(14.21) 到式(14.25) 的解释\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n171\\n14.4.2 式(14.26) 的解释. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n171\\n14.4.3 式(14.27) 的解释. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n172', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 11, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='172\\n14.4.4 式(14.28) 的推导. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n172\\n14.4.5 吉布斯采样与MH 算法\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n173\\n14.4.6 式(14.29) 的解释. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n173\\n14.4.7 式(14.30) 的解释. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n173\\n14.4.8 式(14.31) 的解释. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n173', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 11, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='173\\n14.4.9 式(14.32) 到式(14.34) 的推导\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n173\\n14.4.10 式(14.35) 的解释. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n174\\n14.4.11 式(14.36) 的推导. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n174\\n14.4.12 式(14.37) 到式(14.38) 的解释. . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n175\\n14.4.13 式(14.39) 的解释. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n176', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 11, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='176\\n14.4.14 式(14.40) 的解释. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n176\\n14.5 话题模型\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n177\\n14.5.1 式(14.41) 的解释. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n177\\n14.5.2 式(14.42) 的解释. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n177\\n14.5.3 式(14.43) 的解释. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n177', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 11, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='177\\n14.5.4 式(14.44) 的解释. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n177\\n第15 章规则学习\\n178\\n15.1 剪枝优化\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n178\\n15.1.1 式(15.2) 和式(15.3) 的解释. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n178\\n15.2 归纳逻辑程序设计\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n178\\n15.2.1 式(15.6) 的解释', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 11, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='178\\n15.2.1 式(15.6) 的解释\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n178\\n15.2.2 式(15.7) 的推导\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n178\\n15.2.3 式(15.9) 的推导\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n178\\n15.2.4 式(15.10) 的解释. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n178\\n15.2.5 式(15.11) 的解释. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n179\\n→_→', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 11, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='179\\n→_→\\n配套视频教程：https://www.bilibili.com/video/BV1Mh411e7VU\\n←_←', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 11, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='15.2.6 式(15.12) 的解释. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n179\\n15.2.7 式(15.13) 的解释. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n179\\n15.2.8 式(15.16) 的推导. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n179\\n第16 章强化学习\\n180\\n16.1 任务与奖赏. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n180', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 12, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='180\\n16.2 K-摇臂赌博机. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n180\\n16.2.1 式(16.2) 和式(16.3) 的推导. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n180\\n16.2.2 式(16.4) 的解释\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n180\\n16.3 有模型学习. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n180\\n16.3.1 式(16.7) 的解释\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n180', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 12, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='180\\n16.3.2 式(16.8) 的推导\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n181\\n16.3.3 式(16.10) 的推导. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n181\\n16.3.4 式(16.14) 的解释. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n181\\n16.3.5 式(16.15) 的解释. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n181\\n16.3.6 式(16.16) 的推导. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n181', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 12, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='181\\n16.4 免模型学习. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n182\\n16.4.1 式(16.20) 的解释. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n182\\n16.4.2 式(16.23) 的解释. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n182\\n16.4.3 式(16.31) 的推导. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n182\\n16.5 值函数近似. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n182', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 12, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='182\\n16.5.1 式(16.33) 的解释. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n182\\n16.5.2 式(16.34) 的推导. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n182', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 12, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='→_→\\n欢迎去各大电商平台选购纸质版南瓜书《机器学习公式详解第2 版》\\n←_←\\n第1 章\\n绪论\\n本章作为“西瓜书”的开篇，主要讲解什么是机器学习以及机器学习的相关数学符号，为后续内容作\\n铺垫，并未涉及复杂的算法理论，因此阅读本章时只需耐心梳理清楚所有概念和数学符号即可。此外，在\\n阅读本章前建议先阅读西瓜书目录前页的《主要符号表》\\n，它能解答在阅读“西瓜书”过程中产生的大部\\n分对数学符号的疑惑。\\n本章也作为本书的开篇，笔者在此赘述一下本书的撰写初衷，本书旨在以“过来人”的视角陪读者一\\n起阅读“西瓜书”\\n，尽力帮读者消除阅读过程中的“数学恐惧”\\n，只要读者学习过《高等数学》\\n、\\n《线性代\\n数》和《概率论与数理统计》这三门大学必修的数学课，均能看懂本书对西瓜书中的公式所做的解释和推\\n导，同时也能体会到这三门数学课在机器学习上碰撞产生的“数学之美”\\n。\\n1.1\\n引言\\n本节以概念理解为主，\\n在此对\\n“算法”\\n和\\n“模型”\\n作补充说明。\\n“算法”\\n是指从数据中学得\\n“模型”\\n的具\\n体方法，\\n例如后续章节中将会讲述的线性回归、\\n对数几率回归、\\n决策树等。\\n“算法”\\n产出的结果称为\\n“模型”\\n，', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 13, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='对数几率回归、\\n决策树等。\\n“算法”\\n产出的结果称为\\n“模型”\\n，\\n通常是具体的函数或者可抽象地看作为函数，例如一元线性回归算法产出的模型即为形如f(x) = wx + b\\n的一元一次函数。不过由于严格区分这两者的意义不大，因此多数文献和资料会将其混用，当遇到这两个\\n概念时，其具体指代根据上下文判断即可。\\n1.2\\n基本术语\\n本节涉及的术语较多且很多术语都有多个称呼，下面梳理各个术语，并将最常用的称呼加粗标注。\\n样本：也称为“示例”\\n，是关于一个事件或对象的描述。因为要想让计算机能对现实生活中的事物\\n进行机器学习，必须先将其抽象为计算机能理解的形式，计算机最擅长做的就是进行数学运算，因此考\\n虑如何将其抽象为某种数学形式。显然，线性代数中的向量就很适合，因为任何事物都可以由若干“特\\n征”\\n（或称为“属性”\\n）唯一刻画出来，而向量的各个维度即可用来描述各个特征。例如，如果用色泽、根\\n蒂和敲声这3 个特征来刻画西瓜，\\n那么一个\\n“色泽青绿，\\n根蒂蜷缩，\\n敲声清脆”\\n的西瓜用向量来表示即为x =', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 13, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='那么一个\\n“色泽青绿，\\n根蒂蜷缩，\\n敲声清脆”\\n的西瓜用向量来表示即为x =\\n(青绿; 蜷缩; 清脆) （向量中的元素用分号“;”分隔时表示此向量为列向量，用逗号“,”分隔时表示为行向量），\\n其中青绿、蜷缩和清脆分别对应为相应特征的取值，也称为“属性值”\\n。显然，用中文书写向量的方式不够\\n“数学”\\n，因此需要将属性值进一步数值化，具体例子参见“西瓜书”第3 章3.2。此外，仅靠以上3 个特\\n征来刻画西瓜显然不够全面细致，因此还需要扩展更多维度的特征，一般称此类与特征处理相关的工作为\\n“特征工程”\\n。\\n样本空间：也称为“输入空间”或“属性空间”\\n。由于样本采用的是标明各个特征取值的“特征向量”\\n来进行表示，根据线性代数的知识可知，有向量便会有向量所在的空间，因此称表示样本的特征向量所在\\n的空间为样本空间，通常用花式大写的X 表示。\\n数据集：数据集通常用集合来表示，令集合D = {x1, x2, ..., xm} 表示包含m 个样本的数据集，一般\\n同一份数据集中的每个样本都含有相同个数的特征，假设此数据集中的每个样本都含有d 个特征，则第i', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 13, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='同一份数据集中的每个样本都含有相同个数的特征，假设此数据集中的每个样本都含有d 个特征，则第i\\n个样本的数学表示为d 维向量：xi = (xi1; xi2; ...; xid)，其中xij 表示样本xi 在第j 个属性上的取值。\\n模型：机器学习的一般流程如下：首先收集若干样本（假设此时有100 个）\\n，然后将其分为训练样本\\n（80 个）和测试样本（20 个）\\n，其中80 个训练样本构成的集合称为“训练集”\\n，20 个测试样本构成的集合\\n称为“测试集”\\n，接着选用某个机器学习算法，让其在训练集上进行“学习”\\n（或称为“训练”\\n）\\n，然后产出\\n得到“模型”\\n（或称为“学习器”\\n）\\n，最后用测试集来测试模型的效果。执行以上流程时，表示我们已经默认\\n样本的背后是存在某种潜在的规律，我们称这种潜在的规律为“真相”或者“真实”\\n，例如样本是一堆好西\\n瓜和坏西瓜时，我们默认的便是好西瓜和坏西瓜背后必然存在某种规律能将其区分开。当我们应用某个机\\n器学习算法来学习时，产出得到的模型便是该算法所找到的它自己认为的规律，由于该规律通常并不一定\\n→_→', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 13, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='→_→\\n配套视频教程：https://www.bilibili.com/video/BV1Mh411e7VU\\n←_←', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 13, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='→_→\\n欢迎去各大电商平台选购纸质版南瓜书《机器学习公式详解第2 版》\\n←_←\\n就是所谓的真相，所以也将其称为“假设”\\n。通常机器学习算法都有可配置的参数，同一个机器学习算法，\\n使用不同的参数配置或者不同的训练集，训练得到的模型通常都不同。\\n标记：上文提到机器学习的本质就是在学习样本在某个方面的表现是否存在潜在的规律，我们称该方\\n面的信息为“标记”\\n。例如在学习西瓜的好坏时，\\n“好瓜”和“坏瓜”便是样本的标记。一般第i 个样本的\\n标记的数学表示为yi，标记所在的空间称为“标记空间”或“输出空间”\\n，数学表示为花式大写的Y。标\\n记通常也看作为样本的一部分，因此，一个完整的样本通常表示为(x, y)。\\n根据标记的取值类型不同，可将机器学习任务分为以下两类：\\n• 当标记取值为离散型时，称此类任务为“分类”\\n，例如学习西瓜是好瓜还是坏瓜、学习猫的图片是白\\n猫还是黑猫等。当分类的类别只有两个时，称此类任务为“二分类”\\n，通常称其中一个为“正类”\\n，另\\n一个为“反类”或“负类”\\n；当分类的类别超过两个时，称此类任务为“多分类”\\n。由于标记也属于样', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 14, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='一个为“反类”或“负类”\\n；当分类的类别超过两个时，称此类任务为“多分类”\\n。由于标记也属于样\\n本的一部分，通常也需要参与运算，因此也需要将其数值化，例如对于二分类任务，通常将正类记为\\n1，反类记为0，即Y = {0, 1}。这只是一般默认的做法，具体标记该如何数值化可根据具体机器学\\n习算法进行相应地调整，例如第6 章的支持向量机算法则采用的是Y = {−1, +1}；\\n• 当标记取值为连续型时，称此类任务为“回归”\\n，例如学习预测西瓜的成熟度、学习预测未来的房价\\n等。由于是连续型，因此标记的所有可能取值无法直接罗列，通常只有取值范围，回归任务的标记取\\n值范围通常是整个实数域R，即Y = R。\\n无论是分类还是回归，机器学习算法最终学得的模型都可以抽象地看作为以样本x 为自变量，标记y\\n为因变量的函数y = f(x)，即一个从输入空间X 到输出空间Y 的映射。例如在学习西瓜的好坏时，机器\\n学习算法学得的模型可看作为一个函数f(x)，给定任意一个西瓜样本xi = (青绿; 蜷缩; 清脆)，将其输入', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 14, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='进函数即可计算得到一个输出yi = f(xi)，此时得到的yi 便是模型给出的预测结果，当yi 取值为1 时表\\n明模型认为西瓜xi 是好瓜，当yi 取值为0 时表明模型认为西瓜xi 是坏瓜。\\n根据是否有用到标记信息，可将机器学习任务分为以下两类：\\n• 在模型训练阶段有用到标记信息时，称此类任务为“监督学习”\\n，例如第3 章的线性模型；\\n• 在模型训练阶段没用到标记信息时，称此类任务为“无监督学习”\\n，例如第9 章的聚类。\\n泛化：由于机器学习的目标是根据已知来对未知做出尽可能准确的判断，因此对未知事物判断的准确\\n与否才是衡量一个模型好坏的关键，\\n我们称此为\\n“泛化”\\n能力。\\n例如学习西瓜好坏时，\\n假设训练集中共有3\\n个样本：{(x1 = (青绿; 蜷缩), y1 = 好瓜), (x2 = (乌黑; 蜷缩), y2 = 好瓜), (x3 = (浅白; 蜷缩), y3 = 好瓜)}，\\n同时假设判断西瓜好坏的真相是“只要根蒂蜷缩就是好瓜”\\n，如果应用算法A 在此训练集上训练得到模型\\nfa(x)，模型a 学到的规律是“色泽等于青绿、乌黑或者浅白时，同时根蒂蜷缩即为好瓜，否则便是坏瓜”\\n，', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 14, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='，\\n再应用算法B 在此训练集上训练得到模型fb(x)，\\n模型fb(x) 学到的规律是\\n“只要根蒂蜷缩就是好瓜”\\n，\\n因\\n此对于一个未见过的西瓜样本x = (金黄; 蜷缩) 来说，模型fa(x) 给出的预测结果为“坏瓜”\\n，模型fb(x)\\n给出的预测结果为“好瓜”\\n，此时我们称模型fb(x) 的泛化能力优于模型fa(x)。\\n通过以上举例可知，尽管模型fa(x) 和模型fb(x) 对训练集学得一样好，即两个模型对训练集中每个\\n样本的判断都对，但是其所学到的规律是不同的。导致此现象最直接的原因是算法的不同，但是算法通常\\n是有限的，可穷举的，尤其是在特定任务场景下可使用的算法更是有限，因此，数据便是导致此现象的另\\n一重要原因，\\n这也就是机器学习领域常说的\\n“数据决定模型的上限，\\n而算法则是让模型无限逼近上限”\\n, 下\\n面详细解释此话的含义。\\n先解释“数据决定模型效果的上限”\\n，其中数据是指从数据量和特征工程两个角度考虑。从数据量的\\n角度来说，通常数据量越大模型效果越好，因为数据量大即表示累计的经验多，因此模型学习到的经验也', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 14, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='角度来说，通常数据量越大模型效果越好，因为数据量大即表示累计的经验多，因此模型学习到的经验也\\n多，自然表现效果越好。例如以上举例中如果训练集中含有相同颜色但根蒂不蜷缩的坏瓜，模型a 学到真\\n相的概率则也会增大；从特征工程的角度来说，通常对特征数值化越合理，特征收集越全越细致，模型效\\n果通常越好，因为此时模型更易学得样本之间潜在的规律。例如学习区分亚洲人和非洲人时，此时样本即\\n→_→\\n配套视频教程：https://www.bilibili.com/video/BV1Mh411e7VU\\n←_←', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 14, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='→_→\\n欢迎去各大电商平台选购纸质版南瓜书《机器学习公式详解第2 版》\\n←_←\\n为人，在进行特征工程时，如果收集到每个样本的肤色特征，则其他特征例如年龄、身高和体重等便可省\\n略，因为只需靠肤色这一个特征就足以区分亚洲人和非洲人。\\n而“算法则是让模型无限逼近上限”是指当数据相关的工作已准备充分时，接下来便可用各种可适用\\n的算法从数据中学习其潜在的规律进而得到模型，不同的算法学习得到的模型效果自然有高低之分，效果\\n越好则越逼近上限，即逼近真相。\\n分布：此处的“分布”指的是概率论中的概率分布，通常假设样本空间服从一个未知“分布”D，而\\n我们收集到的每个样本都是独立地从该分布中采样得到，即“独立同分布”\\n。通常收集到的样本越多，越\\n能从样本中反推出D 的信息，即越接近真相。此假设属于机器学习中的经典假设，在后续学习机器学习\\n算法过程中会经常用到。\\n1.3\\n假设空间\\n本节的重点是理解“假设空间”和“版本空间”\\n，下面以“房价预测”举例说明。假设现已收集到某地\\n区近几年的房价和学校数量数据，希望利用收集到的数据训练出能通过学校数量预测房价的模型，具体收\\n集到的数据如表1-1所示。', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 15, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='集到的数据如表1-1所示。\\n表1-1 房价预测\\n年份\\n学校数量\\n房价\\n2020\\n1 所\\n1 万/m2\\n2021\\n2 所\\n4 万/m2\\n基于对以上数据的观察以及日常生活经验，不难得出“房价与学校数量成正比”的假设，若将学校数\\n量设为x，房价设为y，则该假设等价表示学校数量和房价呈y = wx + b 的一元一次函数关系，此时房价\\n预测问题的假设空间即为“一元一次函数”\\n。确定假设空间以后便可以采用机器学习算法从假设空间中学\\n得模型，即从一元一次函数空间中学得能满足表1-1中数值关系的某个一元一次函数。学完第3 章的线性\\n回归可知当前问题属于一元线性回归问题，根据一元线性回归算法可学得模型为y = 3x −2。\\n除此之外，也可以将问题复杂化，假设学校数量和房价呈y = wx2 + b 一元二次函数关系，此时问题\\n变为了线性回归中的多项式回归问题，按照多项式回归算法可学得模型为y = x2。因此，以表1-1中数据\\n作为训练集可以有多个假设空间，且在不同的假设空间中都有可能学得能够拟合训练集的模型，我们将所\\n有能够拟合训练集的模型构成的集合称为“版本空间”\\n。\\n1.4\\n归纳偏好', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 15, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='有能够拟合训练集的模型构成的集合称为“版本空间”\\n。\\n1.4\\n归纳偏好\\n在上一节“房价预测”的例子中，当选用一元线性回归算法时，学得的模型是一元一次函数，当选\\n用多项式回归算法时，学得的模型是一元二次函数，所以不同的机器学习算法有不同的偏好，我们称为\\n“归纳偏好”\\n。对于当前房价预测这个例子来说，这两个算法学得的模型哪个更好呢？著名的“奥卡姆剃\\n刀”原则认为“若有多个假设与观察一致，则选最简单的那个”\\n，但是何为“简单”便见仁见智了，如\\n果认为函数的幂次越低越简单，则此时一元线性回归算法更好，如果认为幂次越高越简单，则此时多项\\n式回归算法更好，因此该方法其实并不“简单”\\n，所以并不常用，而最常用的方法则是基于模型在测试\\n集上的表现来评判模型之间的优劣。测试集是指由训练集之外的样本构成的集合，例如在当前房价预测\\n问题中，通常会额外留有部分未参与模型训练的数据来对模型进行测试。假设此时额外留有1 条数据：\\n(年份: 2022年; 学校数量: 3所; 房价: 7万/m2) 用于测试，模型y = 3x −2 的预测结果为3 ∗3 −2 = 7，预', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 15, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='测正确，模型y = x2 的预测结果为32 = 9，预测错误，因此，在当前房价预测问题上，我们认为一元线\\n性回归算法优于多项式回归算法。\\n→_→\\n配套视频教程：https://www.bilibili.com/video/BV1Mh411e7VU\\n←_←', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 15, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='→_→\\n欢迎去各大电商平台选购纸质版南瓜书《机器学习公式详解第2 版》\\n←_←\\n机器学习算法之间没有绝对的优劣之分，只有是否适合当前待解决的问题之分，例如上述测试集中的\\n数据如果改为(年份: 2022年; 学校数量: 3所; 房价: 9万/m2) 则结论便逆转为多项式回归算法优于一元线\\n性回归算法。\\n1.4.1\\n式(1.1) 和式(1.2) 的解释\\nX\\nf\\nEote(La|X, f) =\\nX\\nf\\nX\\nh\\nX\\nx∈X−X\\nP(x)I(h(x) ̸= f(x))P(h|X, La)\\n1\\n=\\nX\\nx∈X−X\\nP(x)\\nX\\nh\\nP(h|X, La)\\nX\\nf\\nI(h(x) ̸= f(x))\\n2\\n=\\nX\\nx∈X−X\\nP(x)\\nX\\nh\\nP(h|X, La)\\n1\\n22|X|\\n3\\n=\\n1\\n22|X|\\nX\\nx∈X−X\\nP(x)\\nX\\nh\\nP(h|X, La)\\n4\\n= 2|X|−1\\nX\\nx∈X−X\\nP(x) · 1\\n5\\n1 →2 ：\\nX\\nf\\nX\\nh\\nX\\nx∈X−X\\nP(x)I(h(x) ̸= f(x))P(h|X, La)\\n=\\nX\\nx∈X−X\\nP(x)\\nX\\nf\\nX\\nh', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 16, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='=\\nX\\nx∈X−X\\nP(x)\\nX\\nf\\nX\\nh\\nI(h(x) ̸= f(x))P(h|X, La)\\n=\\nX\\nx∈X−X\\nP(x)\\nX\\nh\\nP(h|X, La)\\nX\\nf\\nI(h(x) ̸= f(x))\\n2 →3 ：首先要知道此时我们假设f 是任何能将样本映射到{0, 1} 的函数。存在不止一个f 时，f\\n服从均匀分布，即每个f 出现的概率相等。例如样本空间只有两个样本时，X = {x1, x2}, |X| = 2。那么\\n所有可能的真实目标函数f 如下：\\nf1 : f1(x1) = 0, f1(x2) = 0\\nf2 : f2(x1) = 0, f2(x2) = 1\\nf3 : f3(x1) = 1, f3(x2) = 0\\nf4 : f4(x1) = 1, f4(x2) = 1\\n一共2|X| = 22 = 4 个可能的真实目标函数。所以此时通过算法La 学习出来的模型h(x) 对每个样本无论\\n预测值为0 还是1，都必然有一半的f 与之预测值相等。例如，现在学出来的模型h(x) 对x1 的预测值', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 16, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='为1，即h(x1) = 1，那么有且只有f3 和f4 与h(x) 的预测值相等，也就是有且只有一半的f 与它预测\\n值相等，所以P\\nf I(h(x) ̸= f(x)) = 1\\n22|X|。\\n需要注意的是，\\n在这里我们假设真实的目标函数f 服从均匀分布，\\n但是实际情形并非如此，\\n通常我们只\\n认为能高度拟合已有样本数据的函数才是真实目标函数，\\n例如，\\n现在已有的样本数据为{(x1, 0), (x2, 1)}，\\n那\\n么此时f2 才是我们认为的真实目标函数，\\n由于没有收集到或者压根不存在{(x1, 0), (x2, 0)}, {(x1, 1), (x2, 0)},\\n{(x1, 1), (x2, 1)} 这类样本，所以f1, f3, f4 都不算是真实目标函数。套用到上述“房价预测”的例子中，我\\n们认为只有能正确拟合测试集的函数才是真实目标函数，也就是我们希望学得的模型。\\n→_→\\n配套视频教程：https://www.bilibili.com/video/BV1Mh411e7VU\\n←_←', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 16, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='→_→\\n欢迎去各大电商平台选购纸质版南瓜书《机器学习公式详解第2 版》\\n←_←\\n第2 章\\n模型评估与选择\\n如“西瓜书”前言所述，本章仍属于机器学习基础知识，如果说第1 章介绍了什么是机器学习及机\\n器学习的相关数学符号，那么本章则进一步介绍机器学习的相关概念。具体来说，介绍内容正如本章名称\\n“模型评估与选择”所述，讲述的是如何评估模型的优劣和选择最适合自己业务场景的模型。\\n由于“模型评估与选择”是在模型产出以后进行的下游工作，要想完全吸收本章内容需要读者对模型\\n有一些基本的认知，因此零基础的读者直接看本章会很吃力，实属正常，在此建议零基础的读者可以简单\\n泛读本章，仅看能看懂的部分即可，或者直接跳过本章从第3 章开始看，直至看完第6 章以后再回头来看\\n本章便会轻松许多。\\n2.1\\n经验误差与过拟合\\n梳理本节的几个概念。\\n错误率：E = a\\nm，其中m 为样本个数，a 为分类错误样本个数。\\n精度：精度=1-错误率。\\n误差：学习器的实际预测输出与样本的真实输出之间的差异。\\n经验误差：学习器在训练集上的误差，又称为“训练误差”\\n。\\n泛化误差：学习器在新样本上的误差。', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 17, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='经验误差：学习器在训练集上的误差，又称为“训练误差”\\n。\\n泛化误差：学习器在新样本上的误差。\\n经验误差和泛化误差用于分类问题的定义式可参见“西瓜书”第12 章的式(12.1) 和式(12.2)，接下\\n来辨析一下以上几个概念。\\n错误率和精度很容易理解，而且很明显是针对分类问题的。误差的概念更适用于回归问题，但是，根\\n据“西瓜书”第12 章的式(12.1) 和式(12.2) 的定义可以看出，在分类问题中也会使用误差的概念，此时\\n的“差异”指的是学习器的实际预测输出的类别与样本真实的类别是否一致，若一致则“差异”为0，若\\n不一致则“差异”为1，训练误差是在训练集上差异的平均值，而泛化误差则是在新样本（训练集中未出\\n现过的样本）上差异的平均值。\\n过拟合是由于模型的学习能力相对于数据来说过于强大，反过来说，欠拟合是因为模型的学习能力相\\n对于数据来说过于低下。暂且抛开“没有免费的午餐”定理不谈，例如对于“西瓜书”第1 章图1.4 中的\\n训练样本（黑点）来说，用类似于抛物线的曲线A 去拟合则较为合理，而比较崎岖的曲线B 相对于训练', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 17, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='训练样本（黑点）来说，用类似于抛物线的曲线A 去拟合则较为合理，而比较崎岖的曲线B 相对于训练\\n样本来说学习能力过于强大，但若仅用一条直线去训练则相对于训练样本来说直线的学习能力过于低下。\\n2.2\\n评估方法\\n本节介绍了3 种模型评估方法：留出法、交叉验证法、自助法。留出法由于操作简单，因此最常用；\\n交叉验证法常用于对比同一算法的不同参数配置之间的效果，以及对比不同算法之间的效果；自助法常用\\n于集成学习（详见“西瓜书”第8 章的8.2 节和8.3 节）产生基分类器。留出法和自助法简单易懂，在此\\n不再赘述，下面举例说明交叉验证法的常用方式。\\n对比同一算法的不同参数配置之间的效果：假设现有数据集D，且有一个被评估认为适合用于数据集\\nD 的算法L，该算法有可配置的参数，假设备选的参数配置方案有两套：方案a，方案b。下面通过交叉\\n验证法为算法L 筛选出在数据集D 上效果最好的参数配置方案。以3 折交叉验证为例，首先按照“西瓜\\n书”中所说的方法，通过分层采样将数据集D 划分为3 个大小相似的互斥子集：D1, D2, D3，然后分别用', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 17, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='其中1 个子集作为测试集，其他子集作为训练集，这样就可获得3 组训练集和测试集：\\n训练集1：D1 ∪D2，测试集1:D3\\n训练集2：D1 ∪D3，测试集2:D2\\n训练集3：D2 ∪D3，测试集3:D1\\n接下来用算法L 搭配方案a 在训练集1 上进行训练，训练结束后将训练得到的模型在测试集1 上进\\n行测试，得到测试结果1，依此方法再分别通过训练集2 和测试集2、训练集3 和测试集3 得到测试结果\\n→_→\\n配套视频教程：https://www.bilibili.com/video/BV1Mh411e7VU\\n←_←', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 17, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='→_→\\n欢迎去各大电商平台选购纸质版南瓜书《机器学习公式详解第2 版》\\n←_←\\n2 和测试结果3，最后将3 次测试结果求平均即可得到算法L 搭配方案a 在数据集D 上的最终效果，记\\n为Scorea。同理，按照以上方法也可得到算法L 搭配方案b 在数据集D 上的最终效果Scoreb，最后通\\n过比较Scorea 和Scoreb 之间的优劣来确定算法L 在数据集D 上效果最好的参数配置方案。\\n对比不同算法之间的效果：同上述“对比同一算法的不同参数配置之间的效果”中所讲的方法一样，\\n只需将其中的“算法L 搭配方案a”和“算法L 搭配方案b”分别换成需要对比的算法α 和算法β 即可。\\n从以上的举例可以看出，交叉验证法本质上是在进行多次留出法，且每次都换不同的子集做测试集，\\n最终让所有样本均至少做1 次测试样本。这样做的理由其实很简单，因为一般的留出法只会划分出1 组\\n训练集和测试集，仅依靠1 组训练集和测试集去对比不同算法之间的效果显然不够置信，偶然性太强，因\\n此要想基于固定的数据集产生多组不同的训练集和测试集，则只有进行多次划分，每次采用不同的子集作\\n为测试集，也即为交叉验证法。\\n2.2.1', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 18, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='为测试集，也即为交叉验证法。\\n2.2.1\\n算法参数（超参数）与模型参数\\n算法参数是指算法本身的一些参数（也称超参数）\\n，例如k 近邻的近邻个数k、支持向量机的参数C\\n（详见“西瓜书”第6 章式(6.29)）\\n。算法配置好相应参数后进行训练，训练结束会得到一个模型，例如支\\n持向量机最终会得到w 和b 的具体数值（此处不考虑核函数）\\n，这就是模型参数，模型配置好相应模型参\\n数后即可对新样本做预测。\\n2.2.2\\n验证集\\n带有参数的算法一般需要从候选参数配置方案中选择相对于当前数据集的最优参数配置方案，例如支\\n持向量机的参数C，一般采用的是前面讲到的交叉验证法，但是交叉验证法操作起来较为复杂，实际中更\\n多采用的是：先用留出法将数据集划分出训练集和测试集，然后再对训练集采用留出法划分出训练集和新\\n的测试集，称新的测试集为验证集，接着基于验证集的测试结果来调参选出最优参数配置方案，最后将验\\n证集合并进训练集（训练集数据量够的话也可不合并）\\n，用选出的最优参数配置在合并后的训练集上重新\\n训练，再用测试集来评估训练得到的模型的性能。\\n2.3\\n性能度量', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 18, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='训练，再用测试集来评估训练得到的模型的性能。\\n2.3\\n性能度量\\n本节性能度量指标较多，但是一般常用的只有错误率、精度、查准率、查全率、F1、ROC 和AUC。\\n2.3.1\\n式(2.2) 到式(2.7) 的解释\\n这几个公式简单易懂，几乎不需要额外解释，但是需要补充说明的是式(2.2)、式(2.4) 和式(2.5) 假\\n设了数据分布为均匀分布，即每个样本出现的概率相同，而式(2.3)、式(2.6) 和式(2.7) 则为更一般的表\\n达式。此外，在无特别说明的情况下，2.3 节所有公式中的“样例集D”均默认为非训练集（测试集、验\\n证集或其他未用于训练的样例集）\\n。\\n2.3.2\\n式(2.8) 和式(2.9) 的解释\\n查准率P：被学习器预测为正例的样例中有多大比例是真正例。\\n查全率R：所有正例当中有多大比例被学习器预测为正例。\\n2.3.3\\n图2.3 的解释\\nP-R 曲线的画法与ROC 曲线的画法类似，也是通过依次改变模型阈值，然后计算出查准率和查全率\\n并画出相应坐标点，具体参见“式(2.20) 的推导”部分的讲解。这里需要说明的是，\\n“西瓜书”中的图2.3', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 18, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='“西瓜书”中的图2.3\\n仅仅是示意图，除了图左侧提到的“现实任务中的P-R 曲线常是非单调、不平滑的，在很多局部有上下波\\n→_→\\n配套视频教程：https://www.bilibili.com/video/BV1Mh411e7VU\\n←_←', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 18, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='→_→\\n欢迎去各大电商平台选购纸质版南瓜书《机器学习公式详解第2 版》\\n←_←\\n动”以外，通常不会取到(1, 0) 点。当取到(1, 0) 点时，就会将所有样本均判为正例，此时FN = 0，根\\n据式(2.9) 可算得查全率为1，但是此时TP + FP 为样本总数，根据式(2.8) 可算得查准率此时为正例在\\n全体样本中的占比，显然在现实任务中正例的占比通常不为0，因此P-R 曲线在现实任务中通常不会取到\\n(1, 0) 点。\\n2.3.4\\n式(2.10) 的推导\\n将式(2.8) 和式(2.9) 代入式(2.10)，得\\nF1 = 2 × P × R\\nP + R\\n=\\n2 ×\\nT P\\nT P +F P ×\\nT P\\nT P +F N\\nT P\\nT P +F P +\\nT P\\nT P +F N\\n=\\n2 × TP × TP\\nTP(TP + FN) + TP(TP + FP)\\n=\\n2 × TP\\n(TP + FN) + (TP + FP)\\n=\\n2 × TP\\n(TP + FN + FP + TN) + TP −TN\\n=\\n2 × TP\\n样例总数+ TP −TN', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 19, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='=\\n2 × TP\\n样例总数+ TP −TN\\n若现有数据集D = {(xi, yi) | 1 ⩽i ⩽m}，其中标记yi ∈{0, 1}（1 表示正例，0 表示反例）\\n，假设模型\\nf(x) 对xi 的预测结果为hi ∈{0, 1}，则模型f(x) 在数据集D 上的F1 为\\nF1 =\\n2 Pm\\ni=1 yihi\\nPm\\ni=1 yi + Pm\\ni=1 hi\\n不难看出上式的本质为\\nF1 =\\n2 × TP\\n(TP + FN) + (TP + FP)\\n2.3.5\\n式(2.11) 的解释\\n“西瓜书”\\n在式(2.11) 左侧提到Fβ 本质是加权调和平均，\\n且和常用的算数平均相比，\\n其更重视较小值，\\n在此举例说明。例如a 同学有两门课的成绩分别为100 分和60 分，b 同学相应的成绩为80 分和80 分，\\n此时若计算a 同学和b 同学的算数平均分则均为80 分，无法判断两位同学成绩的优劣，但是若计算加权\\n调和平均，当β = 1 时，a 同学的加权调和平均为2×100×60\\n100+60\\n= 75，b 同学的加权调和平均为2×80×80\\n80+80\\n= 80，', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 19, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='100+60\\n= 75，b 同学的加权调和平均为2×80×80\\n80+80\\n= 80，\\n此时b 同学的平均成绩更优，原因是a 同学由于偏科导致其中一门成绩过低，而调和平均更重视较小值，\\n所以a 同学的偏科便被凸显出来。\\n式(2.11) 下方有提到“β > 1 时查全率有更大影响；β < 1 时查准率有更大影响”\\n，下面解释其原因。\\n将式(2.11) 恒等变形为如下形式\\nFβ =\\n1\\n1\\n1+β2 · 1\\nP +\\nβ2\\n1+β2 · 1\\nR\\n从上式可以看出，当β > 1 时\\nβ2\\n1+β2 >\\n1\\n1+β2 ，所以\\n1\\nR 的权重比\\n1\\nP 的权重高，因此查全率R 对Fβ 的影响\\n更大，反之查准率P 对Fβ 的影响更大。\\n2.3.6\\n式(2.12) 到式(2.17) 的解释\\n式(2.12) 的macro-P 和式(2.13) 的macro-R 是基于各个二分类问题的P 和R 计算而得的；\\n式(2.15)\\n的micro-P 和式(2.16) 的micro-R 是基于各个二分类问题的TP、FP、TN、FN 计算而得的；\\n“宏”可', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 19, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='“宏”可\\n以认为是只关注宏观而不看具体细节，而“微”可以认为是要从具体细节做起，因为相比于P 和R 指标\\n来说，TP、FP、TN、FN 更微观，毕竟P 和R 是基于TP、FP、TN、FN 计算而得。\\n→_→\\n配套视频教程：https://www.bilibili.com/video/BV1Mh411e7VU\\n←_←', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 19, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='→_→\\n欢迎去各大电商平台选购纸质版南瓜书《机器学习公式详解第2 版》\\n←_←\\n从“宏”和“微”的计算方式可以看出，\\n“宏”没有考虑每个类别下的的样本数量，所以平等看待每个\\n类别，因此会受到高P 和高R 类别的影响，而“微”则考虑到了每个类别的样本数量，因为样本数量多\\n的类相应的TP、FP、TN、FN 也会占比更多，所以在各类别样本数量极度不平衡的情况下，数量较多\\n的类别会主导最终结果。\\n式(2.14) 的macro-F1 是将macro-P 和macro-R 代入式(2.10) 所得；式(2.17) 的micro-F1 是将\\nmicro-P 和micro-R 代入式(2.10) 所得。值得一提的是，以上只是macro-F1 和micro-F1 的常用计算方\\n式之一，如若在查阅资料的过程中看到其他的计算方式也属正常。\\n2.3.7\\n式(2.18) 和式(2.19) 的解释\\n式(2.18) 定义了真正例率TPR。先解释公式中出现的真正例和假反例，真正例即实际为正例预测结\\n果也为正例，假反例即实际为正例但预测结果为反例，式(2.18) 分子为真正例，分母为真正例和假反例', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 20, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='果也为正例，假反例即实际为正例但预测结果为反例，式(2.18) 分子为真正例，分母为真正例和假反例\\n之和（即实际的正例个数）\\n，因此式(2.18) 的含义是所有正例当中有多大比例被预测为正例（即查全率\\nRecall）\\n。\\n式(2.19) 定义了假正例率FPR。先解释式子中出现的假正例和真反例，假正例即实际为反例但预测\\n结果为正例，真反例即实际为反例预测结果也为反例，式(2.19) 分子为假正例，分母为真反例和假正例之\\n和（即实际的反例个数）\\n，因此式(2.19) 的含义是所有反例当中有多大比例被预测为正例。\\n除了真正例率TPR 和假正例率FPR，还有真反例率TNR 和假反例率FNR：\\nTNR =\\nTN\\nFP + TN\\nFNR =\\nFN\\nTP + FN\\n2.3.8\\n式(2.20) 的推导\\n在推导式(2.20) 之前，需要先弄清楚ROC 曲线的具体绘制过程。下面我们就举个例子，按照“西瓜\\n书”图2.4 下方给出的绘制方法来讲解一下ROC 曲线的具体绘制过程。\\n假设我们已经训练得到一个学习器f(s)，现在用该学习器来对8 个测试样本（4 个正例，4 个反例，', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 20, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='假设我们已经训练得到一个学习器f(s)，现在用该学习器来对8 个测试样本（4 个正例，4 个反例，\\n即m+ = m−= 4）进行预测，预测结果为（此处用s 表示样本，以和坐标(x, y) 作出区分）：\\n(s1, 0.77, +), (s2, 0.62, −), (s3, 0.58, +), (s4, 0.47, +),\\n(s5, 0.47, −), (s6, 0.33, −), (s7, 0.23, +), (s8, 0.15, −)\\n其中，+ 和−分别表示样本为正例和为反例，数字表示学习器f 预测该样本为正例的概率，例如对\\n于反例s2 来说，当前学习器f(s) 预测它是正例的概率为0.62。\\n根据“西瓜书”上给出的绘制方法，首先需要对所有测试样本按照学习器给出的预测结果进行排\\n序（上面给出的预测结果已经按照预测值从大到小排序），接着将分类阈值设为一个不可能取到的超大\\n值，例如设为1。显然，此时所有样本预测为正例的概率都一定小于分类阈值，那么预测为正例的样本个\\n数为0，相应的真正例率和假正例率也都为0，所以我们可以在坐标(0, 0) 处标记一个点。接下来需要把', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 20, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='分类阈值从大到小依次设为每个样本的预测值，也就是依次设为0.77, 0.62, 0.58, 0.47, 0.33, 0.23,0.15，然\\n后分别计算真正例率和假正例率，再在相应的坐标上标记点，最后再将各个点用直线连接, 即可得到ROC\\n曲线。需要注意的是，在统计预测结果时，预测值等于分类阈值的样本也被算作预测为正例。例如，当分\\n类阈值为0.77 时，测试样本s1 被预测为正例，由于它的真实标记也是正例，所以此时s1 是一个真正例。\\n为了便于绘图，我们将x 轴（假正例率轴）的“步长”定为\\n1\\nm−，y 轴（真正例率轴）的“步长”定为\\n1\\nm+ 。\\n根据真正例率和假正例率的定义可知，每次变动分类阈值时，若新增i 个假正例，那么相应的x 轴坐标也\\n就增加\\ni\\nm−；若新增j 个真正例，那么相应的y 轴坐标也就增加\\nj\\nm+ 。按照以上讲述的绘制流程，最终我\\n们可以绘制出如图2-1所示的ROC 曲线。\\n→_→\\n配套视频教程：https://www.bilibili.com/video/BV1Mh411e7VU\\n←_←', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 20, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='→_→\\n欢迎去各大电商平台选购纸质版南瓜书《机器学习公式详解第2 版》\\n←_←\\n图2-1 ROC 曲线示意\\n在这里，为了能在解释式(2.21) 时复用此图，我们没有写上具体的数值，转而用其数学符号代替。其\\n中绿色线段表示在分类阈值变动的过程中只新增了真正例，红色线段表示只新增了假正例，蓝色线段表示\\n既新增了真正例也新增了假正例。根据AUC 值的定义可知，此时的AUC 值其实就是所有红色线段和蓝\\n色线段与x 轴围成的面积之和。观察图2-1可知，红色线段与x 轴围成的图形恒为矩形，蓝色线段与x 轴\\n围成的图形恒为梯形。由于梯形面积式既能算梯形面积，也能算矩形面积，所以无论是红色线段还是蓝色\\n线段，其与x 轴围成的面积都能用梯形公式来计算：\\n1\\n2 · (xi+1 −xi) · (yi + yi+1)\\n其中，(xi+1 −xi) 为“高”\\n，yi 为“上底”\\n，yi+1 为“下底”\\n。那么对所有红色线段和蓝色线段与x 轴围成\\n的面积进行求和，则有\\nm−1\\nX\\ni=1\\n\\x141\\n2 · (xi+1 −xi) · (yi + yi+1)\\n\\x15\\n此即为AUC。', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 21, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='X\\ni=1\\n\\x141\\n2 · (xi+1 −xi) · (yi + yi+1)\\n\\x15\\n此即为AUC。\\n通过以上ROC 曲线的绘制流程可以看出，ROC 曲线上每一个点都表示学习器f(s) 在特定阈值下构\\n成的一个二分类器，越好的二分类器其假正例率（反例被预测错误的概率，横轴）越小，真正例率（正例\\n被预测正确的概率，纵轴）越大，所以这个点越靠左上角（即点(0, 1)）越好。因此，越好的学习器，其\\nROC 曲线上的点越靠左上角，相应的ROC 曲线下的面积也越大，即AUC 也越大。\\n2.3.9\\n式(2.21) 和式(2.22) 的推导\\n下面针对“西瓜书”上所说的“ℓrank 对应的是ROC 曲线之上的面积”进行推导。按照我们上述对式\\n(2.20) 的推导思路，ℓrank 可以看作是所有绿色线段和蓝色线段与y 轴围成的面积之和，但从式(2.21) 中\\n很难一眼看出其面积的具体计算方式，因此我们进行恒等变形如下：\\nℓrank =\\n1\\nm+m−\\nX\\nx+∈D+\\nX\\nx−∈D−\\n\\x12\\nI\\n\\x00f(x+) < f(x−)\\n\\x01\\n+ 1\\n2I\\n\\x00f(x+) = f(x−)\\n\\x01\\x13\\n=\\n1\\nm+m−\\nX\\nx+∈D+', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 21, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='\\x01\\n+ 1\\n2I\\n\\x00f(x+) = f(x−)\\n\\x01\\x13\\n=\\n1\\nm+m−\\nX\\nx+∈D+\\n\" X\\nx−∈D−\\nI\\n\\x00f(x+) < f(x−)\\n\\x01\\n+ 1\\n2 ·\\nX\\nx−∈D−\\nI\\n\\x00f(x+) = f(x−)\\n\\x01\\n#\\n=\\nX\\nx+∈D+\\n\"\\n1\\nm+ ·\\n1\\nm−\\nX\\nx−∈D−\\nI\\n\\x00f(x+) < f(x−)\\n\\x01\\n+ 1\\n2 ·\\n1\\nm+ ·\\n1\\nm−\\nX\\nx−∈D−\\nI\\n\\x00f(x+) = f(x−)\\n\\x01\\n#\\n=\\nX\\nx+∈D+\\n1\\n2 ·\\n1\\nm+ ·\\n\"\\n2\\nm−\\nX\\nx−∈D−\\nI\\n\\x00f(x+) < f(x−)\\n\\x01\\n+\\n1\\nm−\\nX\\nx−∈D−\\nI\\n\\x00f(x+) = f(x−)\\n\\x01\\n#\\n在变动分类阈值的过程当中，如果有新增真正例，那么图2-1就会相应地增加一条绿色线段或蓝色线\\n段，所以上式中的P\\nx+∈D+ 可以看作是在累加所有绿色和蓝色线段，相应地，P\\nx+∈D+ 后面的内容便是\\n→_→\\n配套视频教程：https://www.bilibili.com/video/BV1Mh411e7VU\\n←_←', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 21, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='→_→\\n欢迎去各大电商平台选购纸质版南瓜书《机器学习公式详解第2 版》\\n←_←\\n在求绿色线段或者蓝色线段与y 轴围成的面积，即：\\n1\\n2 ·\\n1\\nm+ ·\\n\"\\n2\\nm−\\nX\\nx−∈D−\\nI\\n\\x00f(x+) < f(x−)\\n\\x01\\n+\\n1\\nm−\\nX\\nx−∈D−\\nI\\n\\x00f(x+) = f(x−)\\n\\x01\\n#\\n与式(2.20) 中的推导思路相同，不论是绿色线段还是蓝色线段，其与y 轴围成的图形面积都可以用\\n梯形公式来进行计算，所以上式表示的依旧是一个梯形的面积公式。其中\\n1\\nm+ 即梯形的“高”\\n，中括号内\\n便是“上底+ 下底”\\n，下面我们来分别推导一下“上底”\\n（较短的底）和“下底”\\n（较长的底）\\n。\\n由于在绘制ROC 曲线的过程中，每新增一个假正例时x 坐标也就新增一个步长，所以对于“上底”\\n，\\n也就是绿色或者蓝色线段的下端点到y 轴的距离，长度就等于\\n1\\nm−乘以预测值大于f(x+) 的假正例的个\\n数，即\\n1\\nm−\\nX\\nx−∈D−\\nI\\n\\x00f(x+) < f(x−)\\n\\x01\\n而对于“下底”\\n，长度就等于\\n1\\nm−乘以预测值大于等于f(x+) 的假正例的个数，即\\n1\\nm−\\n X\\nx−∈D−\\nI', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 22, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='1\\nm−乘以预测值大于等于f(x+) 的假正例的个数，即\\n1\\nm−\\n X\\nx−∈D−\\nI\\n\\x00f(x+) < f(x−)\\n\\x01\\n+\\nX\\nx−∈D−\\nI\\n\\x00f(x+) = f(x−)\\n\\x01\\n!\\n到此，推导完毕。\\n若不考虑f(x+) = f(x−)，\\n从直观上理解ℓrank，\\n其表示的是：\\n对于待测试的模型f(x)，\\n从测试集中随\\n机抽取一个正反例对儿{x+, x−}，模型f(x) 对正例的打分f(x+) 小于对反例的打分f(x−) 的概率，即\\n“排序错误”的概率。推导思路如下：采用频率近似概率的思路，组合出测试集中的所有正反例对儿，假设\\n组合出来的正反例对儿的个数为m，用模型f(x) 对所有正反例对儿打分并统计“排序错误”的正反例对\\n儿个数n，然后计算出\\nn\\nm 即为模型f(x)“排序错误”的正反例对儿的占比，其可近似看作为f(x) 在测\\n试集上“排序错误”的概率。具体推导过程如下：测试集中的所有正反例对儿的个数为\\nm+ × m−\\n“排序错误”的正反例对儿个数为\\nX\\nx+∈D+\\nX\\nx−∈D−\\n\\x00I\\n\\x00f(x+) < f(x−)\\n\\x01\\x01\\n因此，\\n“排序错误”的概率为\\nP\\nx+∈D+\\nP', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 22, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='\\x00I\\n\\x00f(x+) < f(x−)\\n\\x01\\x01\\n因此，\\n“排序错误”的概率为\\nP\\nx+∈D+\\nP\\nx−∈D−(I (f(x+) < f(x−)))\\nm+ × m−\\n若再考虑f(x+) = f(x−) 时算半个“排序错误”\\n，则上式可进一步扩展为\\nP\\nx+∈D+\\nP\\nx−∈D−\\n\\x00I\\n\\x00f(x+) < f(x−) + 1\\n2I (f(x+) = f(x−))\\n\\x01\\x01\\nm+ × m−\\n此即为ℓrank。\\n如果说ℓrank 指的是从测试集中随机抽取正反例对儿，模型f(x)“排序错误”的概率，那么根据式\\n(2.22) 可知，AUC 则指的是从测试集中随机抽取正反例对儿，模型f(x)“排序正确”的概率。显然，此\\n概率越大越好。\\n2.3.10\\n式(2.23) 的解释\\n本公式很容易理解，只是需要注意该公式上方交代了“若将表2.2 中的第0 类作为正类、第1 类作为\\n反类”\\n，若不注意此条件，按习惯（0 为反类、1 为正类）会产生误解。为避免产生误解，在接下来的解释\\n→_→\\n配套视频教程：https://www.bilibili.com/video/BV1Mh411e7VU\\n←_←', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 22, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='→_→\\n欢迎去各大电商平台选购纸质版南瓜书《机器学习公式详解第2 版》\\n←_←\\n中将cost01 记为cost+−，cost10 记为cost−+。本公式还可以作如下恒等变形\\nE(f; D; cost) = 1\\nm\\n\\uf8eb\\n\\uf8edm+ ×\\n1\\nm+\\nX\\nxi∈D+\\nI (f (xi ̸= yi)) × cost+−+ m−×\\n1\\nm−\\nX\\nxi∈D−\\nI (f (xi ̸= yi)) × cost−+\\n\\uf8f6\\n\\uf8f8\\n= m+\\nm ×\\n1\\nm+\\nX\\nxi∈D+\\nI (f (xi ̸= yi)) × cost+−+ m−\\nm ×\\n1\\nm−\\nX\\nxi∈D−\\nI (f (xi ̸= yi)) × cost−+\\n其中m+ 和m−分别表示正例集D+ 和反例集D−的样本个数。\\n1\\nm+\\nP\\nxi∈D+ I (f (xi ̸= yi)) 表示正例集D+ 中预测错误样本所占比例，即假反例率FNR。\\n1\\nm−\\nP\\nxi∈D−I (f (xi ̸= yi)) 表示反例集D−中预测错误样本所占比例，即假正例率FPR。\\nm+\\nm 表示样例集D 中正例所占比例，或理解为随机从D 中取一个样例取到正例的概率。\\nm−', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 23, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='m+\\nm 表示样例集D 中正例所占比例，或理解为随机从D 中取一个样例取到正例的概率。\\nm−\\nm 表示样例集D 中反例所占比例，或理解为随机从D 中取一个样例取到反例的概率。\\n因此，若将样例为正例的概率m+\\nm 记为p，则样例为f 反例的概率m−\\nm 为1 −p，上式可进一步写为\\nE(f; D; cost) = p × FNR × cost+−+ (1 −p) × FPR × cost−+\\n此公式在接下来式(2.25) 的解释中会用到。\\n2.3.11\\n式(2.24) 的解释\\n当cost+−= cost−+ 时，本公式可化简为\\nP(+)cost =\\np\\np + (1 −p) = p\\n其中p 是样例为正例的概率\\n（一般用正例在样例集中所占的比例近似代替）\\n。\\n因此，\\n当代价不敏感时\\n（也即\\ncost+−= cost−+）\\n，P(+)cost 就是正例在样例集中的占比。那么，当代价敏感时（也即cost+−̸= cost−+）\\n，\\nP(+)cost 即为正例在样例集中的加权占比。具体来说，对于样例集\\nD =\\n\\x08\\nx+\\n1 , x+\\n2 , x−\\n3 , x−\\n4 , x−\\n5 , x−', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 23, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='D =\\n\\x08\\nx+\\n1 , x+\\n2 , x−\\n3 , x−\\n4 , x−\\n5 , x−\\n6 , x−\\n7 , x−\\n8 , x−\\n9 , x−\\n10\\n\\t\\n其中x+ 表示正例，\\nx−表示反例。\\n可以看出p = 0.2，\\n若想让正例得到更多重视，\\n考虑代价敏感cost+−= 4\\n和cost−+ = 1，这实际等价于在以下样例集上进行代价不敏感的正例概率代价计算\\nD′ =\\n\\x08\\nx+\\n1 , x+\\n1 , x+\\n1 , x+\\n1 , x+\\n2 , x+\\n2 , x+\\n2 , x+\\n2 , x−\\n3 , x−\\n4 , x−\\n5 , x−\\n6 , x−\\n7 , x−\\n8 , x−\\n9 , x−\\n10\\n\\t\\n即将每个正例样本复制4 份，若有1 个出错，则有4 个一起出错，代价为4。此时可计算出\\nP(+)cost =\\np × cost+−\\np × cost+−+ (1 −p) × cost−+\\n=\\n0.2 × 4\\n0.2 × 4 + (1 −0.2) × 1 = 0.5\\n也就是正例在等价的样例集D′ 中的占比。所以，无论代价敏感还是不敏感，P(+)cost 本质上表示的都是', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 23, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='样例集中正例的占比。在实际应用过程中，如果由于某种原因无法将cost+−和cost−+ 设为不同取值，可\\n以采用上述“复制样本”的方法间接实现将cost+−和cost−+ 设为不同取值。\\n对于不同的cost+−和cost−+ 取值，若二者的比值保持相同，则P(+)cost 不变。例如，对于上面的\\n例子，若设cost+−= 40 和cost−+ = 10，所得P(+)cost 仍为0.5。\\n此外，根据此式还可以相应地推导出反例概率代价\\nP(−)cost = 1 −P(+)cost =\\n(1 −p) × cost−+\\np × cost+−+ (1 −p) × cost−+\\n→_→\\n配套视频教程：https://www.bilibili.com/video/BV1Mh411e7VU\\n←_←', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 23, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='→_→\\n欢迎去各大电商平台选购纸质版南瓜书《机器学习公式详解第2 版》\\n←_←\\n2.3.12\\n式(2.25) 的解释\\n对于包含m 个样本的样例集D，可以算出学习器f(x) 总的代价是\\ncostse = m × p × FNR × cost+−+ m × (1 −p) × FPR × cost−+\\n+ m × p × TPR × cost++ + m × (1 −p) × TNR × cost−−\\n其中p 是正例在样例集中所占的比例（或严格地称为样例为正例的概率）\\n，costse 下标中的“se”表示\\nsensitive，即代价敏感，根据前面讲述的FNR、FPR、TPR、TNR 的定义可知：\\nm × p × FNR 表示正例被预测为反例（正例预测错误）的样本个数；\\nm × (1 −p) × FPR 表示反例被预测为正例（反例预测错误）的样本个数；\\nm × p × TPR 表示正例被预测为正例（正例预测正确）的样本个数；\\nm × (1 −p) × TNR 表示反例预测为反例（反例预测正确）的样本个数。', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 24, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='m × (1 −p) × TNR 表示反例预测为反例（反例预测正确）的样本个数。\\n以上各种样本个数乘以相应的代价则得到总的代价costse。但是，按照此公式计算出的代价与样本个\\n数m 呈正比，\\n显然不具有一般性，\\n因此需要除以样本个数m，\\n而且一般来说，\\n预测出错才会产生代价，\\n预\\n测正确则没有代价，也即cost++ = cost−−= 0，所以costse 更为一般化的表达式为\\ncostse = p × FNR × cost+−+ (1 −p) × FPR × cost−+\\n回顾式(2.23) 的解释可知，此式即为式(2.23) 的恒等变形，所以此式可以同式(2.23) 一样理解为学习器\\nf(x) 在样例集D 上的“代价敏感错误率”\\n。显然，costse 的取值范围并不在0 到1 之间，且costse 在\\nFNR = FPR = 1 时取到最大值，因为FNR = FPR = 1 时表示所有正例均被预测为反例，反例均被预测\\n为正例，代价达到最大，即\\nmax(costse) = p × cost+−+ (1 −p) × cost−+', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 24, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='max(costse) = p × cost+−+ (1 −p) × cost−+\\n所以，如果要将costse 的取值范围归一化到0 到1 之间，则只需将其除以其所能取到的最大值即可，也即\\ncostse\\nmax(costse) = p × FNR × cost+−+ (1 −p) × FPR × cost−+\\np × cost+−+ (1 −p) × cost−+\\n此即为式(2.25)，也即为costnorm，其中下标“norm”表示normalization。\\n进一步地，根据式(2.24) 中P(+)cost 的定义可知，式(2.25) 可以恒等变形为\\ncostnorm = FNR × P(+)cost + FPR × (1 −P(+)cost)\\n对于二维直角坐标系中的两个点(0, B) 和(1, A) 以及实数p ∈[0, 1]，\\n(p, pA+(1−p)B) 一定是线段A−B\\n上的点，且当p 从0 变到1 时，点(p, pA + (1 −p)B) 的轨迹为从(0, B) 到(1, A)，基于此，结合上述', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 24, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='costnorm 的表达式可知：(P(+)cost, costnorm) 即为线段FPR −FNR 上的点，当(P(+)cost 从0 变到1\\n时，(P(+)cost, costnorm) 的轨迹为从(0, FPR) 到(1, FNR) ，也即图2.5 中的各条线段。需要注意的是，\\n以上只是从数学逻辑自洽的角度对图2.5 中的各条线段进行解释，实际中各条线段并非按照上述方法绘\\n制。理由如下：\\nP(+)cost 表示的是样例集中正例的占比，而在进行学习器的比较时，变动的只是训练学习器的算法\\n或者算法的超参数，用来评估学习器性能的样例集是固定的（单一变量原则）\\n，所以P(+)cost 是一个固\\n定值，因此图2.5 中的各条线段并不是通过变动P(+)cost 然后计算costnorm 画出来的，而是按照“西瓜\\n书”上式(2.25) 下方所说对ROC 曲线上每一点计算FPR 和FNR，然后将点(0, FPR) 和点(1, FNR) 直\\n接连成线段。\\n虽然图2.5 中的各条线段并不是通过变动横轴表示的P(+)cost 来进行绘制，但是横轴仍然有其他用', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 24, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='处，例如用来找使学习器的归一化代价costnorm 达到最小的阈值（暂且称其为最佳阈值）\\n。具体地，首先\\n计算当前样例集的P(+)cost 值，然后根据计算出来的值在横轴上标记出具体的点，再基于该点作一条垂\\n→_→\\n配套视频教程：https://www.bilibili.com/video/BV1Mh411e7VU\\n←_←', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 24, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='→_→\\n欢迎去各大电商平台选购纸质版南瓜书《机器学习公式详解第2 版》\\n←_←\\n直于横轴的垂线，与该垂线最先相交（从下往上看）的线段所对应的阈值（因为每条线段都对应ROC 曲\\n线上的点，\\nROC 曲线上的点又对应着具体的阈值）\\n即为最佳阈值。\\n原因是与该垂线最先相交的线段必然最\\n靠下，因此其交点的纵坐标最小，而纵轴表示的便是归一化代价costnorm，所以此时归一化代价costnorm\\n达到最小。特别地，当P(+)cost = 0 时，即样例集中没有正例，全是负例，因此最佳阈值应该是学习器\\n不可能取到的最大值，且按照此阈值计算出来出来的FPR = 0, FNR = 1, costnorm = 0。那么按照上述作\\n垂线的方法去图2.5 中进行实验，也即在横轴0 刻度处作垂线，显然与该垂线最先相交的线段是点(0, 0)\\n和点(1, 1) 连成的线段，交点为(0, 0)，此时对应的也为FPR = 0, FNR = 1, costnorm = 0，且该条线段所\\n对应的阈值也确实为“学习器不可能取到的最大值”\\n（因为该线段对应的是ROC 曲线中的起始点）\\n。\\n2.4\\n比较检验', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 25, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='（因为该线段对应的是ROC 曲线中的起始点）\\n。\\n2.4\\n比较检验\\n为什么要做比较检验？\\n“西瓜书”在本节开篇的两段话已经交代原由。简单来说，从统计学的角度，取\\n得的性能度量的值本质上仍是一个随机变量，因此并不能简单用比较大小来直接判定算法（或者模型）之\\n间的优劣，而需要更置信的方法来进行判定。\\n在此说明一下，如果不做算法理论研究，也不需要对算法（或模型）之间的优劣给出严谨的数学分析，\\n本节可以暂时跳过。本节主要使用的数学知识是“统计假设检验”\\n，该知识点在各个高校的概率论与数理\\n统计教材（例如参考文献[1]）上均有讲解。此外，有关检验变量的公式，例如式(2.30) 至式(2.36)，并不\\n需要清楚是怎么来的（这是统计学家要做的事情）\\n，只需要会用即可。\\n2.4.1\\n式(2.26) 的解释\\n理解本公式时需要明确的是：ϵ 是未知的，是当前希望估算出来的，ˆ\\nϵ 是已知的，是已经用m 个测试\\n样本对学习器进行测试得到的。因此，本公式也可理解为：当学习器的泛化错误率为ϵ 时，被测得测试错\\n误率为ˆ\\nϵ 的条件概率。所以本公式可以改写为\\nP(ˆ\\nϵ|ϵ) =\\n \\nm\\nˆ\\nϵ × m\\n!\\nϵˆ', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 25, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='ϵ 的条件概率。所以本公式可以改写为\\nP(ˆ\\nϵ|ϵ) =\\n \\nm\\nˆ\\nϵ × m\\n!\\nϵˆ\\nϵ×m(1 −ϵ)m−ˆ\\nϵ×m\\n其中\\n \\nm\\nˆ\\nϵ × m\\n!\\n=\\nm!\\n(ˆ\\nϵ × m)!(m −ˆ\\nϵ × m)!\\n为中学时学的组合数，即Cˆ\\nϵ×m\\nm\\n。\\n在已知ˆ\\nε 时，求使得条件概率P(ˆ\\nϵ|ϵ) 达到最大的ϵ 是概率论与数理统计中经典的极大似然估计问题。\\n从极大似然估计的角度可知，由于ˆ\\nϵ, m 均为已知量，所以P(ˆ\\nϵ|ϵ) 可以看作为一个关于ϵ 的函数，称为似\\n然函数，于是问题转化为求使得似然函数取到最大值的ϵ，即\\nϵ = arg max\\nϵ\\nP(ˆ\\nϵ|ϵ)\\n首先对ϵ 求一阶导数\\n∂P(ˆ\\nϵ | ϵ)\\n∂ϵ\\n=\\n \\nm\\nˆ\\nϵ × m\\n!\\n∂ϵˆ\\nϵ×m(1 −ϵ)m−ˆ\\nϵ×m\\n∂ϵ\\n=\\n \\nm\\nˆ\\nϵ × m\\n!\\n\\x00ˆ\\nϵ × m × ϵˆ\\nϵ×m−1(1 −ϵ)m−ˆ\\nϵ×m + ϵˆ\\nϵ×m × (m −ˆ\\nϵ × m) × (1 −ϵ)m−ˆ\\nϵ×m−1 × (−1)\\n\\x01\\n=\\n \\nm\\nˆ\\nϵ × m\\n!\\nϵˆ\\nϵ×m−1(1 −ϵ)m−ˆ', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 25, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='ϵ×m−1 × (−1)\\n\\x01\\n=\\n \\nm\\nˆ\\nϵ × m\\n!\\nϵˆ\\nϵ×m−1(1 −ϵ)m−ˆ\\nϵ×m−1(ˆ\\nϵ × m × (1 −ϵ) −ϵ × (m −ˆ\\nϵ × m))\\n=\\n \\nm\\nˆ\\nϵ × m\\n!\\nϵˆ\\nϵ×m−1(1 −ϵ)m−ˆ\\nϵ×m−1(ˆ\\nϵ × m −ϵ × m)\\n→_→\\n配套视频教程：https://www.bilibili.com/video/BV1Mh411e7VU\\n←_←', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 25, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='→_→\\n欢迎去各大电商平台选购纸质版南瓜书《机器学习公式详解第2 版》\\n←_←\\n分析上式可知，\\n其中\\n \\nm\\nˆ\\nϵ × m\\n!\\n为常数，\\n由于ϵ ∈[0, 1]，\\n所以ϵˆ\\nϵ×m−1(1−ϵ)m−ˆ\\nϵ×m−1 恒大于0，\\n(ˆ\\nϵ×m−ϵ×m)\\n在0 ⩽ϵ < ˆ\\nϵ 时大于0，在ϵ = ˆ\\nϵ 时等于0，在ˆ\\nϵ ⩽ϵ < 1 时小于0，因此P(ˆ\\nϵ | ϵ) 是关于ϵ 开口向下的凹\\n函数（此处采用的是最优化中对凹凸函数的定义，\\n“西瓜书”第3 章3.2 节左侧边注对凹凸函数的定义也\\n是如此）\\n。所以，当且仅当一阶导数∂P (ˆ\\nϵ|ϵ)\\n∂ϵ\\n= 0 时P(ˆ\\nϵ | ϵ) 取到最大值，此时ϵ = ˆ\\nϵ。\\n2.4.2\\n式(2.27) 的推导\\n截至2021 年5 月，\\n“西瓜书”第1 版第36 次印刷，式(2.27) 应当勘误为\\nϵ = min ϵ\\ns.t.\\nm\\nX\\ni=ϵ×m+1\\n \\nm\\ni\\n!\\nϵi\\n0(1 −ϵ0)m−i < α\\n在推导此公式之前，先铺垫讲解一下“二项分布参数p 的假设检验”\\n[1]：', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 26, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='在推导此公式之前，先铺垫讲解一下“二项分布参数p 的假设检验”\\n[1]：\\n设某事件发生的概率为p，p 未知。做m 次独立试验，每次观察该事件是否发生，以X 记该事件发\\n生的次数，则X 服从二项分布B(m, p)，现根据X 检验如下假设：\\nH0 : p ⩽p0\\nH1 : p > p0\\n由二项分布本身的特性可知：p 越小，X 取到较小值的概率越大。因此，对于上述假设，一个直观上\\n合理的检验为\\nφ : 当X > C时拒绝H0, 否则就接受H0。\\n其中，C 表示事件最大发生次数。此检验对应的功效函数为\\nβφ(p) = P(X > C)\\n= 1 −P(X ⩽C)\\n= 1 −\\nC\\nX\\ni=0\\n \\nm\\ni\\n!\\npi(1 −p)m−i\\n=\\nm\\nX\\ni=C+1\\n \\nm\\ni\\n!\\npi(1 −p)m−i\\n由于“p 越小，X 取到较小值的概率越大”可以等价表示为：P(X ⩽C) 是关于p 的减函数，所以\\nβφ(p) = P(X > C) = 1 −P(X ⩽C) 是关于p 的增函数，那么当p ⩽p0 时，βφ(p0) 即为βφ(p) 的上确界。', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 26, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='（更为严格的数学证明参见参考文献[1] 中第2 章习题7）又根据参考文献[1] 中5.1.3 的定义1.2 可知，\\n在给定检验水平α 时，要想使得检验φ 达到水平α，则必须保证βφ(p) ⩽α，因此可以通过如下方程解得\\n使检验φ 达到水平α 的整数C：\\nα = sup {βφ(p)}\\n显然，当p ⩽p0 时有\\nα = sup {βφ(p)}\\n= βφ(p0)\\n=\\nm\\nX\\ni=C+1\\n \\nm\\ni\\n!\\npi\\n0(1 −p0)m−i\\n对于此方程，通常不一定正好解得一个使得方程成立的整数C，较常见的情况是存在这样一个C 使得\\nm\\nX\\ni=C+1\\n \\nm\\ni\\n!\\npi\\n0(1 −p0)m−i < α\\nm\\nX\\ni=C\\n \\nm\\ni\\n!\\npi\\n0(1 −p0)m−i > α\\n→_→\\n配套视频教程：https://www.bilibili.com/video/BV1Mh411e7VU\\n←_←', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 26, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='→_→\\n欢迎去各大电商平台选购纸质版南瓜书《机器学习公式详解第2 版》\\n←_←\\n此时，C 只能取C 或者C + 1。若C 取C，则相当于升高了检验水平α；若C 取C + 1 则相当于降低了\\n检验水平α。具体如何取舍需要结合实际情况，一般的做法是使α 尽可能小，因此倾向于令C 取C + 1。\\n下面考虑如何求解C。易证βφ(p0) 是关于C 的减函数，再结合上述关于C 的两个不等式易推得\\nC = min C\\ns.t.\\nm\\nX\\ni=C+1\\n \\nm\\ni\\n!\\npi\\n0(1 −p0)m−i < α\\n由“西瓜书”中的上下文可知，对ϵ ⩽ϵ0 进行假设检验，等价于“二项分布参数p 的假设检验”中所\\n述的对p ⩽p0 进行假设检验，所以在“西瓜书”中求解最大错误率ϵ 等价于在“二项分布参数p 的假设\\n检验”中求解事件最大发生频率C\\nm。由上述“二项分布参数p 的假设检验”中的推导可知\\nC = min C\\ns.t.\\nm\\nX\\ni=C+1\\n \\nm\\ni\\n!\\npi\\n0(1 −p0)m−i < α\\n所以\\nC\\nm = min C\\nm\\ns.t.\\nm\\nX\\ni=C+1\\n \\nm\\ni\\n!\\npi\\n0(1 −p0)m−i < α', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 27, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='m\\ns.t.\\nm\\nX\\ni=C+1\\n \\nm\\ni\\n!\\npi\\n0(1 −p0)m−i < α\\n将上式中的C\\nm, C\\nm, p0 等价替换为ϵ, ϵ, ϵ0 可得\\nϵ = min ϵ\\ns.t.\\nm\\nX\\ni=ϵ×m+1\\n \\nm\\ni\\n!\\nϵi\\n0(1 −ϵ0)m−i < α\\n2.5\\n偏差与方差\\n2.5.1\\n式(2.37) 到式(2.42) 的推导\\n首先，梳理一下“西瓜书”中的符号，书中称x 为测试样本，但是书中又提到“令yD 为x 在数据集\\n中的标记”\\n，那么x 究竟是测试集中的样本还是训练集中的样本呢？这里暂且理解为x 为从训练集中抽取\\n出来用于测试的样本。此外，\\n“西瓜书”中左侧边注中提到“有可能出现噪声使得yD ̸= y”\\n，其中所说的\\n“噪声”通常是指人工标注数据时带来的误差，例如标注“身高”时，由于测量工具的精度等问题，测出来\\n的数值必然与真实的“身高”之间存在一定误差，此即为“噪声”\\n。\\n为了进一步解释式(2.37)、(2.38) 和(2.39)，在这里设有n 个训练集D1, ..., Dn，这n 个训练集都是', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 27, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='以独立同分布的方式从样本空间中采样而得，并且恰好都包含测试样本x，该样本在这n 个训练集的标记\\n分别为yD1, ..., yDn。书中已明确，此处以回归任务为例，也即yD, y, f(x; D) 均为实值。\\n式(2.37) 可理解为：\\n¯\\nf(x) = ED[f(x; D)] = 1\\nn (f (x; D1) + . . . + f (x; Dn))\\n式(2.38) 可理解为：\\nvar(x) = ED\\n\\x02\\n(f(x; D) −¯\\nf(x))2\\x03\\n= 1\\nn\\n\\x10\\x00f (x; D1) −¯\\nf(x)\\n\\x012 + . . . +\\n\\x00f (x; Dn) −¯\\nf(x)\\n\\x012\\x11\\n式(2.39) 可理解为：\\nε2 = ED\\nh\\n(yD −y)2i\\n= 1\\nn\\n\\x10\\n(yD1 −y)2 + . . . + (yDn −y)2\\x11\\n最后，推导一下式(2.41) 和式(2.42)，由于推导完式(2.41) 自然就会得到式(2.42)，因此下面仅推导\\n→_→\\n配套视频教程：https://www.bilibili.com/video/BV1Mh411e7VU\\n←_←', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 27, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='→_→\\n欢迎去各大电商平台选购纸质版南瓜书《机器学习公式详解第2 版》\\n←_←\\n式(2.41) 即可。\\nE(f; D) =ED\\nh\\n(f(x; D) −yD)2i\\n1\\n=ED\\nh\\x00f(x; D) −¯\\nf(x) + ¯\\nf(x) −yD\\n\\x012i\\n2\\n=ED\\nh\\x00f(x; D) −¯\\nf(x)\\n\\x012i\\n+ ED\\nh\\x00 ¯\\nf(x) −yD\\n\\x012i\\n+\\nED\\n\\x02\\n2\\n\\x00f(x; D) −¯\\nf(x)\\n\\x01 \\x00 ¯\\nf(x) −yD\\n\\x01\\x03\\n3\\n=ED\\nh\\x00f(x; D) −¯\\nf(x)\\n\\x012i\\n+ ED\\nh\\x00 ¯\\nf(x) −yD\\n\\x012i\\n4\\n=ED\\nh\\x00f(x; D) −¯\\nf(x)\\n\\x012i\\n+ ED\\nh\\x00 ¯\\nf(x) −y + y −yD\\n\\x012i\\n5\\n=ED\\nh\\x00f(x; D) −¯\\nf(x)\\n\\x012i\\n+ ED\\nh\\x00 ¯\\nf(x) −y\\n\\x012i\\n+ ED\\nh\\n(y −yD)2i\\n+\\n2ED\\n\\x02\\x00 ¯\\nf(x) −y\\n\\x01\\n(y −yD)\\n\\x03\\n6\\n=ED\\nh\\x00f(x; D) −¯\\nf(x)\\n\\x012i\\n+\\n\\x00 ¯\\nf(x) −y\\n\\x012 + ED\\nh\\n(yD −y)2i\\n7', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 28, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='f(x)\\n\\x012i\\n+\\n\\x00 ¯\\nf(x) −y\\n\\x012 + ED\\nh\\n(yD −y)2i\\n7\\n上式即为式(2.41)，下面给出每一步的推导过程：\\n1 →2 ：减一个¯\\nf(x) 再加一个¯\\nf(x)，属于简单的恒等变形。\\n2 →3 ：首先将中括号内的式子展开，有\\nED\\nh\\x00f(x; D) −¯\\nf(x)\\n\\x012 +\\n\\x00 ¯\\nf(x) −yD\\n\\x012 + 2\\n\\x00f(x; D) −¯\\nf(x)\\n\\x01 \\x00 ¯\\nf(x) −yD\\n\\x01i\\n然后根据期望的运算性质E[X + Y ] = E[X] + E[Y ] 可将上式化为\\nED\\nh\\x00f(x; D) −¯\\nf(x)\\n\\x012i\\n+ ED\\nh\\x00 ¯\\nf(x) −yD\\n\\x012i\\n+ ED\\n\\x02\\n2\\n\\x00f(x; D) −¯\\nf(x)\\n\\x01 \\x00 ¯\\nf(x) −yD\\n\\x01\\x03\\n3 →4 ：再次利用期望的运算性质将3 的最后一项展开，有\\nED\\n\\x02\\n2\\n\\x00f(x; D) −¯\\nf(x)\\n\\x01 \\x00 ¯\\nf(x) −yD\\n\\x01\\x03\\n= ED\\n\\x02\\n2\\n\\x00f(x; D) −¯\\nf(x)\\n\\x01\\n· ¯\\nf(x)\\n\\x03\\n−ED\\n\\x02\\n2\\n\\x00f(x; D) −¯\\nf(x)\\n\\x01\\n· yD\\n\\x03', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 28, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='\\x01\\n· ¯\\nf(x)\\n\\x03\\n−ED\\n\\x02\\n2\\n\\x00f(x; D) −¯\\nf(x)\\n\\x01\\n· yD\\n\\x03\\n首先计算展开后得到的第1 项，有\\nED\\n\\x02\\n2\\n\\x00f(x; D) −¯\\nf(x)\\n\\x01\\n· ¯\\nf(x)\\n\\x03\\n= ED\\n\\x02\\n2f(x; D) · ¯\\nf(x) −2 ¯\\nf(x) · ¯\\nf(x)\\n\\x03\\n由于¯\\nf(x) 是常量，所以由期望的运算性质：E[AX + B] = AE[X] + B（其中A, B 均为常量）可得\\nED\\n\\x02\\n2\\n\\x00f(x; D) −¯\\nf(x)\\n\\x01\\n· ¯\\nf(x)\\n\\x03\\n= 2 ¯\\nf(x) · ED [f(x; D)] −2 ¯\\nf(x) · ¯\\nf(x)\\n由式(2.37) 可知ED [f(x; D)] = ¯\\nf(x)，所以\\nED\\n\\x02\\n2\\n\\x00f(x; D) −¯\\nf(x)\\n\\x01\\n· ¯\\nf(x)\\n\\x03\\n= 2 ¯\\nf(x) · ¯\\nf(x) −2 ¯\\nf(x) · ¯\\nf(x) = 0\\n接着计算展开后得到的第2 项\\nED\\n\\x02\\n2\\n\\x00f(x; D) −¯\\nf(x)\\n\\x01\\n· yD\\n\\x03\\n= 2ED [f(x; D) · yD] −2 ¯\\nf(x) · ED [yD]', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 28, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='\\x01\\n· yD\\n\\x03\\n= 2ED [f(x; D) · yD] −2 ¯\\nf(x) · ED [yD]\\n由于噪声和f 无关，所以f(x; D) 和yD 是两个相互独立的随机变量。根据期望的运算性质E[XY ] =\\nE[X]E[Y ]（其中X 和Y 为相互独立的随机变量）可得\\nED\\n\\x02\\n2\\n\\x00f(x; D) −¯\\nf(x)\\n\\x01\\n· yD\\n\\x03\\n= 2ED [f(x; D) · yD] −2 ¯\\nf(x) · ED [yD]\\n= 2ED [f(x; D)] · ED [yD] −2 ¯\\nf(x) · ED [yD]\\n= 2 ¯\\nf(x) · ED [yD] −2 ¯\\nf(x) · ED [yD]\\n= 0\\n→_→\\n配套视频教程：https://www.bilibili.com/video/BV1Mh411e7VU\\n←_←', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 28, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='→_→\\n欢迎去各大电商平台选购纸质版南瓜书《机器学习公式详解第2 版》\\n←_←\\n所以\\nED\\n\\x02\\n2\\n\\x00f(x; D) −¯\\nf(x)\\n\\x01 \\x00 ¯\\nf(x) −yD\\n\\x01\\x03\\n= ED\\n\\x02\\n2\\n\\x00f(x; D) −¯\\nf(x)\\n\\x01\\n· ¯\\nf(x)\\n\\x03\\n−ED\\n\\x02\\n2\\n\\x00f(x; D) −¯\\nf(x)\\n\\x01\\n· yD\\n\\x03\\n= 0 + 0\\n= 0\\n4 →5 ：同1 →2 一样，减一个y 再加一个y，属于简单的恒等变形。\\n5 →6 ：同2 →3 一样，将最后一项利用期望的运算性质进行展开。\\n6 →7 ：因为¯\\nf(x) 和y 均为常量，根据期望的运算性质，6 中的第2 项可化为\\nED\\nh\\x00 ¯\\nf(x) −y\\n\\x012i\\n=\\n\\x00 ¯\\nf(x) −y\\n\\x012\\n同理，6 中的最后一项可化为\\n2ED\\n\\x02\\x00 ¯\\nf(x) −y\\n\\x01\\n(y −yD)\\n\\x03\\n= 2\\n\\x00 ¯\\nf(x) −y\\n\\x01\\nED [(y −yD)]\\n由于此时假定噪声的期望为0，即ED [(y −yD)] = 0，所以\\n2ED\\n\\x02\\x00 ¯\\nf(x) −y\\n\\x01\\n(y −yD)\\n\\x03\\n= 2\\n\\x00 ¯\\nf(x) −y\\n\\x01\\n· 0 = 0\\n参考文献', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 29, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='\\x01\\n(y −yD)\\n\\x03\\n= 2\\n\\x00 ¯\\nf(x) −y\\n\\x01\\n· 0 = 0\\n参考文献\\n[1] 陈希孺. 概率论与数理统计. 中国科学技术大学出版社, 2009.\\n→_→\\n配套视频教程：https://www.bilibili.com/video/BV1Mh411e7VU\\n←_←', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 29, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='→_→\\n欢迎去各大电商平台选购纸质版南瓜书《机器学习公式详解第2 版》\\n←_←\\n第3 章\\n线性模型\\n作为“西瓜书”介绍机器学习模型的开篇，线性模型也是机器学习中最为基础的模型，很多复杂模型\\n均可认为由线性模型衍生而得，无论是曾经红极一时的支持向量机还是如今万众瞩目的神经网络，其中都\\n有线性模型的影子。\\n本章的线性回归和对数几率回归分别是回归和分类任务上常用的算法，因此属于重点内容，线性判别\\n分析不常用，但是其核心思路和后续第10 章将会讲到的经典降维算法主成分分析相同，因此也属于重点\\n内容，且两者结合在一起看理解会更深刻。\\n3.1\\n基本形式\\n第1 章的1.2 基本术语中讲述样本的定义时，我们说明了“西瓜书”和本书中向量的写法，当向量\\n中的元素用分号“;”分隔时表示此向量为列向量，用逗号“,”分隔时表示为行向量。因此，式(3.2) 中\\nw = (w1; w2; ...; wd) 和x = (x1; x2; ...; xd) 均为d 行1 列的列向量。\\n3.2\\n线性回归\\n3.2.1\\n属性数值化\\n为了能进行数学运算，\\n样本中的非数值类属性都需要进行数值化。\\n对于存在\\n“序”\\n关系的属性，', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 30, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='为了能进行数学运算，\\n样本中的非数值类属性都需要进行数值化。\\n对于存在\\n“序”\\n关系的属性，\\n可通过\\n连续化将其转化为带有相对大小关系的连续值；\\n对于不存在\\n“序”\\n关系的属性，\\n可根据属性取值将其拆解为\\n多个属性，\\n例如\\n“西瓜书”\\n中所说的\\n“瓜类”\\n属性，\\n可将其拆解为\\n“是否是西瓜”\\n、\\n“是否是南瓜”\\n、\\n“是否是黄\\n瓜”\\n3 个属性，\\n其中每个属性的取值为1 或0，\\n1 表示\\n“是”\\n，\\n0 表示\\n“否”\\n。\\n具体地，\\n假如现有3 个瓜类样本：\\nx1 = (甜度= 高; 瓜类= 西瓜), x2 = (甜度= 中; 瓜类= 南瓜), x3 = (甜度= 低; 瓜类= 黄瓜)，其中“甜\\n度”\\n属性存在序关系，\\n因此可将\\n“高”\\n、\\n“中”\\n、\\n“低”\\n转化为{1.0, 0.5, 0.0}，\\n“瓜类”\\n属性不存在序关系，\\n则按照上\\n述方法进行拆解，3 个瓜类样本数值化后的结果为：x1 = (1.0; 1; 0; 0), x1 = (0.5; 0; 1; 0), x1 = (0.0; 0; 0; 1)。', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 30, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='以上针对样本属性所进行的处理工作便是第1 章1.2 基本术语中提到的“特征工程”范畴，完成属性\\n数值化以后通常还会进行缺失值处理、规范化、降维等一系列处理工作。由于特征工程属于算法实践过程\\n中需要掌握的内容，待学完机器学习算法以后，再进一步学习特征工程相关知识即可，在此先不展开。\\n3.2.2\\n式(3.4) 的解释\\n下面仅针对式(3.4) 中的数学符号进行解释。\\n首先解释一下符号\\n“arg min”\\n，\\n其中\\n“arg”\\n是\\n“argument”\\n（参\\n数）的前三个字母，\\n“min”是“minimum”\\n（最小值）的前三个字母，该符号表示求使目标函数达到最小值\\n的参数取值。例如式(3.4) 表示求出使目标函数Pm\\ni=1 (yi −wxi −b)2 达到最小值的参数取值(w∗, b∗)，注\\n意目标函数是以(w, b) 为自变量的函数，(xi, yi) 均是已知常量，即训练集中的样本数据。\\n类似的符号还有“min”\\n，例如将式(3.4) 改为\\nmin\\n(w,b)\\nm\\nX\\ni=1\\n(yi −wxi −b)2\\n则表示求目标函数的最小值。对比知道，', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 30, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='(w,b)\\nm\\nX\\ni=1\\n(yi −wxi −b)2\\n则表示求目标函数的最小值。对比知道，\\n“min”和“arg min”的区别在于，前者输出目标函数的最小值，\\n而后者输出使得目标函数达到最小值时的参数取值。\\n若进一步修改式(3.4) 为\\nmin\\n(w,b)\\nm\\nX\\ni=1\\n(yi −wxi −b)2\\ns.t.w > 0,\\nb < 0.\\n→_→\\n配套视频教程：https://www.bilibili.com/video/BV1Mh411e7VU\\n←_←', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 30, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='→_→\\n欢迎去各大电商平台选购纸质版南瓜书《机器学习公式详解第2 版》\\n←_←\\n则表示在w > 0, b < 0 范围内寻找目标函数的最小值，\\n“s.t.”是“subject to”的简写，意思是“受约束\\n于”\\n，即为约束条件。\\n以上介绍的符号都是应用数学领域的一个分支——“最优化”中的内容，若想进一步了解可找一本最\\n优化的教材（例如参考文献[1]）进行系统性地学习。\\n3.2.3\\n式(3.5) 的推导\\n“西瓜书”在式(3.5) 左侧给出的凸函数的定义是最优化中的定义，与高等数学中的定义不同，本书也\\n默认采用此种定义。由于一元线性回归可以看作是多元线性回归中元的个数为1 时的情形，所以此处暂不\\n给出E(w,b) 是关于w 和b 的凸函数的证明，在推导式(3.11) 时一并给出，下面开始推导式(3.5)。\\n已知E(w,b) =\\nm\\nP\\ni=1\\n(yi −wxi −b)2，所以\\n∂E(w,b)\\n∂w\\n= ∂\\n∂w\\n\" m\\nX\\ni=1\\n(yi −wxi −b)2\\n#\\n=\\nm\\nX\\ni=1\\n∂\\n∂w\\nh\\n(yi −wxi −b)2i\\n=\\nm\\nX\\ni=1', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 31, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='#\\n=\\nm\\nX\\ni=1\\n∂\\n∂w\\nh\\n(yi −wxi −b)2i\\n=\\nm\\nX\\ni=1\\n[2 · (yi −wxi −b) · (−xi)]\\n=\\nm\\nX\\ni=1\\n\\x02\\n2 ·\\n\\x00wx2\\ni −yixi + bxi\\n\\x01\\x03\\n= 2 ·\\n \\nw\\nm\\nX\\ni=1\\nx2\\ni −\\nm\\nX\\ni=1\\nyixi + b\\nm\\nX\\ni=1\\nxi\\n!\\n= 2\\n \\nw\\nm\\nX\\ni=1\\nx2\\ni −\\nm\\nX\\ni=1\\n(yi −b) xi\\n!\\n3.2.4\\n式(3.6) 的推导\\n已知E(w,b) =\\nm\\nP\\ni=1\\n(yi −wxi −b)2，所以\\n∂E(w,b)\\n∂b\\n= ∂\\n∂b\\n\" m\\nX\\ni=1\\n(yi −wxi −b)2\\n#\\n=\\nm\\nX\\ni=1\\n∂\\n∂b\\nh\\n(yi −wxi −b)2i\\n=\\nm\\nX\\ni=1\\n[2 · (yi −wxi −b) · (−1)]\\n=\\nm\\nX\\ni=1\\n[2 · (b −yi + wxi)]\\n= 2 ·\\n\" m\\nX\\ni=1\\nb −\\nm\\nX\\ni=1\\nyi +\\nm\\nX\\ni=1\\nwxi\\n#\\n= 2\\n \\nmb −\\nm\\nX\\ni=1\\n(yi −wxi)\\n!\\n3.2.5\\n式(3.7) 的推导', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 31, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='#\\n= 2\\n \\nmb −\\nm\\nX\\ni=1\\n(yi −wxi)\\n!\\n3.2.5\\n式(3.7) 的推导\\n推导之前先重点说明一下“闭式解”或称为“解析解”\\n。闭式解是指可以通过具体的表达式解出待解\\n参数，例如可根据式(3.7) 直接解得w。机器学习算法很少有闭式解，线性回归是一个特例，接下来推导\\n→_→\\n配套视频教程：https://www.bilibili.com/video/BV1Mh411e7VU\\n←_←', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 31, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='→_→\\n欢迎去各大电商平台选购纸质版南瓜书《机器学习公式详解第2 版》\\n←_←\\n式(3.7)。\\n令式(3.5) 等于0\\n0 = w\\nm\\nX\\ni=1\\nx2\\ni −\\nm\\nX\\ni=1\\n(yi −b)xi\\nw\\nm\\nX\\ni=1\\nx2\\ni =\\nm\\nX\\ni=1\\nyixi −\\nm\\nX\\ni=1\\nbxi\\n由于令式(3.6) 等于0 可得b =\\n1\\nm\\nPm\\ni=1(yi −wxi)，\\n又因为\\n1\\nm\\nPm\\ni=1 yi = ¯\\ny，1\\nm\\nPm\\ni=1 xi = ¯\\nx，\\n则b = ¯\\ny −w¯\\nx，\\n代入上式可得\\nw\\nm\\nX\\ni=1\\nx2\\ni =\\nm\\nX\\ni=1\\nyixi −\\nm\\nX\\ni=1\\n(¯\\ny −w¯\\nx)xi\\nw\\nm\\nX\\ni=1\\nx2\\ni =\\nm\\nX\\ni=1\\nyixi −¯\\ny\\nm\\nX\\ni=1\\nxi + w¯\\nx\\nm\\nX\\ni=1\\nxi\\nw(\\nm\\nX\\ni=1\\nx2\\ni −¯\\nx\\nm\\nX\\ni=1\\nxi) =\\nm\\nX\\ni=1\\nyixi −¯\\ny\\nm\\nX\\ni=1\\nxi\\nw =\\nPm\\ni=1 yixi −¯\\ny Pm\\ni=1 xi\\nPm\\ni=1 x2\\ni −¯\\nx Pm\\ni=1 xi\\n将¯\\ny Pm', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 32, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='y Pm\\ni=1 xi\\nPm\\ni=1 x2\\ni −¯\\nx Pm\\ni=1 xi\\n将¯\\ny Pm\\ni=1 xi =\\n1\\nm\\nPm\\ni=1 yi\\nPm\\ni=1 xi = ¯\\nx Pm\\ni=1 yi 和¯\\nx Pm\\ni=1 xi =\\n1\\nm\\nPm\\ni=1 xi\\nPm\\ni=1 xi =\\n1\\nm(Pm\\ni=1 xi)2 代入上\\n式，即可得式(3.7)：\\nw =\\nPm\\ni=1 yi(xi −¯\\nx)\\nPm\\ni=1 x2\\ni −1\\nm(Pm\\ni=1 xi)2\\n如果要想用Python 来实现上式的话，上式中的求和运算只能用循环来实现。但是如果能将上式向量化，\\n也就是转换成矩阵（即向量）运算的话，我们就可以利用诸如NumPy 这种专门加速矩阵运算的类库来进\\n行编写。下面我们就尝试将上式进行向量化。\\n将\\n1\\nm(Pm\\ni=1 xi)2 = ¯\\nx Pm\\ni=1 xi 代入分母可得\\nw =\\nPm\\ni=1 yi(xi −¯\\nx)\\nPm\\ni=1 x2\\ni −¯\\nx Pm\\ni=1 xi\\n=\\nPm\\ni=1(yixi −yi¯\\nx)\\nPm\\ni=1(x2\\ni −xi¯\\nx)\\n又因为¯\\ny Pm\\ni=1 xi = ¯', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 32, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='x)\\nPm\\ni=1(x2\\ni −xi¯\\nx)\\n又因为¯\\ny Pm\\ni=1 xi = ¯\\nx Pm\\ni=1 yi = Pm\\ni=1 ¯\\nyxi = Pm\\ni=1 ¯\\nxyi = m¯\\nx¯\\ny = Pm\\ni=1 ¯\\nx¯\\ny 且Pm\\ni=1 xi¯\\nx = ¯\\nx Pm\\ni=1 xi =\\n¯\\nx · m · 1\\nm · Pm\\ni=1 xi = m¯\\nx2 = Pm\\ni=1 ¯\\nx2，则有\\nw =\\nPm\\ni=1(yixi −yi¯\\nx −xi¯\\ny + ¯\\nx¯\\ny)\\nPm\\ni=1(x2\\ni −xi¯\\nx −xi¯\\nx + ¯\\nx2)\\n=\\nPm\\ni=1(xi −¯\\nx)(yi −¯\\ny)\\nPm\\ni=1(xi −¯\\nx)2\\n若令x = (x1; x2; ...; xm)，xd = (x1 −¯\\nx; x2 −¯\\nx; ...; xm −¯\\nx) 为去均值后的x；y = (y1; y2; ...; ym)，yd =\\n(y1 −¯\\ny; y2 −¯\\ny; ...; ym −¯\\ny) 为去均值后的y，（x、xd、y、yd 均为m 行1 列的列向量）代入上式可得\\nw = xT\\nd yd\\nxT', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 32, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='w = xT\\nd yd\\nxT\\nd xd\\n→_→\\n配套视频教程：https://www.bilibili.com/video/BV1Mh411e7VU\\n←_←', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 32, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='→_→\\n欢迎去各大电商平台选购纸质版南瓜书《机器学习公式详解第2 版》\\n←_←\\n3.2.6\\n式(3.9) 的推导\\n式(3.4) 是最小二乘法运用在一元线性回归上的情形，那么对于多元线性回归来说，我们可以类似得\\n到\\n(w∗, b∗) = arg min\\n(w,b)\\nm\\nX\\ni=1\\n(f (xi) −yi)2\\n= arg min\\n(w,b)\\nm\\nX\\ni=1\\n(yi −f (xi))2\\n= arg min\\n(w,b)\\nm\\nX\\ni=1\\n\\x00yi −\\n\\x00wTxi + b\\n\\x01\\x012\\n为便于讨论，我们令ˆ\\nw = (w; b) = (w1; ...; wd; b) ∈R(d+1)×1, ˆ\\nxi = (xi1; ...; xid; 1) ∈R(d+1)×1，那么上式可\\n以简化为\\nˆ\\nw∗= arg min\\nˆ\\nw\\nm\\nX\\ni=1\\n\\x10\\nyi −ˆ\\nwTˆ\\nxi\\n\\x112\\n= arg min\\nˆ\\nw\\nm\\nX\\ni=1\\n\\x10\\nyi −ˆ\\nxT\\ni ˆ\\nw\\n\\x112\\n根据向量内积的定义可知，上式可以写成如下向量内积的形式\\nˆ\\nw∗= arg min\\nˆ\\nw\\nh\\ny1 −ˆ\\nxT\\n1 ˆ\\nw\\n· · ·\\nym −ˆ\\nxT', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 33, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='ˆ\\nw∗= arg min\\nˆ\\nw\\nh\\ny1 −ˆ\\nxT\\n1 ˆ\\nw\\n· · ·\\nym −ˆ\\nxT\\nm ˆ\\nw\\ni\\n\\uf8ee\\n\\uf8ef\\n\\uf8ef\\n\\uf8f0\\ny1 −ˆ\\nxT\\n1 ˆ\\nw\\n.\\n.\\n.\\nym −ˆ\\nxT\\nm ˆ\\nw\\n\\uf8f9\\n\\uf8fa\\n\\uf8fa\\n\\uf8fb\\n其中\\n\\uf8ee\\n\\uf8ef\\n\\uf8ef\\n\\uf8f0\\ny1 −ˆ\\nxT\\n1 ˆ\\nw\\n.\\n.\\n.\\nym −ˆ\\nxT\\nm ˆ\\nw\\n\\uf8f9\\n\\uf8fa\\n\\uf8fa\\n\\uf8fb=\\n\\uf8ee\\n\\uf8ef\\n\\uf8ef\\n\\uf8f0\\ny1\\n.\\n.\\n.\\nym\\n\\uf8f9\\n\\uf8fa\\n\\uf8fa\\n\\uf8fb−\\n\\uf8ee\\n\\uf8ef\\n\\uf8ef\\n\\uf8f0\\nˆ\\nxT\\n1 ˆ\\nw\\n.\\n.\\n.\\nˆ\\nxT\\nm ˆ\\nw\\n\\uf8f9\\n\\uf8fa\\n\\uf8fa\\n\\uf8fb\\n= y −\\n\\uf8ee\\n\\uf8ef\\n\\uf8ef\\n\\uf8f0\\nˆ\\nxT\\n1\\n.\\n.\\n.\\nˆ\\nxT\\nm\\n\\uf8f9\\n\\uf8fa\\n\\uf8fa\\n\\uf8fb· ˆ\\nw\\n= y −X ˆ\\nw\\n所以\\nˆ\\nw∗= arg min\\nˆ\\nw\\n(y −X ˆ\\nw)T(y −X ˆ\\nw)\\n3.2.7\\n式(3.10) 的推导\\n将E ˆ\\nw = (y −X ˆ\\nw)T(y −X ˆ\\nw) 展开可得\\nE ˆ\\nw = yTy −yTX ˆ\\nw −ˆ\\nwTXTy + ˆ\\nwTXTX ˆ\\nw\\n对ˆ\\nw 求导可得\\n∂E ˆ\\nw\\n∂ˆ\\nw = ∂yTy\\n∂ˆ\\nw\\n−∂yTX ˆ\\nw\\n∂ˆ\\nw\\n−∂ˆ\\nwTXTy\\n∂ˆ', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 33, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='w\\n∂ˆ\\nw = ∂yTy\\n∂ˆ\\nw\\n−∂yTX ˆ\\nw\\n∂ˆ\\nw\\n−∂ˆ\\nwTXTy\\n∂ˆ\\nw\\n+ ∂ˆ\\nwTXTX ˆ\\nw\\n∂ˆ\\nw\\n由矩阵微分公式∂aTx\\n∂x\\n= ∂xTa\\n∂x\\n= a, ∂xTAx\\n∂x\\n= (A+AT)x （更多矩阵微分公式可查阅[2]，矩阵微分原理可查阅[3]）可\\n得\\n∂E ˆ\\nw\\n∂ˆ\\nw = 0 −XTy −XTy + (XTX + XTX) ˆ\\nw\\n= 2XT(X ˆ\\nw −y)\\n→_→\\n配套视频教程：https://www.bilibili.com/video/BV1Mh411e7VU\\n←_←', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 33, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='→_→\\n欢迎去各大电商平台选购纸质版南瓜书《机器学习公式详解第2 版》\\n←_←\\n3.2.8\\n式(3.11) 的推导\\n首先铺垫讲解接下来以及后续内容将会用到的多元函数相关基础知识\\n[1]。\\nn 元实值函数：含n 个自变量，值域为实数域R 的函数称为n 元实值函数，记为f(x)，其中x =\\n(x1; x2; ...; xn) 为n 维向量。\\n“西瓜书”和本书中的多元函数未加特殊说明均为实值函数。\\n凸集：设集合D ⊂Rn 为n 维欧式空间中的子集，如果对D 中任意的n 维向量x ∈D 和y ∈D 与\\n任意的α ∈[0, 1]，有\\nαx + (1 −α)y ∈D\\n则称集合D 是凸集。凸集的几何意义是：若两个点属于此集合，则这两点连线上的任意一点均属于此集\\n合。常见的凸集有空集∅，整个n 维欧式空间Rn。\\n凸函数：设D ⊂Rn 是非空凸集，f 是定义在D 上的函数，如果对任意的x1, x2 ∈D, α ∈(0, 1)，均\\n有\\nf\\n\\x00αx1 + (1 −α)x2\\x01\\n⩽αf(x1) + (1 −α)f(x2)\\n则称f 为D 上的凸函数。若其中的⩽改为< 也恒成立，则称f 为D 上的严格凸函数。', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 34, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='则称f 为D 上的凸函数。若其中的⩽改为< 也恒成立，则称f 为D 上的严格凸函数。\\n梯度：若n 元函数f(x) 对x = (x1; x2; ...; xn) 中各分量xi 的偏导数∂f(x)\\n∂xi (i = 1, 2, ..., n) 都存在，则\\n称函数f(x) 在x 处一阶可导，并称以下列向量\\n∇f(x) = ∂f(x)\\n∂x\\n=\\n\\uf8ee\\n\\uf8ef\\n\\uf8ef\\n\\uf8ef\\n\\uf8ef\\n\\uf8ef\\n\\uf8f0\\n∂f(x)\\n∂x1\\n∂f(x)\\n∂x2\\n.\\n.\\n.\\n∂f(x)\\n∂xn\\n\\uf8f9\\n\\uf8fa\\n\\uf8fa\\n\\uf8fa\\n\\uf8fa\\n\\uf8fa\\n\\uf8fb\\n为函数f(x) 在x 处的一阶导数或梯度，易证梯度指向的方向是函数值增大速度最快的方向。∇f(x) 也可\\n写成行向量形式\\n∇f(x) = ∂f(x)\\n∂xT\\n=\\n\\x14∂f(x)\\n∂x1\\n, ∂f(x)\\n∂x2\\n, · · ·, ∂f(x)\\n∂xn\\n\\x15\\n我们称列向量形式为“分母布局”\\n，行向量形式为“分子布局”\\n，由于在最优化中习惯采用分母布局，因此\\n“西瓜书”以及本书中也采用分母布局。为了便于区分当前采用何种布局，通常在采用分母布局时偏导符\\n号∂后接的是x，采用分子布局时后接的是xT。', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 34, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='号∂后接的是x，采用分子布局时后接的是xT。\\nHessian 矩阵：若n 元函数f(x) 对x = (x1; x2; ...; xn) 中各分量xi 的二阶偏导数\\n∂2f(x)\\n∂xi∂xj (i =\\n1, 2, ..., n; j = 1, 2, ..., n) 都存在，则称函数f(x) 在x 处二阶阶可导，并称以下矩阵\\n∇2f(x) = ∂2f(x)\\n∂x∂xT =\\n\\uf8ee\\n\\uf8ef\\n\\uf8ef\\n\\uf8ef\\n\\uf8ef\\n\\uf8ef\\n\\uf8f0\\n∂2f(x)\\n∂x2\\n1\\n∂2f(x)\\n∂x1∂x2\\n· · ·\\n∂2f(x)\\n∂x1∂xn\\n∂2f(x)\\n∂x2∂x1\\n∂2f(x)\\n∂x2\\n2\\n· · ·\\n∂2f(x)\\n∂x2∂xn\\n.\\n.\\n.\\n.\\n.\\n.\\n...\\n.\\n.\\n.\\n∂2f(x)\\n∂xn∂x1\\n∂2f(x)\\n∂xn∂x2\\n· · ·\\n∂2f(x)\\n∂x2\\nn\\n\\uf8f9\\n\\uf8fa\\n\\uf8fa\\n\\uf8fa\\n\\uf8fa\\n\\uf8fa\\n\\uf8fb\\n为函数f(x) 在x 处的二阶导数或Hessian 矩阵。若其中的二阶偏导数均连续，则\\n∂2f(x)\\n∂xi∂xj\\n= ∂2f(x)\\n∂xj∂xi\\n此时Hessian 矩阵为对称矩阵。', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 34, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='∂2f(x)\\n∂xi∂xj\\n= ∂2f(x)\\n∂xj∂xi\\n此时Hessian 矩阵为对称矩阵。\\n定理3.1：设D ⊂Rn 是非空开凸集，f(x) 是定义在D 上的实值函数，且f(x) 在D 上二阶连续可\\n微，如果f(x) 的Hessian 矩阵∇2f(x) 在D 上是半正定的，则f(x) 是D 上的凸函数；如果∇2f(x) 在\\nD 上是正定的，则f(x) 是D 上的严格凸函数。\\n定理3.2：若f(x) 是凸函数，且f(x) 一阶连续可微，则x∗是全局解的充分必要条件是其梯度等于\\n零向量，即∇f(x∗) = 0。\\n→_→\\n配套视频教程：https://www.bilibili.com/video/BV1Mh411e7VU\\n←_←', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 34, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='→_→\\n欢迎去各大电商平台选购纸质版南瓜书《机器学习公式详解第2 版》\\n←_←\\n式(3.11) 的推导思路如下：首先根据定理3.1 推导出E ˆ\\nw 是ˆ\\nw 的凸函数，接着根据定理3.2 推导出\\n式(3.11)。下面按照此思路进行推导。\\n由于式(3.10) 已推导出E ˆ\\nw 关于ˆ\\nw 的一阶导数，接着基于此进一步推导出二阶导数，即Hessian 矩\\n阵。推导过程如下：\\n∇2E ˆ\\nw =\\n∂\\n∂ˆ\\nwT\\n\\x12∂E ˆ\\nw\\n∂ˆ\\nw\\n\\x13\\n=\\n∂\\n∂ˆ\\nwT\\n\\x02\\n2XT(X ˆ\\nw −y)\\n\\x03\\n=\\n∂\\n∂ˆ\\nwT\\n\\x002XTX ˆ\\nw −2XTy\\n\\x01\\n由矩阵微分公式∂Ax\\nxT = A 可得\\n∇2E ˆ\\nw = 2XTX\\n如“西瓜书”中式(3.11) 上方的一段话所说，假定XTX 为正定矩阵，根据定理3.1 可知此时E ˆ\\nw 是\\nˆ\\nw 的严格凸函数，接着根据定理3.2 可知只需令E ˆ\\nw 关于ˆ\\nw 的一阶导数等于零向量，即令式(3.10) 等于\\n零向量即可求得全局最优解ˆ\\nw∗，具体求解过程如下：\\n∂E ˆ\\nw\\n∂ˆ\\nw = 2XT(X ˆ\\nw −y) = 0\\n2XTX ˆ', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 35, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='∂E ˆ\\nw\\n∂ˆ\\nw = 2XT(X ˆ\\nw −y) = 0\\n2XTX ˆ\\nw −2XTy = 0\\n2XTX ˆ\\nw = 2XTy\\nˆ\\nw = (XTX)−1XTy\\n令其为ˆ\\nw∗即为式(3.11)。\\n由于X 是由样本构成的矩阵，而样本是千变万化的，因此无法保证XTX 一定是正定矩阵，极易出现\\n非正定的情形。当XTX 非正定矩阵时，除了“西瓜书”中所说的引入正则化外，也可用XTX 的伪逆矩阵\\n代入式(3.11) 求解出ˆ\\nw∗，只是此时并不保证求解得到的ˆ\\nw∗一定是全局最优解。除此之外，也可用下一\\n节将会讲到的“梯度下降法”求解，同样也不保证求得全局最优解。\\n3.3\\n对数几率回归\\n对数几率回归的一般使用流程如下：首先在训练集上学得模型\\ny =\\n1\\n1 + e−(wTx+b)\\n然后对于新的测试样本xi，将其代入模型得到预测结果yi，接着自行设定阈值θ，通常设为θ = 0.5，如\\n果yi ⩾θ 则判xi 为正例，反之判为反例。\\n3.3.1\\n式(3.27) 的推导\\n将式(3.26) 代入式(3.25) 可得\\nℓ(β) =\\nm\\nX\\ni=1\\nln (yip1(ˆ', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 35, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='将式(3.26) 代入式(3.25) 可得\\nℓ(β) =\\nm\\nX\\ni=1\\nln (yip1(ˆ\\nxi; β) + (1 −yi)p0(ˆ\\nxi; β))\\n其中p1(ˆ\\nxi; β) =\\neβT ˆ\\nxi\\n1+eβT ˆ\\nxi , p0(ˆ\\nxi; β) =\\n1\\n1+eβT ˆ\\nxi ，代入上式可得\\nℓ(β) =\\nm\\nX\\ni=1\\nln\\n \\nyieβT ˆ\\nxi + 1 −yi\\n1 + eβT ˆ\\nxi\\n!\\n=\\nm\\nX\\ni=1\\n\\x10\\nln(yieβT ˆ\\nxi + 1 −yi) −ln(1 + eβT ˆ\\nxi)\\n\\x11\\n→_→\\n配套视频教程：https://www.bilibili.com/video/BV1Mh411e7VU\\n←_←', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 35, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='→_→\\n欢迎去各大电商平台选购纸质版南瓜书《机器学习公式详解第2 版》\\n←_←\\n由于yi=0 或1，则\\nℓ(β) =\\n\\uf8f1\\n\\uf8f2\\n\\uf8f3\\nPm\\ni=1(−ln(1 + eβT ˆ\\nxi)),\\nyi = 0\\nPm\\ni=1(βTˆ\\nxi −ln(1 + eβT ˆ\\nxi)),\\nyi = 1\\n两式综合可得\\nℓ(β) =\\nm\\nX\\ni=1\\n\\x10\\nyiβTˆ\\nxi −ln(1 + eβT ˆ\\nxi)\\n\\x11\\n由于此式仍为极大似然估计的似然函数，所以最大化似然函数等价于最小化似然函数的相反数，即在似然\\n函数前添加负号即可得式(3.27)。\\n值得一提的是，\\n若将式(3.26) 改写为p(yi|xi; w, b) = [p1(ˆ\\nxi; β)]yi[p0(ˆ\\nxi; β)]1−yi，\\n再代入式(3.25) 可得\\nℓ(β) =\\nm\\nX\\ni=1\\nln\\n\\x00[p1(ˆ\\nxi; β)]yi[p0(ˆ\\nxi; β)]1−yi\\x01\\n=\\nm\\nX\\ni=1\\n[yi ln (p1(ˆ\\nxi; β)) + (1 −yi) ln (p0(ˆ\\nxi; β))]\\n=\\nm\\nX\\ni=1\\n{yi [ln (p1(ˆ\\nxi; β)) −ln (p0(ˆ', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 36, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='=\\nm\\nX\\ni=1\\n{yi [ln (p1(ˆ\\nxi; β)) −ln (p0(ˆ\\nxi; β))] + ln (p0(ˆ\\nxi; β))}\\n=\\nm\\nX\\ni=1\\n\\x14\\nyi ln\\n\\x12p1(ˆ\\nxi; β)\\np0(ˆ\\nxi; β)\\n\\x13\\n+ ln (p0(ˆ\\nxi; β))\\n\\x15\\n=\\nm\\nX\\ni=1\\n\\x14\\nyi ln\\n\\x10\\neβT ˆ\\nxi\\x11\\n+ ln\\n\\x12\\n1\\n1 + eβT ˆ\\nxi\\n\\x13\\x15\\n=\\nm\\nX\\ni=1\\n\\x10\\nyiβTˆ\\nxi −ln(1 + eβT ˆ\\nxi)\\n\\x11\\n显然，此种方式更易推导出式(3.27)。\\n“西瓜书”在式(3.27) 下方有提到式(3.27) 是关于β 的凸函数，其证明过程如下：由于若干半正定矩\\n阵的加和仍为半正定矩阵，\\n则根据定理3.1 可知，\\n若干凸函数的加和仍为凸函数。\\n因此，\\n只需证明式(3.27)\\n求和符号后的式子−yiβTˆ\\nxi + ln(1 + eβT ˆ\\nxi)（记为f(β)）为凸函数即可。根据式(3.31) 可知，f(β) 的二\\n阶导数，即Hessian 矩阵为\\nˆ\\nxiˆ\\nxT\\ni p1 (ˆ\\nxi; β) (1 −p1 (ˆ\\nxi; β))', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 36, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='ˆ\\nxiˆ\\nxT\\ni p1 (ˆ\\nxi; β) (1 −p1 (ˆ\\nxi; β))\\n对于任意非零向量y ∈Rd+1，恒有\\nyT · ˆ\\nxiˆ\\nxT\\ni p1 (ˆ\\nxi; β) (1 −p1 (ˆ\\nxi; β)) · y\\nyTˆ\\nxiˆ\\nxT\\ni yp1 (ˆ\\nxi; β) (1 −p1 (ˆ\\nxi; β))\\n\\x00yTˆ\\nxi\\n\\x012 p1 (ˆ\\nxi; β) (1 −p1 (ˆ\\nxi; β))\\n由于p1 (ˆ\\nxi; β) > 0，因此上式恒大于等于0，根据半正定矩阵的定义可知此时f(β) 的Hessian 矩阵为半\\n正定矩阵，所以f(β) 是关于β 的凸函数。\\n3.3.2\\n梯度下降法\\n不同于式(3.7) 可求得闭式解，式(3.27) 中的β 没有闭式解，因此需要借助其他工具进行求解。求解\\n使得式(3.27) 取到最小值的β 属于最优化中的“无约束优化问题”\\n，在无约束优化问题中最常用的求解算\\n法有“梯度下降法”和“牛顿法”\\n[1]，下面分别展开讲解。\\n梯度下降法是一种迭代求解算法，其基本思路如下：先在定义域中随机选取一个点x0，将其代入函', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 36, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='梯度下降法是一种迭代求解算法，其基本思路如下：先在定义域中随机选取一个点x0，将其代入函\\n数f(x) 并判断此时f(x0) 是否是最小值，如果不是的话，则找下一个点x1，且保证f(x1) < f(x0)，然\\n→_→\\n配套视频教程：https://www.bilibili.com/video/BV1Mh411e7VU\\n←_←', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 36, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='→_→\\n欢迎去各大电商平台选购纸质版南瓜书《机器学习公式详解第2 版》\\n←_←\\n后接着判断f(x1) 是否是最小值，如果不是的话则重复上述步骤继续迭代寻找x2、x3、...... 直到找到使\\n得f(x) 取到最小值的x∗。\\n显然，此算法要想行得通就必须解决在找到第t 个点xt 时，能进一步找到第t + 1 个点xt+1，且保\\n证f(xt+1) < f(xt)。梯度下降法利用“梯度指向的方向是函数值增大速度最快的方向”这一特性，每次迭\\n代时朝着梯度的反方向进行，进而实现函数值越迭代越小，下面给出完整的数学推导过程。\\n根据泰勒公式可知，当函数f(x) 在xt 处一阶可导时，在其邻域内进行一阶泰勒展开恒有\\nf(x) = f\\n\\x00xt\\x01\\n+ ∇f\\n\\x00xt\\x01T \\x00x −xt\\x01\\n+ o\\n\\x00\\r\\n\\rx −xt\\r\\n\\r\\x01\\n其中∇f (xt) 是函数f(x) 在点xt 处的梯度，∥x −xt∥是指向量x −xt 的模。若令x −xt = adt，其中\\na > 0，dt 是模长为1 的单位向量，则上式可改写为\\nf(xt + adt) = f\\n\\x00xt\\x01\\n+ a∇f\\n\\x00xt\\x01T dt + o\\n\\x00\\r\\n\\rdt\\r\\n\\r\\x01', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 37, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='\\x00xt\\x01\\n+ a∇f\\n\\x00xt\\x01T dt + o\\n\\x00\\r\\n\\rdt\\r\\n\\r\\x01\\nf(xt + adt) −f\\n\\x00xt\\x01\\n= a∇f\\n\\x00xt\\x01T dt + o\\n\\x00\\r\\n\\rdt\\r\\n\\r\\x01\\n观察上式可知，如果能保证a∇f (xt)\\nT dt < 0，则一定能保证f(xt + adt) < f (xt)，此时再令xt+1 =\\nxt + adt，即可推得我们想要的f(xt+1) < f(xt)。所以，此时问题转化为了求解能使得a∇f (xt)\\nT dt < 0\\n的dt，且a∇f (xt)\\nT dt 比0 越小，相应地f(xt+1) 也会比f(xt) 越小，也更接近最小值。\\n根据向量的内积公式可知\\na∇f\\n\\x00xt\\x01T dt = a × ∥∇f\\n\\x00xt\\x01\\n∥× ∥dt∥× cos θt\\n其中θt 是向量∇f (xt) 与向量dt 之间的夹角。观察上式易知，此时∥∇f (xt) ∥是固定常量，∥dt∥= 1，\\n所以当a 也固定时，取θt = π，即向量dt 与向量∇f (xt) 的方向刚好相反时，上式取到最小值。通常为', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 37, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='了精简计算步骤，可直接令dt = −∇f (xt)，因此便得到了第t + 1 个点xt+1 的迭代公式\\nxt+1 = xt −a∇f\\n\\x00xt\\x01\\n其中a 也称为“步长”或“学习率”\\n，是需要自行设定的参数，且每次迭代时可取不同值。\\n除了需要解决如何找到xt+1 以外，梯度下降法通常还需要解决如何判断当前点是否使得函数取到了\\n最小值，否则的话迭代过程便可能会无休止进行。常用的做法是预先设定一个极小的阈值ϵ，当某次迭代\\n造成的函数值波动已经小于ϵ 时，即|f(xt+1) −f(xt)| < ϵ，我们便近似地认为此时f(xt+1) 取到了最小\\n值。\\n3.3.3\\n牛顿法\\n同梯度下降法，牛顿法也是一种迭代求解算法，其基本思路和梯度下降法一致，只是在选取第t + 1\\n个点xt+1 时所采用的策略有所不同，即迭代公式不同。梯度下降法每次选取xt+1 时，只要求通过泰勒公\\n式在xt 的邻域内找到一个函数值比其更小的点即可，而牛顿法则期望在此基础之上，xt+1 还必须是xt\\n的邻域内的极小值点。\\n类似一元函数取到极值点的必要条件是一阶导数等于0，多元函数取到极值点的必要条件是其梯度等', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 37, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='类似一元函数取到极值点的必要条件是一阶导数等于0，多元函数取到极值点的必要条件是其梯度等\\n于零向量0，为了能求解出xt 的邻域内梯度等于0 的点，需要进行二阶泰勒展开，其展开式如下\\nf(x) = f\\n\\x00xt\\x01\\n+ ∇f\\n\\x00xt\\x01T \\x00x −xt\\x01\\n+ 1\\n2\\n\\x00x −xt\\x01T ∇2f\\n\\x00xt\\x01 \\x00x −xt\\x01\\n+ o\\n\\x00\\r\\n\\rx −xt\\r\\n\\r\\x01\\n为了后续计算方便，我们取其近似形式\\nf(x) ≈f\\n\\x00xt\\x01\\n+ ∇f\\n\\x00xt\\x01T \\x00x −xt\\x01\\n+ 1\\n2\\n\\x00x −xt\\x01T ∇2f\\n\\x00xt\\x01 \\x00x −xt\\x01\\n→_→\\n配套视频教程：https://www.bilibili.com/video/BV1Mh411e7VU\\n←_←', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 37, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='→_→\\n欢迎去各大电商平台选购纸质版南瓜书《机器学习公式详解第2 版》\\n←_←\\n首先对上式求导\\n∂f(x)\\n∂x\\n= ∂f (xt)\\n∂x\\n+ ∂∇f (xt)\\nT (x −xt)\\n∂x\\n+ 1\\n2\\n∂(x −xt)\\nT ∇2f (xt) (x −xt)\\n∂x\\n= 0 + ∇f\\n\\x00xt\\x01\\n+ 1\\n2\\n\\x10\\n∇2f\\n\\x00xt\\x01\\n+ ∇2f\\n\\x00xt\\x01T\\x11 \\x00x −xt\\x01\\n假设函数f(x) 在xt 处二阶可导，且偏导数连续，则∇2f (xt) 是对称矩阵，上式可写为\\n∂f(x)\\n∂x\\n= 0 + ∇f\\n\\x00xt\\x01\\n+ 1\\n2 × 2 × ∇2f\\n\\x00xt\\x01 \\x00x −xt\\x01\\n= ∇f\\n\\x00xt\\x01\\n+ ∇2f\\n\\x00xt\\x01 \\x00x −xt\\x01\\n令上式等于0\\n∇f\\n\\x00xt\\x01\\n+ ∇2f\\n\\x00xt\\x01 \\x00x −xt\\x01\\n= 0\\n当∇2f (xt) 是可逆矩阵时，解得\\nx = xt −\\n\\x02\\n∇2f\\n\\x00xt\\x01\\x03−1 ∇f\\n\\x00xt\\x01\\n令上式为xt+1 即可得到牛顿法的迭代公式\\nxt+1 = xt −\\n\\x02\\n∇2f\\n\\x00xt\\x01\\x03−1 ∇f\\n\\x00xt\\x01', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 38, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='xt+1 = xt −\\n\\x02\\n∇2f\\n\\x00xt\\x01\\x03−1 ∇f\\n\\x00xt\\x01\\n通过上述推导可知，牛顿法每次迭代时需要求解Hessian 矩阵的逆矩阵，该步骤计算量通常较大，因此有\\n人基于牛顿法，将其中求Hessian 矩阵的逆矩阵改为求计算量更低的近似逆矩阵，我们称此类算法为“拟\\n牛顿法”\\n。\\n牛顿法虽然期望在每次迭代时能取到极小值点，但是通过上述推导可知，迭代公式是根据极值点的必\\n要条件推导而得，因此并不保证一定是极小值点。\\n无论是梯度下降法还是牛顿法，根据其终止迭代的条件可知，其都是近似求解算法，即使f(x) 是凸\\n函数，也并不一定保证最终求得的是全局最优解，仅能保证其接近全局最优解。不过在解决实际问题时，\\n并不一定苛求解得全局最优解，在能接近全局最优甚至局部最优时通常也能很好地解决问题。\\n3.3.4\\n式(3.29) 的解释\\n根据上述牛顿法的迭代公式可知，此式为式(3.27) 应用牛顿法时的迭代公式。\\n3.3.5\\n式(3.30) 的推导\\n∂ℓ(β)\\n∂β\\n=\\n∂Pm\\ni=1\\n\\x10\\n−yiβTˆ\\nxi + ln\\n\\x10\\n1 + eβT ˆ\\nxi\\n\\x11\\x11\\n∂β\\n=\\nm\\nX\\ni=1\\n\\uf8eb\\n\\uf8ed∂', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 38, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='xi + ln\\n\\x10\\n1 + eβT ˆ\\nxi\\n\\x11\\x11\\n∂β\\n=\\nm\\nX\\ni=1\\n\\uf8eb\\n\\uf8ed∂\\n\\x00−yiβTˆ\\nxi\\n\\x01\\n∂β\\n+\\n∂ln\\n\\x10\\n1 + eβT ˆ\\nxi\\n\\x11\\n∂β\\n\\uf8f6\\n\\uf8f8\\n=\\nm\\nX\\ni=1\\n\\x12\\n−yiˆ\\nxi +\\n1\\n1 + eβT ˆ\\nxi · ˆ\\nxieβT ˆ\\nxi\\n\\x13\\n= −\\nm\\nX\\ni=1\\nˆ\\nxi\\n \\nyi −\\neβT ˆ\\nxi\\n1 + eβT ˆ\\nxi\\n!\\n= −\\nm\\nX\\ni=1\\nˆ\\nxi (yi −p1 (ˆ\\nxi; β))\\n→_→\\n配套视频教程：https://www.bilibili.com/video/BV1Mh411e7VU\\n←_←', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 38, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='→_→\\n欢迎去各大电商平台选购纸质版南瓜书《机器学习公式详解第2 版》\\n←_←\\n此式也可以进行向量化，令p1(ˆ\\nxi; β) = ˆ\\nyi，代入上式得\\n∂ℓ(β)\\n∂β\\n= −\\nm\\nX\\ni=1\\nˆ\\nxi(yi −ˆ\\nyi)\\n=\\nm\\nX\\ni=1\\nˆ\\nxi(ˆ\\nyi −yi)\\n= XT(ˆ\\ny −y)\\n其中ˆ\\ny = (ˆ\\ny1; ˆ\\ny2; ...; ˆ\\nym), y = (y1; y2; ...; ym)。\\n3.3.6\\n式(3.31) 的推导\\n继续对上述式(3.30) 中倒数第二个等号的结果求导\\n∂2ℓ(β)\\n∂β∂βT = −\\n∂Pm\\ni=1 ˆ\\nxi\\n\\x10\\nyi −\\neβT ˆ\\nxi\\n1+eT ˆ\\nxi\\n\\x11\\n∂βT\\n= −\\nm\\nX\\ni=1\\nˆ\\nxi\\n∂\\n\\x10\\nyi −\\neβT ˆ\\nxi\\n1+eβT ˆ\\nxi\\n\\x11\\n∂βT\\n= −\\nm\\nX\\ni=1\\nˆ\\nxi\\n\\uf8eb\\n\\uf8ed∂yi\\n∂βT −\\n∂\\n\\x10\\neβT ˆ\\nxi\\n1+eβT ˆ\\nxi\\n\\x11\\n∂βT\\n\\uf8f6\\n\\uf8f8\\n=\\nm\\nX\\ni=1\\nˆ\\nxi ·\\n∂\\n\\x10\\neβT ˆ\\nxi\\n1+eβT ˆ\\nxi\\n\\x11\\n∂βT\\n根据矩阵微分公式∂aTx\\n∂xT = ∂xTa', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 39, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='eβT ˆ\\nxi\\n1+eβT ˆ\\nxi\\n\\x11\\n∂βT\\n根据矩阵微分公式∂aTx\\n∂xT = ∂xTa\\n∂xT = aT，其中\\n∂\\n\\x10\\neβT ˆ\\nxi\\n1+eβT ˆ\\nxi\\n\\x11\\n∂βT\\n=\\n∂eβT ˆ\\nxi\\n∂βT\\n·\\n\\x10\\n1 + eβT ˆ\\nxi\\n\\x11\\n−eβT ˆ\\nxi ·\\n∂\\n(\\n1+eβT ˆ\\nxi\\n)\\n∂βT\\n\\x001 + eβT ˆ\\nxi\\x012\\n=\\nˆ\\nxT\\ni eβT ˆ\\nxi ·\\n\\x10\\n1 + eβT ˆ\\nxi\\n\\x11\\n−eβT ˆ\\nxi · ˆ\\nxT\\ni eβT ˆ\\nxi\\n\\x001 + eβT ˆ\\nxi\\x012\\n= ˆ\\nxT\\ni eβT ˆ\\nxi ·\\n\\x10\\n1 + eβT ˆ\\nxi\\n\\x11\\n−eβT ˆ\\nxi\\n\\x001 + eβT ˆ\\nxi\\x012\\n= ˆ\\nxT\\ni eβT ˆ\\nxi ·\\n1\\n\\x001 + eβT ˆ\\nxi\\x012\\n= ˆ\\nxT\\ni ·\\neβT ˆ\\nxi\\n1 + eβT ˆ\\nxi ·\\n1\\n1 + eβT ˆ\\nxi\\n所以\\n∂2ℓ(β)\\n∂β∂βT =\\nm\\nX\\ni=1\\nˆ\\nxi · ˆ\\nxT\\ni ·\\neβT ˆ\\nxi\\n1 + eβT ˆ\\nxi ·\\n1\\n1 + eβT ˆ\\nxi\\n=\\nm\\nX\\ni=1', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 39, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='eβT ˆ\\nxi\\n1 + eβT ˆ\\nxi ·\\n1\\n1 + eβT ˆ\\nxi\\n=\\nm\\nX\\ni=1\\nˆ\\nxiˆ\\nxT\\ni p1 (ˆ\\nxi; β) (1 −p1 (ˆ\\nxi; β))\\n3.4\\n线性判别分析\\n线性判别分析的一般使用流程如下：首先在训练集上学得模型\\ny = wTx\\n→_→\\n配套视频教程：https://www.bilibili.com/video/BV1Mh411e7VU\\n←_←', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 39, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='→_→\\n欢迎去各大电商平台选购纸质版南瓜书《机器学习公式详解第2 版》\\n←_←\\n由向量内积的几何意义可知，y 可以看作是x 在w 上的投影，因此在训练集上学得的模型能够保证训练\\n集中的同类样本在w 上的投影y 很相近，而异类样本在w 上的投影y 很疏远。然后对于新的测试样本\\nxi，将其代入模型得到它在w 上的投影yi，然后判别这个投影yi 与哪一类投影更近，则将其判为该类。\\n最后，线性判别分析也是一种降维方法，但不同于第10 章介绍的无监督降维方法，线性判别分析是\\n一种监督降维方法，即降维过程中需要用到样本类别标记信息。\\n3.4.1\\n式(3.32) 的推导\\n式(3.32) 中∥wTµ0 −wTµ1∥2\\n2 右下角的“2”表示求“2 范数”\\n，向量的2 范数即为模，右上角的“2”\\n表示求平方数，基于此，下面推导式(3.32)。\\nJ = ∥wTµ0 −wTµ1∥2\\n2\\nwT(Σ0 + Σ1)w\\n= ∥(wTµ0 −wTµ1)T∥2\\n2\\nwT(Σ0 + Σ1)w\\n= ∥(µ0 −µ1)Tw∥2\\n2\\nwT(Σ0 + Σ1)w\\n=\\n\\x02\\n(µ0 −µ1)Tw\\n\\x03T (µ0 −µ1)Tw', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 40, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='2\\nwT(Σ0 + Σ1)w\\n=\\n\\x02\\n(µ0 −µ1)Tw\\n\\x03T (µ0 −µ1)Tw\\nwT(Σ0 + Σ1)w\\n= wT(µ0 −µ1)(µ0 −µ1)Tw\\nwT(Σ0 + Σ1)w\\n3.4.2\\n式(3.37) 到式(3.39) 的推导\\n由式(3.36)，可定义拉格朗日函数为\\nL(w, λ) = −wTSbw + λ(wTSww −1)\\n对w 求偏导可得\\n∂L(w, λ)\\n∂w\\n= −∂(wTSbw)\\n∂w\\n+ λ∂(wTSww −1)\\n∂w\\n= −(Sb + ST\\nb )w + λ(Sw + ST\\nw)w\\n由于Sb = ST\\nb , Sw = ST\\nw，所以\\n∂L(w, λ)\\n∂w\\n= −2Sbw + 2λSww\\n令上式等于0 即可得\\n−2Sbw + 2λSww = 0\\nSbw = λSww\\n(µ0 −µ1)(µ0 −µ1)Tw = λSww\\n若令(µ0 −µ1)Tw = γ，则有\\nγ(µ0 −µ1) = λSww\\nw = γ\\nλS−1\\nw (µ0 −µ1)\\n由于最终要求解的w 不关心其大小，只关心其方向，所以其大小可以任意取值。又因为µ0 和µ1 的大小', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 40, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='由于最终要求解的w 不关心其大小，只关心其方向，所以其大小可以任意取值。又因为µ0 和µ1 的大小\\n是固定的，所以γ 的大小只受w 的大小影响，因此可以通过调整w 的大小使得γ = λ，西瓜书中所说的\\n“不妨令Sbw = λ(µ0 −µ1)”也可等价理解为令γ = λ，因此，此时γ\\nλ = 1，求解出的w 即为式(3.39)。\\n→_→\\n配套视频教程：https://www.bilibili.com/video/BV1Mh411e7VU\\n←_←', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 40, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='→_→\\n欢迎去各大电商平台选购纸质版南瓜书《机器学习公式详解第2 版》\\n←_←\\n3.4.3\\n式(3.43) 的推导\\n由式(3.40)、式(3.41)、式(3.42) 可得\\nSb = St −Sw\\n=\\nm\\nX\\ni=1\\n(xi −µ)(xi −µ)T −\\nN\\nX\\ni=1\\nX\\nx∈Xi\\n(x −µi)(x −µi)T\\n=\\nN\\nX\\ni=1\\n X\\nx∈Xi\\n\\x00(x −µ)(x −µ)T −(x −µi)(x −µi)T\\x01\\n!\\n=\\nN\\nX\\ni=1\\n X\\nx∈Xi\\n\\x00(x −µ)(xT −µT) −(x −µi)(xT −µT\\ni )\\n\\x01\\n!\\n=\\nN\\nX\\ni=1\\n X\\nx∈Xi\\n\\x00xxT −xµT −µxT + µµT −xxT + xµT\\ni + µixT −µiµT\\ni\\n\\x01\\n!\\n=\\nN\\nX\\ni=1\\n X\\nx∈Xi\\n\\x00−xµT −µxT + µµT + xµT\\ni + µixT −µiµT\\ni\\n\\x01\\n!\\n=\\nN\\nX\\ni=1\\n \\n−\\nX\\nx∈Xi\\nxµT −\\nX\\nx∈Xi\\nµxT +\\nX\\nx∈Xi\\nµµT +\\nX\\nx∈Xi\\nxµT\\ni +\\nX\\nx∈Xi\\nµixT −\\nX\\nx∈Xi\\nµiµT\\ni\\n!\\n=', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 41, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='X\\nx∈Xi\\nxµT\\ni +\\nX\\nx∈Xi\\nµixT −\\nX\\nx∈Xi\\nµiµT\\ni\\n!\\n=\\nN\\nX\\ni=1\\n\\x00−miµiµT −miµµT\\ni + miµµT + miµiµT\\ni + miµiµT\\ni −miµiµT\\ni\\n\\x01\\n=\\nN\\nX\\ni=1\\n\\x00−miµiµT −miµµT\\ni + miµµT + miµiµT\\ni\\n\\x01\\n=\\nN\\nX\\ni=1\\nmi\\n\\x00−µiµT −µµT\\ni + µµT + µiµT\\ni\\n\\x01\\n=\\nN\\nX\\ni=1\\nmi(µi −µ)(µi −µ)T\\n3.4.4\\n式(3.44) 的推导\\n此式是式(3.35) 的推广形式，证明如下。\\n设W = (w1, w2, ..., wi, ..., wN−1) ∈Rd×(N−1)，其中wi ∈Rd×1 为d 行1 列的列向量，则\\n\\uf8f1\\n\\uf8f4\\n\\uf8f4\\n\\uf8f4\\n\\uf8f4\\n\\uf8f4\\n\\uf8f2\\n\\uf8f4\\n\\uf8f4\\n\\uf8f4\\n\\uf8f4\\n\\uf8f4\\n\\uf8f3\\ntr(WTSbW) =\\nN−1\\nX\\ni=1\\nwT\\ni Sbwi\\ntr(WTSwW) =\\nN−1\\nX\\ni=1\\nwT\\ni Swwi\\n所以式(3.44) 可变形为\\nmax\\nW\\nPN−1\\ni=1 wT\\ni Sbwi\\nPN−1\\ni=1 wT\\ni Swwi', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 41, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='max\\nW\\nPN−1\\ni=1 wT\\ni Sbwi\\nPN−1\\ni=1 wT\\ni Swwi\\n对比式(3.35) 易知，上式即式(3.35) 的推广形式。\\n除了式(3.35) 以外，还有一种常见的优化目标形式如下\\nmax\\nW\\nQN−1\\ni=1 wT\\ni Sbwi\\nQN−1\\ni=1 wT\\ni Swwi\\n= max\\nW\\nN−1\\nY\\ni=1\\nwT\\ni Sbwi\\nwT\\ni Swwi\\n无论是采用何种优化目标形式，其优化目标只要满足“同类样例的投影点尽可能接近，异类样例的投\\n影点尽可能远离”即可。\\n→_→\\n配套视频教程：https://www.bilibili.com/video/BV1Mh411e7VU\\n←_←', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 41, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='→_→\\n欢迎去各大电商平台选购纸质版南瓜书《机器学习公式详解第2 版》\\n←_←\\n3.4.5\\n式(3.45) 的推导\\n同式(3.35)，此处也固定式(3.44) 的分母为1，那么式(3.44) 此时等价于如下优化问题\\nmin\\nw\\n−tr(WTSbW)\\ns.t.\\ntr(WTSwW) = 1\\n根据拉格朗日乘子法，可定义上述优化问题的拉格朗日函数\\nL(W, λ) = −tr(WTSbW) + λ(tr(WTSwW) −1)\\n根据矩阵微分公式\\n∂\\n∂X tr (XTBX) = (B + BT)X 对上式关于W 求偏导可得\\n∂L(W, λ)\\n∂W\\n= −∂\\n\\x00tr(WTSbW)\\n\\x01\\n∂W\\n+ λ∂\\n\\x00tr(WTSwW) −1\\n\\x01\\n∂W\\n= −(Sb + ST\\nb )W + λ(Sw + ST\\nw)W\\n由于Sb = ST\\nb , Sw = ST\\nw，所以\\n∂L(W, λ)\\n∂W\\n= −2SbW + 2λSwW\\n令上式等于0 即可得\\n−2SbW + 2λSwW = 0\\nSbW = λSwW\\n此即为式(3.45)，但是此式在解释为何要取N −1 个最大广义特征值所对应的特征向量来构成W 时不够', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 42, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='直观。因此，我们换一种更为直观的方式求解式(3.44)，只需换一种方式构造拉格朗日函数即可。\\n重新定义上述优化问题的拉格朗日函数\\nL(W, Λ) = −tr(WTSbW) + tr\\n\\x00Λ(WTSwW −I)\\n\\x01\\n其中，I ∈R(N−1)×(N−1) 为单位矩阵，Λ = diag(λ1, λ2, ..., λN−1) ∈R(N−1)×(N−1) 是由N −1 个拉格朗日\\n乘子构成的对角矩阵。根据矩阵微分公式\\n∂\\n∂X tr(XTAX) = (A + AT)X,\\n∂\\n∂X tr(XAXTB) =\\n∂\\n∂X tr(AXTBX) =\\nBTXAT + BXA，对上式关于W 求偏导可得\\n∂L(W, Λ)\\n∂W\\n= −∂\\n\\x00tr(WTSbW)\\n\\x01\\n∂W\\n+ ∂\\n\\x00tr\\n\\x00ΛWTSwW −ΛI\\n\\x01\\x01\\n∂W\\n= −(Sb + ST\\nb )W + (ST\\nwWΛT + SwWΛ)\\n由于Sb = ST\\nb , Sw = ST\\nw, ΛT = Λ，所以\\n∂L(W, Λ)\\n∂W\\n= −2SbW + 2SwWΛ\\n令上式等于0 即可得\\n−2SbW + 2SwWΛ = 0\\nSbW = SwWΛ\\n将W 和Λ 展开可得', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 42, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='−2SbW + 2SwWΛ = 0\\nSbW = SwWΛ\\n将W 和Λ 展开可得\\nSbwi = λiSwwi,\\ni = 1, 2, ..., N −1\\n此时便得到了N −1 个广义特征值问题。进一步地，将其代入优化问题的目标函数可得\\nmin\\nW −tr(WTSbW) = max\\nW tr(WTSbW)\\n= max\\nW\\nN−1\\nX\\ni=1\\nwT\\ni Sbwi\\n= max\\nW\\nN−1\\nX\\ni=1\\nλiwT\\ni Swwi\\n→_→\\n配套视频教程：https://www.bilibili.com/video/BV1Mh411e7VU\\n←_←', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 42, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='→_→\\n欢迎去各大电商平台选购纸质版南瓜书《机器学习公式详解第2 版》\\n←_←\\n由于存在约束tr(WTSwW) =\\nN−1\\nP\\ni=1\\nwT\\ni Swwi = 1，所以欲使上式取到最大值，只需取N −1 个最大的λi 即\\n可。根据Sbwi = λiSwwi 可知，λi 对应的便是广义特征值，wi 是λi 所对应的特征向量。\\n（广义特征值的定义和常用求解方法可查阅[3]）\\n对于N 分类问题，一定要求出N −1 个wi 吗？其实不然。之所以将W 定义为d × (N −1) 维的矩\\n阵是因为当d > (N −1) 时，实对称矩阵S−1\\nw Sb 的秩至多为N −1，所以理论上至多能解出N −1 个非零\\n特征值λi 及其对应的特征向量wi。但是S−1\\nw Sb 的秩是受当前训练集中的数据分布所影响的，因此并不一\\n定为N −1。此外，当数据分布本身就足够理想时，即使能求解出多个wi，但是实际可能只需要求解出1\\n个wi 便可将同类样本聚集，异类样本完全分离。\\n当d > (N −1) 时，实对称矩阵S−1\\nw Sb 的秩至多为N −1 的证明过程如下：由于µ =\\n1\\nN\\nN\\nP\\ni=1\\nmiµi，所', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 43, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='w Sb 的秩至多为N −1 的证明过程如下：由于µ =\\n1\\nN\\nN\\nP\\ni=1\\nmiµi，所\\n以µ1 −µ 一定可以由µ 和µ2, ..., µN 线性表示，因此矩阵Sb 中至多有µ2 −µ, ..., µN −µ 共N −1 个\\n线性无关的向量，由于此时d > (N −1)，所以Sb 的秩r(Sb) 至多为N −1。同时假设矩阵Sw 满秩，即\\nr(Sw) = r(S−1\\nw ) = d，则根据矩阵秩的性质r(AB) ⩽min{r(A), r(B)} 可知，S−1\\nw Sb 的秩也至多为N −1。\\n3.5\\n多分类学习\\n3.5.1\\n图3.5 的解释\\n图3.5 中所说的“海明距离”是指两个码对应位置不相同的个数，\\n“欧式距离”则是指两个向量之间\\n的欧氏距离，例如图3.5(a) 中第1 行的编码可以视作为向量(−1, +1, −1, +1, +1)，测试示例的编码则为\\n(−1, −1, +1, −1, +1)，其中第2 个、第3 个、第4 个元素不相同，所以它们的海明距离为3，欧氏距离为\\np', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 43, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='p\\n(−1 −(−1))2 + (1 −(−1))2 + (−1 −1)2 + (1 −(−1))2 + (1 −1)2 = √0 + 4 + 4 + 4 + 0 = 2\\n√\\n3。需要注\\n意的是，在计算海明距离时，与“停用类”不同算作0.5，例如图3.5(b) 中第2 行的海明距离计算公式为\\n0.5 + 0.5 + 0.5 + 0.5 = 2。\\n3.6\\n类别不平衡问题\\n对于类别不平衡问题，\\n“西瓜书”2.3.1 节中的“精度”通常无法满足该特殊任务的需求，例如“西瓜\\n书”在本节第一段的举例：有998 个反例和2 个正例，若机器学习算法返回一个永远将新样本预测为反\\n例的学习器则能达到99.8% 的精度，显然虚高，因此在类别不平衡时常采用2.3 节中的查准率、查全率和\\nF1 来度量学习器的性能。\\n参考文献\\n[1] 王燕军. 最优化基础理论与方法. 复旦大学出版社, 2011.\\n[2] Wikipedia contributors. Matrix calculus, 2022.\\n[3] 张贤达. 矩阵分析与应用. 第2 版. 清华大学出版社, 2013.\\n→_→', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 43, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='[3] 张贤达. 矩阵分析与应用. 第2 版. 清华大学出版社, 2013.\\n→_→\\n配套视频教程：https://www.bilibili.com/video/BV1Mh411e7VU\\n←_←', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 43, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='→_→\\n欢迎去各大电商平台选购纸质版南瓜书《机器学习公式详解第2 版》\\n←_←\\n第4 章\\n决策树\\n本章的决策树算法背后没有复杂的数学推导，其更符合人类日常思维方式，理解起来也更为直观，其\\n引入的数学工具也仅是为了让该算法在计算上可行，同时“西瓜书”在本章列举了大量例子，因此本章的\\n算法会更为通俗易懂。\\n4.1\\n基本流程\\n作为本章的开篇，首先要明白决策树在做什么。正如“西瓜书”中图4.1 所示的决策过程，决策树就\\n是不断根据某属性进行划分的过程（每次决策时都是在上次决策结果的基础之上进行）\\n，即“if⋯⋯elif⋯⋯\\nelse⋯⋯”的决策过程，最终得出一套有效的判断逻辑，便是学到的模型。但是，划分到什么时候就停止\\n划分呢？这就是图4.2 中的3 个“return”代表的递归返回，下面解释图4.2 中的3 个递归返回。\\n首先，应该明白决策树的基本思想是根据某种原则（即图4.2 第8 行）每次选择一个属性作为划分依\\n据，然后按属性的取值将数据集中的样本进行划分，例如将所有触感为“硬滑”的西瓜的分到一起，将所', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 44, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='据，然后按属性的取值将数据集中的样本进行划分，例如将所有触感为“硬滑”的西瓜的分到一起，将所\\n有触感为“软粘”的西瓜分到一起，划分完得到若干子集，接着再对各个子集按照以上流程重新选择某个\\n属性继续递归划分，然而在划分的过程中通常会遇到以下几种特殊情况。\\n（1）若递归划分过程中某个子集中已经只含有某一类的样本（例如只含好瓜）\\n，那么此时划分的目的\\n已经达到了，无需再进行递归划分，此即为递归返回的情形(1)，最极端的情况就是初始数据集中的样本\\n全是某一类的样本，那么此时决策树算法到此终止，建议尝试其他算法；\\n（2）递归划分时每次选择一个属性作为划分依据，并且该属性通常不能重复使用（仅针对离散属性）\\n，\\n原因是划分后产生的各个子集在该属性上的取值相同。例如本次根据触感对西瓜样本进行划分，那么后面\\n对划分出的子集（及子集的子集⋯⋯）再次进行递归划分时不能再使用“触感”\\n，图4.2 第14 行的A\\\\{a∗}\\n表示的便是从候选属性集合A 中将当前正在使用的属性a∗排除。由于样本的属性个数是有限的，因此划', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 44, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='表示的便是从候选属性集合A 中将当前正在使用的属性a∗排除。由于样本的属性个数是有限的，因此划\\n分次数通常不超过属性个数。若所有属性均已被用作过划分依据，即A = ∅，此时子集中仍含有不同类样\\n本（例如仍然同时含有好瓜和坏瓜）\\n，但是因已无属性可用作划分依据，此时只能少数服从多数，以此子\\n集中样本数最多的类为标记。由于无法继续划分的直接原因是各个子集中的样本在各个属性上的取值都相\\n同，所以即使A ̸= ∅，但是当子集中的样本在属性集合A 上取值都相同时，等价视为A = ∅，此即为递\\n归返回的情形(2)；\\n（3）\\n根据某个属性进行划分时，\\n若该属性多个属性值中的某个属性值不包含任何样本\\n（例如未收集到）\\n，\\n例如对当前子集以“纹理”属性来划分，\\n“纹理”共有3 种取值：清晰、稍糊、模糊，但发现当前子集中并\\n无样本“纹理”属性取值为模糊，此时对于取值为清晰的子集和取值为稍糊的子集继续递归，而对于取值\\n为模糊的分支，因为无样本落入，将其标记为叶结点，其类别标记为训练集D 中样本最多的类，即把全体\\n样本的分布作为当前结点的先验分布。其实就是一种盲猜，既然是盲猜，那么合理的做法就是根据已有数', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 44, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='样本的分布作为当前结点的先验分布。其实就是一种盲猜，既然是盲猜，那么合理的做法就是根据已有数\\n据用频率近似概率的思想假设出现频率最高的便是概率最大的。注意，此分支必须保留，因为测试时，可\\n能会有样本落入该分支。此即为递归返回的情形(3)。\\n4.2\\n划分选择\\n本节介绍的三种划分选择方法，即信息增益、增益率、基尼指数分别对应著名的ID3、C4.5 和CART\\n三种决策树算法。\\n4.2.1\\n式(4.1) 的解释\\n该式为信息论中的信息熵定义式，以下先证明0 ⩽Ent(D) ⩽log2 |Y|，然后解释其最大值和最小值所\\n表示的含义。\\n→_→\\n配套视频教程：https://www.bilibili.com/video/BV1Mh411e7VU\\n←_←', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 44, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='→_→\\n欢迎去各大电商平台选购纸质版南瓜书《机器学习公式详解第2 版》\\n←_←\\n已知集合D 的信息熵的定义为\\nEnt(D) = −\\n|Y|\\nX\\nk=1\\npk log2 pk\\n其中，|Y| 表示样本类别总数，pk 表示第k 类样本所占的比例，有0 ⩽pk ⩽1, Pn\\nk=1 pk = 1。若令\\n|Y| = n, pk = xk，那么信息熵Ent(D) 就可以看作一个n 元实值函数，即\\nEnt(D) = f(x1, · · · , xn) = −\\nn\\nX\\nk=1\\nxk log2 xk\\n其中0 ⩽xk ⩽1, Pn\\nk=1 xk = 1。\\n下面考虑求该多元函数的最值.\\n首先我们先来求最大值，如果不考虑约束0 ⩽xk ⩽1 而仅考虑\\nPn\\nk=1 xk = 1，则对f(x1, · · · , xn) 求最大值等价于如下最小化问题：\\nmin\\nn\\nP\\nk=1\\nxk log2 xk\\ns.t.\\nn\\nP\\nk=1\\nxk = 1\\n显然，在0 ⩽xk ⩽1 时，此问题为凸优化问题。对于凸优化问题来说，使其拉格朗日函数的一阶偏导数等\\n于0 的点即最优解。根据拉格朗日乘子法可知，该优化问题的拉格朗日函数为', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 45, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='于0 的点即最优解。根据拉格朗日乘子法可知，该优化问题的拉格朗日函数为\\nL(x1, · · · , xn, λ) =\\nn\\nX\\nk=1\\nxk log2 xk + λ\\n n\\nX\\nk=1\\nxk −1\\n!\\n其中，λ 为拉格朗日乘子。对L(x1, · · · , xn, λ) 分别关于x1, · · · , xn, λ 求一阶偏导数，并令偏导数等于0\\n可得\\n∂L(x1, · · · , xn, λ)\\n∂x1\\n=\\n∂\\n∂x1\\n\" n\\nX\\nk=1\\nxk log2 xk + λ\\n n\\nX\\nk=1\\nxk −1\\n!#\\n= 0\\n= log2 x1 + x1 ·\\n1\\nx1 ln 2 + λ = 0\\n= log2 x1 +\\n1\\nln 2 + λ = 0\\n⇒λ = −log2 x1 −\\n1\\nln 2\\n∂L(x1, · · · , xn, λ)\\n∂x2\\n=\\n∂\\n∂x2\\n\" n\\nX\\nk=1\\nxk log2 xk + λ\\n n\\nX\\nk=1\\nxk −1\\n!#\\n= 0\\n⇒λ = −log2 x2 −\\n1\\nln 2\\n· · ·\\n∂L(x1, · · · , xn, λ)\\n∂xn\\n=\\n∂\\n∂xn\\n\" n\\nX\\nk=1', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 45, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='· · ·\\n∂L(x1, · · · , xn, λ)\\n∂xn\\n=\\n∂\\n∂xn\\n\" n\\nX\\nk=1\\nxk log2 xk + λ\\n n\\nX\\nk=1\\nxk −1\\n!#\\n= 0\\n⇒λ = −log2 xn −\\n1\\nln 2;\\n∂L(x1, · · · , xn, λ)\\n∂λ\\n= ∂\\n∂λ\\n\" n\\nX\\nk=1\\nxk log2 xk + λ\\n n\\nX\\nk=1\\nxk −1\\n!#\\n= 0\\n⇒\\nn\\nX\\nk=1\\nxk = 1\\n整理一下可得\\n\\uf8f1\\n\\uf8f2\\n\\uf8f3\\nλ = −log2 x1 −\\n1\\nln 2 = −log2 x2 −\\n1\\nln 2 = · · · = −log2 xn −\\n1\\nln 2\\nn\\nP\\nk=1\\nxk = 1\\n→_→\\n配套视频教程：https://www.bilibili.com/video/BV1Mh411e7VU\\n←_←', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 45, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='→_→\\n欢迎去各大电商平台选购纸质版南瓜书《机器学习公式详解第2 版》\\n←_←\\n由以上两个方程可以解得\\nx1 = x2 = · · · = xn = 1\\nn\\n又因为xk 还需满足约束0 ⩽xk ⩽1，显然0 ⩽1\\nn ⩽1，所以x1 = x2 = · · · = xn = 1\\nn 是满足所有约束的\\n最优解，即当前最小化问题的最小值点，同时也是f(x1, · · · , xn) 的最大值点。将x1 = x2 = · · · = xn = 1\\nn\\n代入f(x1, · · · , xn) 中可得\\nf\\n\\x12 1\\nn, · · · , 1\\nn\\n\\x13\\n= −\\nn\\nX\\nk=1\\n1\\nn log2\\n1\\nn = −n · 1\\nn log2\\n1\\nn = log2 n\\n所以f(x1, · · · , xn) 在满足约束0 ⩽xk ⩽1, Pn\\nk=1 xk = 1 时的最大值为log2 n。\\n下面求最小值。如果不考虑约束Pn\\nk=1 xk = 1 而仅考虑0 ⩽xk ⩽1，则f(x1, · · · , xn) 可以看作n 个\\n互不相关的一元函数的和，即\\nf(x1, · · · , xn) =\\nn\\nX\\nk=1', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 46, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='互不相关的一元函数的和，即\\nf(x1, · · · , xn) =\\nn\\nX\\nk=1\\ng(xk)\\n其中，\\ng(xk) = −xk log2 xk, 0 ⩽xk ⩽1。\\n那么当g(x1), g(x2), · · · , g(xn) 分别取到其最小值时，\\nf(x1, · · · , xn)\\n也就取到了最小值，所以接下来考虑分别求g(x1), g(x2), · · · , g(xn) 各自的最小值。\\n由于g(x1), g(x2), · · · , g(xn) 的定义域和函数表达式均相同，所以只需求出g(x1) 的最小值也就求出\\n了g(x2), · · · , g(xn) 的最小值。下面考虑求g(x1) 的最小值，首先对g(x1) 关于x1 求一阶和二阶导数，有\\ng′(x1) = d(−x1 log2 x1)\\ndx1\\n= −log2 x1 −x1 ·\\n1\\nx1 ln 2 = −log2 x1 −\\n1\\nln 2\\ng′′(x1) = d (g′(x1))\\ndx1\\n=\\nd\\n\\x12\\n−log2 x1 −\\n1\\nln 2\\n\\x13\\ndx1\\n= −\\n1\\nx1 ln 2', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 46, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='dx1\\n=\\nd\\n\\x12\\n−log2 x1 −\\n1\\nln 2\\n\\x13\\ndx1\\n= −\\n1\\nx1 ln 2\\n显然，当0 ⩽xk ⩽1 时g′′(x1) = −\\n1\\nx1 ln 2 恒小于0，所以g(x1) 是一个在其定义域范围内开口向下的凹函\\n数，那么其最小值必然在边界取。分别取x1 = 0 和x1 = 1，代入g(x1) 可得\\ng(0) = −0 log2 0 = 0\\ng(1) = −1 log2 1 = 0\\n（计算信息熵时约定：若x = 0，则x log2 x = 0）所以，\\ng(x1) 的最小值为0，\\n同理可得g(x2), · · · , g(xn)\\n的最小值也都为0，即f(x1, · · · , xn) 的最小值为0。但是，此时仅考虑约束0 ⩽xk ⩽1，而未考虑\\nPn\\nk=1 xk = 1。若考虑约束Pn\\nk=1 xk = 1，那么f(x1, · · · , xn) 的最小值一定大于等于0。如果令某个\\nxk = 1，那么根据约束Pn\\nk=1 xk = 1 可知x1 = x2 = · · · = xk−1 = xk+1 = · · · = xn = 0，将其代入', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 46, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='f(x1, · · · , xn) 可得\\nf(0, 0, · · · , 0, 1, 0, · · · , 0)\\n= −0 log2 0 −0 log2 0 −· · · −0 log2 0 −1 log2 1 −0 log2 0 −· · · −0 log2 0 = 0\\n所以xk = 1, x1 = x2 = · · · = xk−1 = xk+1 = · · · = xn = 0 一定是f(x1, · · · , xn) 在满足约束Pn\\nk=1 xk = 1\\n和0 ⩽xk ⩽1 的条件下的最小值点，此时f 取到最小值0。\\n综上可知，当f(x1, · · · , xn) 取到最大值时：x1 = x2 = · · · = xn = 1\\nn，此时样本集合纯度最低；当\\nf(x1, · · · , xn) 取到最小值时：xk = 1, x1 = x2 = · · · = xk−1 = xk+1 = · · · = xn = 0，此时样本集合纯度最\\n高。\\n→_→\\n配套视频教程：https://www.bilibili.com/video/BV1Mh411e7VU\\n←_←', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 46, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='→_→\\n欢迎去各大电商平台选购纸质版南瓜书《机器学习公式详解第2 版》\\n←_←\\n4.2.2\\n式(4.2) 的解释\\n此为信息增益的定义式。在信息论中信息增益也称为“互信息”\\n，表示已知一个随机变量的信息后另\\n一个随机变量的不确定性减少的程度。\\n下面给出互信息的定义，在此之前，还需要先解释一下什么是“条件熵”\\n。条件熵表示的是在已知一\\n个随机变量的条件下，另一个随机变量的不确定性。具体地，假设有随机变量X 和Y ，且它们服从以下\\n联合概率分布\\nP(X = xi, Y = yj) = pij,\\ni = 1, 2, · · · , n,\\nj = 1, 2, · · · , m\\n那么在已知X 的条件下，随机变量Y 的条件熵为\\nEnt(Y |X) =\\nn\\nX\\ni=1\\npi Ent(Y |X = xi)\\n其中，pi = P(X = xi)\\uffffi = 1, 2, · · · , n。互信息定义为信息熵和条件熵的差，它表示的是已知一个随机变量\\n的信息后使得另一个随机变量的不确定性减少的程度。具体地，假设有随机变量X 和Y ，那么在已知X\\n的信息后，Y 的不确定性减少的程度为', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 47, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='的信息后，Y 的不确定性减少的程度为\\nI(Y ; X) = Ent(Y ) −Ent(Y |X)\\n此即互信息的数学定义。\\n所以式(4.2) 可以理解为，在已知属性a 的取值后，样本类别这个随机变量的不确定性减小的程度。\\n若根据某个属性计算得到的信息增益越大，则说明在知道其取值后样本集的不确定性减小的程度越大，即\\n“西瓜书”上所说的“纯度提升”越大。\\n4.2.3\\n式(4.4) 的解释\\n为了理解该式的“固有值”的概念，可以将式(4.4) 与式(4.1) 对比理解。式(4.1) 可重写为\\nEnt(D) = −\\n|Y|\\nX\\nk=1\\npk log2 pk = −\\n|Y|\\nX\\nk=1\\n\\x0c\\n\\x0cDk\\x0c\\n\\x0c\\n|D| log2\\n\\x0c\\n\\x0cDk\\x0c\\n\\x0c\\n|D|\\n其中|Dk|\\n|D| = pk，为第k 类样本所占的比例。与式(4.4) 的表达式作一下对比\\nIV(a) = −\\nV\\nX\\nv=1\\n|Dv|\\n|D| log2\\n|Dv|\\n|D|\\n其中|Dv|\\n|D| = pv，为属性a 取值为av 的样本所占的比例。即式(4.1) 是按样本的类别标记计算的信息熵，\\n而式(4.4) 是按样本属性的取值计算的信息熵。', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 47, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='而式(4.4) 是按样本属性的取值计算的信息熵。\\n4.2.4\\n式(4.5) 的推导\\n假设数据集D 中的样例标记种类共有三类，每类样本所占比例分别为p1、p2、p3。现从数据集中随\\n机抽取两个样本，两个样本类别标记正好一致的概率为\\np1p1 + p2p2 + p3p3 =\\n|Y|=3\\nX\\nk=1\\np2\\nk\\n两个样本类别标记不一致的概率为（即“基尼值”\\n）\\nGini(D) = p1p2 + p1p3 + p2p1 + p2p3 + p3p1 + p3p2 =\\n|Y|=3\\nX\\nk=1\\nX\\nk′̸=k\\npkpk′\\n→_→\\n配套视频教程：https://www.bilibili.com/video/BV1Mh411e7VU\\n←_←', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 47, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='→_→\\n欢迎去各大电商平台选购纸质版南瓜书《机器学习公式详解第2 版》\\n←_←\\n易证以上两式之和等于1，证明过程如下\\n|Y|=3\\nX\\nk=1\\np2\\nk +\\n|Y|=3\\nX\\nk=1\\nX\\nk′̸=k\\npkpk′\\n= (p1p1 + p2p2 + p3p3) + (p1p2 + p1p3 + p2p1 + p2p3 + p3p1 + p3p2)\\n= (p1p1 + p1p2 + p1p3) + (p2p1 + p2p2 + p2p3) + (p3p1 + p3p2 + p3p3)\\n=p1 (p1 + p2 + p3) + p2 (p1 + p2 + p3) + p3 (p1 + p2 + p3)\\n=p1 + p2 + p3 = 1\\n所以可进一步推得式(4.5)\\nGini(D) =\\n|Y|\\nX\\nk=1\\nX\\nk′̸=k\\npkpk′ = 1 −\\n|Y|\\nX\\nk=1\\np2\\nk\\n从数据集中D 任取两个样本，\\n类别标记一致的概率越大表示其纯度越高\\n（即大部分样本属于同一类）\\n，\\n类别标记不一致的概率（即基尼值）越大表示纯度越低。\\n4.2.5\\n式(4.6) 的解释', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 48, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='，\\n类别标记不一致的概率（即基尼值）越大表示纯度越低。\\n4.2.5\\n式(4.6) 的解释\\n此为数据集D 中属性a 的基尼指数的定义，表示在属性a 的取值已知的条件下，数据集D 按照属\\n性a 的所有可能取值划分后的纯度。不过在构造CART 决策树时并不会严格按照此式来选择最优划分属\\n性，主要是因为CART 决策树是一棵二叉树，如果用上式去选出最优划分属性，无法进一步选出最优划\\n分属性的最优划分点。常用的CART 决策树的构造算法如下\\n[1]：\\n(1) 考虑每个属性a 的每个可能取值v，将数据集D 分为a = v 和a ̸= v 两部分来计算基尼指数，即\\nGini_index(D, a) = |Da=v|\\n|D|\\nGini(Da=v) + |Da̸=v|\\n|D|\\nGini(Da̸=v)\\n(2) 选择基尼指数最小的属性及其对应取值作为最优划分属性和最优划分点；\\n(3) 重复以上两步，直至满足停止条件。\\n下面以“西瓜书”中表4.2 中西瓜数据集2.0 为例来构造CART 决策树，其中第一个最优划分属性', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 48, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='下面以“西瓜书”中表4.2 中西瓜数据集2.0 为例来构造CART 决策树，其中第一个最优划分属性\\n和最优划分点的计算过程如下：以属性“色泽”为例，它有3 个可能的取值：{青绿，乌黑，浅白}，若使\\n用该属性的属性值是否等于“青绿”对数据集D 进行划分，则可得到2 个子集，分别记为D1(色泽=\\n青绿), D2(色泽̸= 青绿)。子集D1 包含编号{1, 4, 6, 10, 13, 17} 共6 个样例，其中正例占p1 = 3\\n6，反例占\\np2 = 3\\n6；子集D2 包含编号{2, 3, 5, 7, 8, 9, 11, 12, 14, 15, 16} 共11 个样例，其中正例占p1 =\\n5\\n11，反例占\\np2 =\\n6\\n11，根据式(4.5) 可计算出用“色泽= 青绿”划分之后得到基尼指数为\\nGini_index(D, 色泽= 青绿)\\n= 6\\n17 ×\\n \\n1 −\\n\\x123\\n6\\n\\x132\\n−\\n\\x123\\n6\\n\\x132!\\n+ 11\\n17 ×\\n \\n1 −\\n\\x12 5\\n11\\n\\x132\\n−\\n\\x12 6\\n11\\n\\x132!\\n= 0.497\\n类似地，可以计算出不同属性取不同值的基尼指数如下：', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 48, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='\\x132\\n−\\n\\x12 6\\n11\\n\\x132!\\n= 0.497\\n类似地，可以计算出不同属性取不同值的基尼指数如下：\\nGini_index(D, 色泽= 乌黑) = 0.456\\nGini_index(D, 色泽= 浅白) = 0.426\\nGini_index(D, 根蒂= 蜷缩) = 0.456\\nGini_index(D, 根蒂= 稍蜷) = 0.496\\nGini_index(D, 根蒂= 硬挺) = 0.439\\n→_→\\n配套视频教程：https://www.bilibili.com/video/BV1Mh411e7VU\\n←_←', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 48, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='→_→\\n欢迎去各大电商平台选购纸质版南瓜书《机器学习公式详解第2 版》\\n←_←\\nGini_index(D, 敲声= 浊响) = 0.450\\nGini_index(D, 敲声= 沉闷) = 0.494\\nGini_index(D, 敲声= 清脆) = 0.439\\nGini_index(D, 纹理= 清晰) = 0.286\\nGini_index(D, 纹理= 稍稀) = 0.437\\nGini_index(D, 纹理= 模糊) = 0.403\\nGini_index(D, 脐部= 凹陷) = 0.415\\nGini_index(D, 脐部= 稍凹) = 0.497\\nGini_index(D, 脐部= 平坦) = 0.362\\nGini_index(D, 触感= 硬挺) = 0.494\\nGini_index(D, 触感= 软粘) = 0.494\\n特别地，对于属性“触感”\\n，由于它的可取值个数为2，所以其实只需计算其中一个取值的基尼指数即可。\\n根据上面的计算结果可知，Gini_index(D, 纹理= 清晰) = 0.286 最小，所以选择属性“纹理”为最优', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 49, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='划分属性并生成根节点，接着以“纹理= 清晰”为最优划分点生成D1(纹理= 清晰)、D2(纹理̸= 清晰)\\n两个子节点，对两个子节点分别重复上述步骤继续生成下一层子节点，直至满足停止条件。\\n以上便是CART 决策树的构建过程，从构建过程可以看出，CART 决策树最终构造出来的是一棵二\\n叉树。CART 除了决策树能处理分类问题以外，回归树还可以处理回归问题，下面给出CART 回归树的\\n构造算法。\\n假设给定数据集\\nD = {(x1, y1), (x2, y2), · · · , (xN, yN)}\\n其中x ∈Rd 为d 维特征向量，y ∈R 是连续型随机变量。这是一个标准的回归问题的数据集, 若把每个\\n属性视为坐标空间中的一个坐标轴，则d 个属性就构成了一个d 维的特征空间，而每个d 维特征向量x\\n就对应了d 维的特征空间中的一个数据点。CART 回归树的目标是将特征空间划分成若干个子空间，每\\n个子空间都有一个固定的输出值，也就是凡是落在同一个子空间内的数据点xi，它们所对应的输出值yi\\n恒相等，且都为该子空间的输出值。\\n那么如何划分出若干个子空间呢？这里采用一种启发式的方法。', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 49, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='恒相等，且都为该子空间的输出值。\\n那么如何划分出若干个子空间呢？这里采用一种启发式的方法。\\n(1) 任意选择一个属性a，遍历其所有可能取值，根据下式找出属性a 最优划分点v∗：\\nv∗= arg min\\nv\\n\\uf8ee\\n\\uf8f0min\\nc1\\nX\\nxi∈R1(a,v)\\n(yi −c1)2 + min\\nc2\\nX\\nxi∈R2(a,v)\\n(yi −c2)2\\n\\uf8f9\\n\\uf8fb\\n其中，R1(a, v) = {x|x ∈Da⩽v}, R2(a, v) = {x|x ∈Da>v}，c1 和c2 分别为集合R1(a, v) 和\\nR2(a, v) 中的样本xi 对应的输出值yi 的均值，即\\nc1 = ave(yi|x ∈R1(a, v)) =\\n1\\n|R1(a, v)|\\nX\\nxi∈R1(a,v)\\nyi\\nc2 = ave(yi|x ∈R2(a, v)) =\\n1\\n|R2(a, v)|\\nX\\nxi∈R2(a,v)\\nyi\\n→_→\\n配套视频教程：https://www.bilibili.com/video/BV1Mh411e7VU\\n←_←', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 49, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='→_→\\n欢迎去各大电商平台选购纸质版南瓜书《机器学习公式详解第2 版》\\n←_←\\n(2) 遍历所有属性，找到最优划分属性a∗，然后根据a∗的最优划分点v∗将特征空间划分为两个子空\\n间，接着对每个子空间重复上述步骤，直至满足停止条件. 这样就生成了一棵CART 回归树，假\\n设最终将特征空间划分为M 个子空间R1, R2, · · · , RM，那么CART 回归树的模型式可以表示为\\nf(x) =\\nM\\nX\\nm=1\\ncmI(x ∈Rm)\\n同理，其中的cm 表示的也是集合Rm 中的样本xi 对应的输出值yi 的均值。此式直观上的理解\\n就是，对于一个给定的样本xi，首先判断其属于哪个子空间，然后将其所属的子空间对应的输出\\n值作为该样本的预测值yi。\\n4.3\\n剪枝处理\\n本节内容通俗易懂，跟着“西瓜书”中的例子动手演算即可，无需做过多解释。以下仅结合图4.5 继\\n续讨论一下图4.2 中的递归返回条件。图4.5 与图4.4 均是基于信息增益生成的决策树，不同在于图4.4\\n基于表4.1，而图4.5 基于表4.2 的训练集。\\n结点3 包含训练集“脐部”为稍凹的样本（编号6、7、15、17）', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 50, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='结点3 包含训练集“脐部”为稍凹的样本（编号6、7、15、17）\\n，当根据“根蒂”再次进行划分时不\\n含有“根蒂”为硬挺的样本（递归返回情形(3)）\\n，而恰巧四个样本（编号6、7、15、17）含两个好瓜和两\\n个坏瓜，因此叶结点硬挺的类别随机从类别好瓜和坏瓜中选择其一。\\n结点5 包含训练集“脐部”为稍凹且“根蒂”为稍蜷的样本（编号6、7、15）\\n，当根据“色泽”再次\\n进行划分时不含有“色泽”为浅白的样本（递归返回情形(3)）\\n，因此叶结点浅白类别标记为好瓜（编号6、\\n7、15 样本中，前两个为好瓜，最后一个为坏瓜）\\n。\\n结点6 包含训练集“脐部”为稍凹、\\n“根蒂”为稍蜷、\\n“色泽”为乌黑的样本（编号7、15）\\n，当根据“纹\\n理”再次进行划分时不含有“纹理”为模糊的样本（递归返回情形(3)）\\n，而恰巧两个样本（编号7、15）含\\n好瓜和坏瓜各一个，因此叶结点模糊的类别随机从类别好瓜和坏瓜中选择其一。\\n图4.5 两次随机选择均选为好瓜，实际上表示了一种归纳偏好（参见第1 章1.4 节）\\n。\\n4.4\\n连续与缺失值\\n连续与缺失值的预处理均属于特征工程的范畴。', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 50, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='。\\n4.4\\n连续与缺失值\\n连续与缺失值的预处理均属于特征工程的范畴。\\n有些分类器只能使用离散属性，当遇到连续属性时则需要特殊处理，有兴趣可以通过关键词“连续属\\n性离散化”或者“Discretization”查阅更多处理方法。结合第11 章11.2 节至11.4 节分别介绍的“过滤\\n式”算法、\\n“包裹式”算法、\\n“嵌入式”算法的概念，若先使用某个离散化算法对连续属性离散化后再调用\\nC4.5 决策树生成算法，则是一种过滤式算法，若如4.4.1 节所述，则应该属于嵌入式算法，因为并没有以\\n学习器的预测结果准确率为评价标准，而是与决策树生成过程融为一体，因此不应该划入包裹式算法。\\n类似地，有些分类器不能使用含有缺失值的样本，需要进行预处理。常用的缺失值填充方法是：对于\\n连续属性，采用该属性的均值进行填充；对于离散属性，采用属性值个数最多的样本进行填充。这实际上\\n假设了数据集中的样本是基于独立同分布采样得到的。特别地，一般缺失值仅指样本的属性值有缺失，若\\n类别标记有缺失，一般会直接抛弃该样本。当然，也可以尝试根据第11 章11.6 节的式(11.24)，在低秩假\\n设下对数据集缺失值进行填充。', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 50, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='设下对数据集缺失值进行填充。\\n4.4.1\\n式(4.7) 的解释\\n此式所表达的思想很简单，\\n就是以每两个相邻取值的中点作为划分点。\\n下面以\\n“西瓜书”\\n中表4.3 中西瓜\\n数据集3.0 为例来说明此式的用法。\\n对于\\n“密度”\\n这个连续属性，\\n已观测到的可能取值为{0.243, 0.245, 0.343,\\n0.360, 0.403, 0.437, 0.481, 0.556, 0.593, 0.608, 0.634, 0.639, 0.657, 0.666, 0.697, 0.719, 0.774} 共17 个值，\\n根据\\n→_→\\n配套视频教程：https://www.bilibili.com/video/BV1Mh411e7VU\\n←_←', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 50, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='→_→\\n欢迎去各大电商平台选购纸质版南瓜书《机器学习公式详解第2 版》\\n←_←\\n式(4.7) 可知，此时i 依次取1 到16，那么“密度”这个属性的候选划分点集合为\\nTa = {0.243 + 0.245\\n2\\n, 0.245 + 0.343\\n2\\n, 0.343 + 0.360\\n2\\n, 0.360 + 0.403\\n2\\n, 0.403 + 0.437\\n2\\n, 0.437 + 0.481\\n2\\n,\\n0.481 + 0.556\\n2\\n, 0.556 + 0.593\\n2\\n, 0.593 + 0.608\\n2\\n, 0.608 + 0.634\\n2\\n, 0.634 + 0.639\\n2\\n, 0.639 + 0.657\\n2\\n,\\n0.657 + 0.666\\n2\\n, 0.666 + 0.697\\n2\\n, 0.697 + 0.719\\n2\\n, 0.719 + 0.774\\n2\\n}\\n4.4.2\\n式(4.8) 的解释\\n此式是式(4.2) 用于离散化后的连续属性的版本，其中Ta 由式(4.7) 计算得来，λ ∈{−, +} 表示属\\n性a 的取值分别小于等于和大于候选划分点t 时的情形，即当λ = −时有Dλ\\nt = Da⩽t\\nt', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 51, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='性a 的取值分别小于等于和大于候选划分点t 时的情形，即当λ = −时有Dλ\\nt = Da⩽t\\nt\\n，当λ = + 时有\\nDλ\\nt = Da>t\\nt\\n。\\n4.4.3\\n式(4.12) 的解释\\n该式括号内与式(4.2) 基本一样，区别在于式(4.2) 中的|Dv|\\n|D| 改为式(4.11) 的˜\\nrv，在根据式(4.1) 计\\n算信息熵时第k 类样本所占的比例改为式(4.10) 的˜\\npk；所有计算结束后再乘以式(4.9) 的ρ。\\n有关式(4.9) (4.10) (4.11) 中的权重wx，初始化为1。以图4.9 为例，在根据“纹理”进行划分时，除\\n编号为8、10 的两个样本在此属性缺失之外，其余样本根据自身在该属性上的取值分别划入稍糊、清晰、\\n模糊三个子集，而编号为8、10 的两个样本则要按比例同时划入三个子集。具体来说，稍糊子集包含样本\\n7、9、13、14、17 共5 个样本，清晰子集包含样本1、2、3、4、5、6、15 共7 个样本，模糊子集包含样\\n本10、11、16 共3 个样本，总共15 个在该属性不含缺失值的样本，而此时各样本的权重wx 初始化为1，', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 51, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='因此编号为8、10 的两个样本分到稍糊、清晰、模糊三个子集的权重分别为\\n5\\n15, 7\\n15 和\\n3\\n15。\\n4.5\\n多变量决策树\\n本节内容也通俗易懂，以下仅对部分图做进一步解释说明。\\n4.5.1\\n图(4.10) 的解释\\n只想用该图强调一下，离散属性不可以重复使用，但连续属性是可以重复使用的。\\n4.5.2\\n图(4.11) 的解释\\n对照“西瓜书”中图4.10 的决策树，下面给出图4.11 中的划分边界产出过程。\\n在下图4-1中，斜纹阴影部分表示已确定标记为坏瓜的样本，点状阴影部分表示已确定标记为好瓜的\\n样本，\\n空白部分表示需要进一步划分的样本。\\n第一次划分条件是\\n“含糖率⩽0.126?”\\n，\\n满足此条件的样本直\\n接被标记为坏瓜（如图4-1(a) 斜纹阴影部分所示）\\n，而不满足此条件的样本还需要进一步划分（如图4-1(a)\\n空白部分所示）\\n。\\n在第一次划分的基础上对图4-1(a) 空白部分继续进行划分，第二次划分条件是“密度⩽0.381?”\\n，满\\n足此条件的样本直接被标记为坏瓜（如图4-1(b) 新增斜纹阴影部分所示）\\n，而不满足此条件的样本还需要', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 51, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='，而不满足此条件的样本还需要\\n进一步划分（如图4-1(b) 空白部分所示）\\n。\\n在第二次划分的基础上对图4-1(b) 空白部分继续进行划分，第三次划分条件是“含糖率⩽0.205?”\\n，\\n不满足此条件的样本直接标记为好瓜（如图4-1(c) 新增点状阴影部分所示）\\n，而满足此条件的样本还需进\\n一步划分（如图4-1(c) 空白部分所示）\\n。\\n在第三次划分的基础上对图4-1(c) 空白部分继续进行划分，\\n第四次划分的条件是\\n“密度⩽0.560?”\\n，\\n满\\n足此条件的样本直接标记为好瓜（如图4-1(d) 新增点状阴影部分所示）\\n，而不满足此条件的样本直接标记\\n为坏瓜（如图4-1(d) 新增斜纹阴影部分所示）\\n。\\n→_→\\n配套视频教程：https://www.bilibili.com/video/BV1Mh411e7VU\\n←_←', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 51, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='→_→\\n欢迎去各大电商平台选购纸质版南瓜书《机器学习公式详解第2 版》\\n←_←\\n经过四次划分已无空白部分，表示决策树生成完毕，从图4-1(d) 中可以清晰地看出好瓜与坏瓜的分类\\n边界。\\n含糖率\\n密度\\n0.6\\n0.4\\n0.2\\n0.2\\n0.4\\n0.6\\n0.8\\n0\\n(a) 第一次划分\\n含糖率\\n密度\\n0.6\\n0.4\\n0.2\\n0.2\\n0.4\\n0.6\\n0.8\\n0\\n(b) 第二次划分\\n含糖率\\n密度\\n0.6\\n0.4\\n0.2\\n0.2\\n0.4\\n0.6\\n0.8\\n0\\n(c) 第三次划分\\n含糖率\\n密度\\n0.6\\n0.4\\n0.2\\n0.2\\n0.4\\n0.6\\n0.8\\n0\\n(d) 第四次划分\\n图4-1 图4.11 中的划分边界产出过程\\n参考文献\\n[1] 李航. 统计学习方法. 清华大学出版社, 2012.\\n→_→\\n配套视频教程：https://www.bilibili.com/video/BV1Mh411e7VU\\n←_←', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 52, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='→_→\\n欢迎去各大电商平台选购纸质版南瓜书《机器学习公式详解第2 版》\\n←_←\\n第5 章\\n神经网络\\n神经网络类算法可以堪称当今最主流的一类机器学习算法，其本质上和前几章讲到的线性回归、对数\\n几率回归、决策树等算法一样均属于机器学习算法，也是被发明用来完成分类和回归等任务。不过由于神\\n经网络类算法在如今超强算力的加持下效果表现极其出色，且从理论角度来说神经网络层堆叠得越深其效\\n果越好，因此也单独称用深层神经网络类算法所做的机器学习为深度学习，属于机器学习的子集。\\n5.1\\n神经元模型\\n本节对神经元模型的介绍通俗易懂，\\n在此不再赘述。\\n本节第2 段提到\\n“阈值”\\n(threshold) 的概念时，\\n“西\\n瓜书”左侧边注特意强调是“阈(yù)”而不是“阀(fá)”\\n，这是因为该字确实很容易认错，读者注意一下\\n即可。\\n图5.1 所示的M-P 神经元模型，其中的“M-P”便是两位作者McCulloch 和Pitts 的首字母简写。\\n5.2\\n感知机与多层网络\\n5.2.1\\n式(5.1) 和式(5.2) 的推导\\n此式是感知机学习算法中的参数更新公式，下面依次给出感知机模型、学习策略和学习算法的具体介\\n绍', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 53, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='此式是感知机学习算法中的参数更新公式，下面依次给出感知机模型、学习策略和学习算法的具体介\\n绍\\n[1]：\\n感知机模型：已知感知机由两层神经元组成，故感知机模型的公式可表示为\\ny = f\\n n\\nX\\ni=1\\nwixi −θ\\n!\\n= f(wTx −θ)\\n其中，x ∈Rn，为样本的特征向量，是感知机模型的输入；w, θ 是感知机模型的参数，w ∈Rn，为权重，\\nθ 为阈值。假定f 为阶跃函数，那么感知机模型的公式可进一步表示为（用ε(·) 代表阶跃函数）\\ny = ε(wTx −θ) =\\n(\\n1,\\nwTx −θ ⩾0;\\n0,\\nwTx −θ < 0.\\n由于n 维空间中的超平面方程为\\nw1x1 + w2x2 + · · · + wnxn + b = wTx + b = 0\\n所以此时感知机模型公式中的wTx −θ 可以看作是n 维空间中的一个超平面，将n 维空间划分为wTx −\\nθ ⩾0 和wTx −θ < 0 两个子空间，落在前一个子空间的样本对应的模型输出值为1，落在后一个子空间\\n的样本对应的模型输出值为0，如此便实现了分类功能。\\n感知机学习策略：给定一个数据集', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 53, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='的样本对应的模型输出值为0，如此便实现了分类功能。\\n感知机学习策略：给定一个数据集\\nT = {(x1, y1), (x2, y2), · · · , (xN, yN)}\\n其中xi ∈Rn, yi ∈{0, 1}, i = 1, 2, · · · , N。如果存在某个超平面\\nwTx + b = 0\\n能将数据集T 中的正样本和负样本完全正确地划分到超平面两侧，即对所有yi = 1 的样本xi 有wTxi +\\nb ⩾0，对所有yi = 0 的样本xi 有wTxi + b < 0，则称数据集T 线性可分，否则称数据集T 线性不可分。\\n现给定一个线性可分的数据集T，感知机的学习目标是求得能对数据集T 中的正负样本完全正确划\\n分的分离超平面\\nwTx −θ = 0\\n→_→\\n配套视频教程：https://www.bilibili.com/video/BV1Mh411e7VU\\n←_←', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 53, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='→_→\\n欢迎去各大电商平台选购纸质版南瓜书《机器学习公式详解第2 版》\\n←_←\\n假设此时误分类样本集合为M ⊆T，对任意一个误分类样本(x, y) ∈M 来说，当wTx −θ ⩾0 时，模型\\n输出值为ˆ\\ny = 1，样本真实标记为y = 0；反之，当wTx −θ < 0 时，模型输出值为ˆ\\ny = 0，样本真实标记\\n为y = 1。综合两种情形可知，以下公式恒成立：\\n(ˆ\\ny −y)\\n\\x00wTx −θ\\n\\x01\\n⩾0\\n所以，给定数据集T，其损失函数可以定义为\\nL(w, θ) =\\nX\\nx∈M\\n(ˆ\\ny −y)\\n\\x00wTx −θ\\n\\x01\\n显然，此损失函数是非负的。如果没有误分类点，则损失函数值为0。而且，误分类点越少，误分类点离\\n超平面越近（超平面相关知识参见本书6.1.2 节）\\n，损失函数值就越小。因此，给定数据集T，损失函数\\nL(w, θ) 是关于w, θ 的连续可导函数。\\n感知机学习算法：感知机模型的学习问题可以转化为求解损失函数的最优化问题，具体地，给定数据\\n集\\nT = {(x1, y1), (x2, y2), · · · , (xN, yN)}', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 54, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='集\\nT = {(x1, y1), (x2, y2), · · · , (xN, yN)}\\n其中xi ∈Rn, yi ∈{0, 1}，求参数w, θ，使其为极小化损失函数的解：\\nmin\\nw,θ L(w, θ) = min\\nw,θ\\nX\\nxi∈M\\n(ˆ\\nyi −yi)(wTxi −θ)\\n其中M ⊆T 为误分类样本集合。若将阈值θ 看作一个固定输入为−1 的“哑节点”\\n，即\\n−θ = −1 · wn+1 = xn+1 · wn+1\\n那么wTxi −θ 可化简为\\nwTxi −θ =\\nn\\nX\\nj=1\\nwjxj + xn+1 · wn+1\\n=\\nn+1\\nX\\nj=1\\nwjxj\\n= wTxi\\n其中xi ∈Rn+1, w ∈Rn+1。根据该公式，可将要求解的极小化问题进一步简化为\\nmin\\nw L(w) = min\\nw\\nX\\nxi∈M\\n(ˆ\\nyi −yi)wTxi\\n假设误分类样本集合M 固定，那么可以求得损失函数L(w) 的梯度\\n∇wL(w) =\\nX\\nxi∈M\\n(ˆ\\nyi −yi)xi\\n感知机的学习算法具体采用的是随机梯度下降法，即在极小化过程中，不是一次使M 中所有误分类点的', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 54, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='感知机的学习算法具体采用的是随机梯度下降法，即在极小化过程中，不是一次使M 中所有误分类点的\\n梯度下降，而是一次随机选取一个误分类点并使其梯度下降。所以权重w 的更新公式为\\nw ←w + ∆w\\n∆w = −η(ˆ\\nyi −yi)xi = η(yi −ˆ\\nyi)xi\\n相应地，w 中的某个分量wi 的更新公式即式(5.2)。\\n实践中常用的求解方法是先随机初始化一个模型权重w0，此时将训练集中的样本一一代入模型便可\\n确定误分类点集合M，然后从M 中随机抽选取一个误分类点计算得到∆w，接着按照上述权重更新公式\\n计算得到新的权重w1 = w0 + ∆w，并重新确定误分类点集合，如此迭代直至误分类点集合为空，即训练\\n样本中的样本均完全正确分类。显然，随机初始化的w0 不同，每次选取的误分类点不同，最后都有可能\\n导致求解出的模型不同，因此感知模型的解不唯一。\\n→_→\\n配套视频教程：https://www.bilibili.com/video/BV1Mh411e7VU\\n←_←', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 54, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='→_→\\n欢迎去各大电商平台选购纸质版南瓜书《机器学习公式详解第2 版》\\n←_←\\n5.2.2\\n图5.5 的解释\\n图5.5 中(0, 0), (0, 1), (1, 0), (1, 1) 这4 个样本点实现“异或”计算的过程如下：\\n(x1, x2) →h1 = ε(x1 −x2 −0.5), h2 = ε(x2 −x1 −0.5) →y = ε(h1 + h2 −0.5)\\n以(0, 1) 为例，\\n首先求得h1 = ε(0−1−0.5) = 0, h2 = ε(1−0−0.5) = 1，\\n然后求得y = ε(0+1−0.5) = 1。\\n5.3\\n误差逆传播算法\\n5.3.1\\n式(5.10) 的推导\\n参见式(5.12) 的推导\\n5.3.2\\n式(5.12) 的推导\\n因为\\n∆θj = −η\\n∂Ek\\n∂θj\\n又\\n∂Ek\\n∂θj\\n=\\n∂Ek\\n∂ˆ\\nyk\\nj\\n·\\n∂ˆ\\nyk\\nj\\n∂θj\\n=\\n∂Ek\\n∂ˆ\\nyk\\nj\\n·\\n∂[f(βj −θj)]\\n∂θj\\n=\\n∂Ek\\n∂ˆ\\nyk\\nj\\n· f ′(βj −θj) × (−1)\\n=\\n∂Ek\\n∂ˆ\\nyk\\nj', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 55, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='=\\n∂Ek\\n∂ˆ\\nyk\\nj\\n· f ′(βj −θj) × (−1)\\n=\\n∂Ek\\n∂ˆ\\nyk\\nj\\n· f (βj −θj) × [1 −f (βj −θj)] × (−1)\\n=\\n∂Ek\\n∂ˆ\\nyk\\nj\\n· ˆ\\nyk\\nj\\n\\x001 −ˆ\\nyk\\nj\\n\\x01\\n× (−1)\\n=\\n∂\\n\\uf8ee\\n\\uf8f01\\n2\\nl\\nP\\nj=1\\n\\x00ˆ\\nyk\\nj −yk\\nj\\n\\x012\\n\\uf8f9\\n\\uf8fb\\n∂ˆ\\nyk\\nj\\n· ˆ\\nyk\\nj\\n\\x001 −ˆ\\nyk\\nj\\n\\x01\\n× (−1)\\n=\\n1\\n2 × 2(ˆ\\nyk\\nj −yk\\nj ) × 1 · ˆ\\nyk\\nj\\n\\x001 −ˆ\\nyk\\nj\\n\\x01\\n× (−1)\\n= (yk\\nj −ˆ\\nyk\\nj )ˆ\\nyk\\nj\\n\\x001 −ˆ\\nyk\\nj\\n\\x01\\n= gj\\n所以\\n∆θj = −η\\n∂Ek\\n∂θj\\n= −ηgj\\n5.3.3\\n式(5.13) 的推导\\n因为\\n∆vih = −η\\n∂Ek\\n∂vih\\n→_→\\n配套视频教程：https://www.bilibili.com/video/BV1Mh411e7VU\\n←_←', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 55, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='→_→\\n欢迎去各大电商平台选购纸质版南瓜书《机器学习公式详解第2 版》\\n←_←\\n又\\n∂Ek\\n∂vih\\n=\\nl\\nX\\nj=1\\n∂Ek\\n∂ˆ\\nyk\\nj\\n·\\n∂ˆ\\nyk\\nj\\n∂βj\\n·\\n∂βj\\n∂bh\\n·\\n∂bh\\n∂αh\\n·\\n∂αh\\n∂vih\\n=\\nl\\nX\\nj=1\\n∂Ek\\n∂ˆ\\nyk\\nj\\n·\\n∂ˆ\\nyk\\nj\\n∂βj\\n·\\n∂βj\\n∂bh\\n·\\n∂bh\\n∂αh\\n· xi\\n=\\nl\\nX\\nj=1\\n∂Ek\\n∂ˆ\\nyk\\nj\\n·\\n∂ˆ\\nyk\\nj\\n∂βj\\n·\\n∂βj\\n∂bh\\n· f ′(αh −γh) · xi\\n=\\nl\\nX\\nj=1\\n∂Ek\\n∂ˆ\\nyk\\nj\\n·\\n∂ˆ\\nyk\\nj\\n∂βj\\n· whj · f ′(αh −γh) · xi\\n=\\nl\\nX\\nj=1\\n(−gj) · whj · f ′(αh −γh) · xi\\n= −f ′(αh −γh) ·\\nl\\nX\\nj=1\\ngj · whj · xi\\n= −bh(1 −bh) ·\\nl\\nX\\nj=1\\ngj · whj · xi\\n= −eh · xi\\n所以\\n∆vih = −η\\n∂Ek\\n∂vih\\n= ηehxi\\n5.3.4\\n式(5.14) 的推导\\n因为\\n∆γh = −η\\n∂Ek\\n∂γh', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 56, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='= ηehxi\\n5.3.4\\n式(5.14) 的推导\\n因为\\n∆γh = −η\\n∂Ek\\n∂γh\\n又\\n∂Ek\\n∂γh\\n=\\nl\\nX\\nj=1\\n∂Ek\\n∂ˆ\\nyk\\nj\\n·\\n∂ˆ\\nyk\\nj\\n∂βj\\n·\\n∂βj\\n∂bh\\n·\\n∂bh\\n∂γh\\n=\\nl\\nX\\nj=1\\n∂Ek\\n∂ˆ\\nyk\\nj\\n·\\n∂ˆ\\nyk\\nj\\n∂βj\\n·\\n∂βj\\n∂bh\\n· f ′(αh −γh) · (−1)\\n= −\\nl\\nX\\nj=1\\n∂Ek\\n∂ˆ\\nyk\\nj\\n·\\n∂ˆ\\nyk\\nj\\n∂βj\\n· whj · f ′(αh −γh)\\n= −\\nl\\nX\\nj=1\\n∂Ek\\n∂ˆ\\nyk\\nj\\n·\\n∂ˆ\\nyk\\nj\\n∂βj\\n· whj · bh(1 −bh)\\n=\\nl\\nX\\nj=1\\ngj · whj · bh(1 −bh)\\n= eh\\n所以\\n∆γh = −η\\n∂Ek\\n∂γh\\n= −ηeh\\n→_→\\n配套视频教程：https://www.bilibili.com/video/BV1Mh411e7VU\\n←_←', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 56, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='→_→\\n欢迎去各大电商平台选购纸质版南瓜书《机器学习公式详解第2 版》\\n←_←\\n5.3.5\\n式(5.15) 的推导\\n参见式(5.13) 的推导\\n5.4\\n全局最小与局部极小\\n由图5.10 可以直观理解局部极小和全局最小的概念，其余概念如模拟退火、遗传算法、启发式等，则\\n需要查阅专业资料系统化学习。\\n5.5\\n其他常见神经网络\\n本节所提到的神经网络其实如今已不太常见，更为常见的神经网络是下一节深度学习里提到的卷积神\\n经网络、循环神经网络等。\\n5.5.1\\n式(5.18) 的解释\\n从式(5.18) 可以看出，对于样本x 来说，RBF 网络的输出为q 个ρ(x, ci) 的线性组合。若换个角\\n度来看这个问题，将q 个ρ(x, ci) 当作是将d 维向量x 基于式(5.19) 进行特征转换后所得的q 维特征，\\n即˜\\nx = (ρ(x, c1); ρ(x, c2); ...; ρ(x, cq))，则式(5.18) 求线性加权系数wi 相当于求解第3.2 节的线性回归\\nf(˜\\nx) = wT˜\\nx + b，对于仅有的差别b 来说，当然可以在式(5.18) 中补加一个b。因此，RBF 网络在确定', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 57, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='q 个神经元中心ci 之后，接下来要做的就是线性回归。\\n5.5.2\\n式(5.20) 的解释\\nBoltzmann 机（Restricted Boltzmann Machine，简称RBM）本质上是一个引入了隐变量的无向图模\\n型，其能量可理解为\\nEgraph = Eedges + Enodes\\n其中，Egraph 表示图的能量，Eedges 表示图中边的能量，Enodes 表示图中结点的能量。边能量由两连接\\n结点的值及其权重的乘积确定，即Eedgeij = −wijsisj；结点能量由结点的值及其阈值的乘积确定，即\\nEnodei = −θisi。图中边的能量为所有边能量之和为\\nEedges =\\nn−1\\nX\\ni=1\\nn\\nX\\nj=i+1\\nEedgeij = −\\nn−1\\nX\\ni=1\\nn\\nX\\nj=i+1\\nwijsisj\\n图中结点的能量为所有结点能量之和\\nEnodes =\\nn\\nX\\ni=1\\nEnodei = −\\nn\\nX\\ni=1\\nθisi\\n故状态向量s 所对应的Boltzmann 机能量\\nEgraph = Eedges + Enodes = −\\nn−1\\nX\\ni=1\\nn\\nX\\nj=i+1', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 57, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='Egraph = Eedges + Enodes = −\\nn−1\\nX\\ni=1\\nn\\nX\\nj=i+1\\nwijsisj −\\nn\\nX\\ni=1\\nθisi\\n5.5.3\\n式(5.22) 的解释\\n受限Boltzmann 机仅保留显层与隐层之间的连接。显层状态向量v = (v1; v2; ...; vd)，隐层状态向量\\nh = (h1; h2; ...; hq)。显层状态向量v 中的变量vi 仅与隐层状态向量h 有关，所以给定隐层状态向量h，\\n有v1, v2, ..., vd 相互独立。\\n5.5.4\\n式(5.23) 的解释\\n由式(5.22) 的解释同理可得，给定显层状态向量v，有h1, h2, ..., hq 相互独立。\\n→_→\\n配套视频教程：https://www.bilibili.com/video/BV1Mh411e7VU\\n←_←', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 57, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='→_→\\n欢迎去各大电商平台选购纸质版南瓜书《机器学习公式详解第2 版》\\n←_←\\n5.6\\n深度学习\\n“西瓜书”在本节并未对如今深度学习领域的诸多经典神经网络作展开介绍，而是从更宏观的角度详\\n细解释了应该如何理解深度学习。因此，本书也顺着“西瓜书”的思路对深度学习相关概念作进一步说明，\\n对深度学习的经典神经网络感兴趣的读者可查阅其他相关书籍进行系统性学习。\\n5.6.1\\n什么是深度学习\\n深度学习就是很深层的神经网络，而神经网络属于机器学习算法的范畴，因此深度学习是机器学习的\\n子集。\\n5.6.2\\n深度学习的起源\\n深度学习中的经典神经网络以及用于训练神经网络的BP 算法其实在很早就已经被提出，例如卷积神\\n经网络\\n[2] 是在1989 提出，BP 算法\\n[3] 是在1986 年提出，但是在当时的计算机算力水平下，其他非神经\\n网络类算法（例如当时红极一时的支持向量机算法）的效果优于神经网络类算法，因此神经网络类算法进\\n入瓶颈期。随着计算机算力的不断提升，以及2012 年Hinton 和他的学生提出了AlexNet 并在ImageNet', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 58, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='评测中以明显优于第二名的成绩夺冠后，引起了学术界和工业界的广泛关注，紧接着三位深度学习之父\\nLeCun、Bengio 和Hinton 在2015 年正式提出深度学习的概念，自此深度学习开始成为机器学习的主流\\n研究方向。\\n5.6.3\\n怎么理解特征学习\\n举例来说，用非深度学习算法做西瓜分类时，首先需要人工设计西瓜的各个特征，比如根蒂、色泽等，\\n然后将其表示为数学向量，这些过程统称为“特征工程”\\n，完成特征工程后用算法分类即可，其分类效果\\n很大程度上取决于特征工程做得是否够好。而对于深度学习算法来说，只需将西瓜的图片表示为数学向量\\n输入，输出层设置为想要的分类结果即可（例如二分类通常设置为对数几率回归）\\n，之前的“特征工程”交\\n由神经网络来自动完成，即让神经网络进行“特征学习”\\n，通过在输出层约束分类结果，神经网络会自动\\n从西瓜的图片上提取出有助于西瓜分类的特征。\\n因此，如果分别用对数几率回归和卷积神经网络来做西瓜分类，其算法运行流程分别是“人工特征工\\n程→对数几率回归分类”和“卷积神经网络特征学习→对数几率回归分类”\\n。\\n参考文献', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 58, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='程→对数几率回归分类”和“卷积神经网络特征学习→对数几率回归分类”\\n。\\n参考文献\\n[1] 李航. 统计学习方法. 清华大学出版社, 2012.\\n[2] Yann LeCun, Bernhard Boser, John S Denker, Donnie Henderson, Richard E Howard, Wayne Hub-\\nbard, and Lawrence D Jackel. Backpropagation applied to handwritten zip code recognition. Neural\\ncomputation, 1(4):541–551, 1989.\\n[3] David E Rumelhart, Geoffrey E Hinton, and Ronald J Williams. Learning representations by back-\\npropagating errors. nature, 323(6088):533–536, 1986.\\n→_→', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 58, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='→_→\\n配套视频教程：https://www.bilibili.com/video/BV1Mh411e7VU\\n←_←', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 58, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='→_→\\n欢迎去各大电商平台选购纸质版南瓜书《机器学习公式详解第2 版》\\n←_←\\n第6 章\\n支持向量机\\n在深度学习流行之前，支持向量机及其核方法一直是机器学习领域中的主流算法，尤其是核方法至今\\n都仍有相关学者在持续研究。\\n6.1\\n间隔与支持向量\\n6.1.1\\n图6.1 的解释\\n回顾第5 章5.2 节的感知机模型可知，图6.1 中的黑色直线均可作为感知机模型的解，因为感知机模\\n型求解的是能将正负样本完全正确划分的超平面，因此解不唯一。而支持向量机想要求解的则是离正负样\\n本都尽可能远且刚好位于“正中间”的划分超平面，因为这样的超平面理论上泛化性能更好。\\n6.1.2\\n式(6.1) 的解释\\nn 维空间的超平面定义为wTx + b = 0，其中w, x ∈Rn，w = (w1; w2; ...; wn) 称为法向量，b 称为\\n位移项。超平面具有以下性质：\\n(1) 法向量w 和位移项b 确定一个唯一超平面；\\n(2) 超平面方程不唯一，因为当等倍缩放w 和b 时（假设缩放倍数为α），所得的新超平面方程\\nαwTx + αb = 0 和wTx + b = 0 的解完全相同，因此超平面不变，仅超平面方程有变；', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 59, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='(3) 法向量w 垂直于超平面；\\n(4) 超平面将n 维空间切割为两半，其中法向量w 指向的那一半空间称为正空间，另一半称为负空\\n间，正空间中的点x+ 代入进方程wTx+ + b 其计算结果大于0，反之负空间中的点代入进方程其计算结\\n果小于0；\\n(5)n 维空间中的任意点x 到超平面的距离公式为r = |wTx+b|\\n∥w∥\\n，其中∥w∥表示向量w 的模。\\n6.1.3\\n式(6.2) 的推导\\n对于任意一点x0 = (x0\\n1; x0\\n2; ...; x0\\nn)，设其在超平面wTx + b = 0 上的投影点为x1 = (x1\\n1; x1\\n2; ...; x1\\nn)，\\n则wTx1 + b = 0。根据超平面的性质(3) 可知，此时向量−\\n−\\n−\\n→\\nx1x0 与法向量w 平行，因此\\n|w · −\\n−\\n−\\n→\\nx1x0| = |∥w∥· cos π · ∥−\\n−\\n−\\n→\\nx1x0∥| = ∥w∥· ∥−\\n−\\n−\\n→\\nx1x0∥= ∥w∥· r\\n又\\nw · −\\n−\\n−\\n→\\nx1x0 = w1(x0\\n1 −x1\\n1) + w2(x0\\n2 −x1\\n2) + ... + wn(x0\\nn −x1', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 59, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='1 −x1\\n1) + w2(x0\\n2 −x1\\n2) + ... + wn(x0\\nn −x1\\nn)\\n= w1x0\\n1 + w2x0\\n2 + ... + wnx0\\nn −(w1x1\\n1 + w2x1\\n2 + ... + wnx1\\nn)\\n= wTx0 −wTx1\\n= wTx0 + b\\n所以\\n|wTx0 + b| = ∥w∥· r\\nr =\\n\\x0c\\n\\x0cwTx + b\\n\\x0c\\n\\x0c\\n∥w∥\\n6.1.4\\n式(6.3) 的推导\\n支持向量机所要求的超平面需要满足三个条件，第一个是能正确划分正负样本，第二个是要位于正负\\n样本正中间，第三个是离正负样本都尽可能远。式(6.3) 仅满足前两个条件，第三个条件由式(6.5) 来满\\n足，因此下面仅基于前两个条件来进行推导。\\n→_→\\n配套视频教程：https://www.bilibili.com/video/BV1Mh411e7VU\\n←_←', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 59, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='→_→\\n欢迎去各大电商平台选购纸质版南瓜书《机器学习公式详解第2 版》\\n←_←\\n对于第一个条件，当超平面满足该条件时，根据超平面的性质(4) 可知，若yi = +1 的正样本被划分\\n到正空间（当然也可以将其划分到负空间）\\n，yi = −1 的负样本被划分到负空间，以下不等式成立\\n(\\nwTxi + b ⩾0,\\nyi = +1\\nwTxi + b ⩽0,\\nyi = −1\\n对于第二个条件，首先设离超平面最近的正样本为x+\\n∗，离超平面最近的负样本为x−\\n∗，由于这两样本\\n是离超平面最近的点，所以其他样本到超平面的距离均大于等于它们，即\\n\\uf8f1\\n\\uf8f2\\n\\uf8f3\\n|wTxi+b|\\n∥w∥\\n⩾|wTx+\\n∗+b|\\n∥w∥\\n,\\nyi = +1\\n|wTxi+b|\\n∥w∥\\n⩾|wTx−\\n∗+b|\\n∥w∥\\n,\\nyi = −1\\n结合第一个条件中推导出的不等式，可将上式中的绝对值符号去掉并推得\\n(\\nwTxi+b\\n∥w∥\\n⩾wTx+\\n∗+b\\n∥w∥\\n,\\nyi = +1\\nwTxi+b\\n∥w∥\\n⩽wTx−\\n∗+b\\n∥w∥\\n,\\nyi = −1\\n基于此再考虑第二个条件，\\n“位于正负样本正中间”等价于要求超平面到x+\\n∗和x−', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 60, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='yi = −1\\n基于此再考虑第二个条件，\\n“位于正负样本正中间”等价于要求超平面到x+\\n∗和x−\\n∗这两点的距离相等，即\\n\\x0c\\n\\x0cwTx+\\n∗+ b\\n\\x0c\\n\\x0c\\n∥w∥\\n=\\n\\x0c\\n\\x0cwTx−\\n∗+ b\\n\\x0c\\n\\x0c\\n∥w∥\\n综上，支持向量机所要求的超平面所需要满足的条件如下\\n\\uf8f1\\n\\uf8f4\\n\\uf8f4\\n\\uf8f2\\n\\uf8f4\\n\\uf8f4\\n\\uf8f3\\nwTxi+b\\n∥w∥\\n⩾wTx+\\n∗+b\\n∥w∥\\n,\\nyi = +1\\nwTxi+b\\n∥w∥\\n⩽wTx−\\n∗+b\\n∥w∥\\n,\\nyi = −1\\n|wTx+\\n∗+b|\\n∥w∥\\n= |wTx−\\n∗+b|\\n∥w∥\\n但是根据超平面的性质(2) 可知，当等倍缩放法向量w 和位移项b 时，超平面不变，且上式也恒成\\n立，\\n因此会导致所求的超平面的参数w 和b 有无穷多解。\\n因此为了保证每个超平面的参数只有唯一解，\\n不\\n妨再额外施加一些约束，例如约束x+\\n∗和x−\\n∗代入进超平面方程后的绝对值为1，也就是令wTx+\\n∗+ b =\\n1, wTx−\\n∗+ b = −1。此时支持向量机所要求的超平面所需要满足的条件变为\\n(\\nwTxi+b\\n∥w∥\\n⩾\\n+1\\n∥w∥,\\nyi = +1\\nwTxi+b\\n∥w∥\\n⩽\\n−1\\n∥w∥,', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 60, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='wTxi+b\\n∥w∥\\n⩾\\n+1\\n∥w∥,\\nyi = +1\\nwTxi+b\\n∥w∥\\n⩽\\n−1\\n∥w∥,\\nyi = −1\\n由于∥w∥恒大于0，因此上式可进一步化简为\\n(\\nwTxi + b ⩾+1,\\nyi = +1\\nwTxi + b ⩽−1,\\nyi = −1\\n6.1.5\\n式(6.4) 的推导\\n根据式(6.3) 的推导可知，x+\\n∗和x−\\n∗便是“支持向量”\\n，因此支持向量到超平面的距离已经被约束为\\n1\\n∥w∥，所以两个异类支持向量到超平面的距离之和为\\n2\\n∥w∥。\\n6.1.6\\n式(6.5) 的解释\\n式(6.5) 是通过“最大化间隔”来保证超平面离正负样本都尽可能远，且该超平面有且仅有一个，因\\n此可以解出唯一解。\\n→_→\\n配套视频教程：https://www.bilibili.com/video/BV1Mh411e7VU\\n←_←', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 60, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='→_→\\n欢迎去各大电商平台选购纸质版南瓜书《机器学习公式详解第2 版》\\n←_←\\n6.2\\n对偶问题\\n6.2.1\\n凸优化问题\\n考虑一般地约束优化问题\\nmin\\nf(x)\\ns.t.\\ngi(x) ⩽0,\\ni = 1, 2, ..., m\\nhj(x) = 0,\\nj = 1, 2, ..., n\\n若目标函数f(x) 是凸函数，不等式约束gi(x) 是凸函数，等式约束hj(x) 是仿射函数，则称该优化问题\\n为凸优化问题。\\n由于1\\n2∥w∥2 和1 −yi\\n\\x00wTxi + b\\n\\x01\\n均是关于w 和b 的凸函数，所以式(6.6) 是凸优化问题。凸优化问\\n题是最优化里比较易解的一类优化问题，因为其拥有诸多良好的数学性质和现成的数学工具，因此如果非\\n凸优化问题能等价转化为凸优化问题，其求解难度通常也会减小。\\n6.2.2\\nKKT 条件\\n考虑一般的约束优化问题\\nmin\\nf(x)\\ns.t.\\ngi(x) ⩽0,\\ni = 1, 2, ..., m\\nhj(x) = 0,\\nj = 1, 2, ..., n', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 61, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='i = 1, 2, ..., m\\nhj(x) = 0,\\nj = 1, 2, ..., n\\n若f(x), gi(x), hj(x) 的一阶偏导连续，x∗是优化问题的局部解，µ = (µ1; µ2; ...; µm), λ = (λ1; λ2; ...; λn)\\n为拉格朗日乘子向量，L(x, µ, λ) = f(x) + Pm\\ni=1 µigi(x) + Pn\\nj=1 λjhj(x) 为拉格朗日函数，且该优化问题\\n满足任何一个特定的约束限制条件，则一定存在µ∗= (µ∗\\n1; µ∗\\n2; ...; µ∗\\nm), λ∗= (λ∗\\n1; λ∗\\n2; ...; λ∗\\nn)，使得：\\n(1) ∇xL(x∗, µ∗, λ∗) = ∇f(x∗) + Pm\\ni=1 µ∗\\ni ∇gi(x∗) + Pn\\nj=1 λ∗\\nj∇hj(x∗) = 0；\\n(2) hj(x∗) = 0,\\nj = 1, 2, ..., n；\\n(3) gi(x∗) ⩽0,\\ni = 1, 2, ..., m；\\n(4) µ∗\\ni ⩾0,\\ni = 1, 2, ..., m；\\n(5) µ∗\\ni gi(x∗) = 0,', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 61, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='i ⩾0,\\ni = 1, 2, ..., m；\\n(5) µ∗\\ni gi(x∗) = 0,\\ni = 1, 2, ..., m。\\n以上5 条便是Karush–Kuhn–Tucker Conditions\\n（简称KKT 条件）\\n。\\nKKT 条件是局部解的必要条件，\\n也就是说只要该优化问题满足任何一个特定的约束限制条件，局部解就一定会满足以上5 个条件。常用的\\n约束限制条件可查阅维基百科“Karush–Kuhn–Tucker Conditions”词条以及查阅参考文献[1] 的第4.2.2\\n节，若对KKT 条件的数学证明感兴趣可查阅参考文献[1] 的第4.2.1 节。\\n6.2.3\\n拉格朗日对偶函数\\n考虑一般地约束优化问题\\nmin\\nf(x)\\ns.t.\\ngi(x) ⩽0,\\ni = 1, 2, ..., m\\nhj(x) = 0,\\nj = 1, 2, ..., n\\n设上述优化问题的定义域为D = dom f ∩\\nm\\nT\\ni=1\\ndom gi ∩\\nn\\nT\\nj=1\\ndom hj，可行集为˜\\nD = {x|x ∈D, gi(x) ⩽\\n0, hj(x) = 0}（显然˜\\nD 是D 的子集）', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 61, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='D = {x|x ∈D, gi(x) ⩽\\n0, hj(x) = 0}（显然˜\\nD 是D 的子集）\\n，最优值为p∗= min{f(˜\\nx)}, ˜\\nx ∈˜\\nD。上述优化问题的拉格朗日函数\\n定义为\\nL(x, µ, λ) = f(x) +\\nm\\nX\\ni=1\\nµigi(x) +\\nn\\nX\\nj=1\\nλjhj(x)\\n→_→\\n配套视频教程：https://www.bilibili.com/video/BV1Mh411e7VU\\n←_←', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 61, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='→_→\\n欢迎去各大电商平台选购纸质版南瓜书《机器学习公式详解第2 版》\\n←_←\\n其中µ = (µ1; µ2; ...; µm), λ = (λ1; λ2; ...; λn) 为拉格朗日乘子向量。相应地拉格朗日对偶函数Γ(µ, λ)（简\\n称对偶函数）定义为L(x, µ, λ) 关于x 的下确界，即\\nΓ(µ, λ) = inf\\nx∈D L(x, µ, λ) = inf\\nx∈D\\n \\nf(x) +\\nm\\nX\\ni=1\\nµigi(x) +\\nn\\nX\\nj=1\\nλjhj(x)\\n!\\n对偶函数有如下性质：\\n(1) 无论上述优化问题是否为凸优化问题，其对偶函数Γ(µ, λ) 恒为凹函数，详细证明可查阅参考文\\n献[2] 的第5.1.2 和3.2.3 节；\\n(2) 当µ ⪰0 时（µ ⪰0 表示µ 的分量均为非负），Γ(µ, λ) 构成了上述优化问题最优值p∗的下界，\\n即\\nΓ(µ, λ) ⩽p∗\\n其推导过程如下：\\n设˜\\nx ∈˜\\nD 是优化问题的可行点，\\n则gi(˜\\nx) ⩽0, hj(˜\\nx) = 0，\\n因此，\\n当µ ⪰0 时，\\nµigi(˜\\nx) ⩽0, λjhj(˜\\nx) = 0\\n恒成立，所以\\nm\\nX\\ni=1', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 62, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='µigi(˜\\nx) ⩽0, λjhj(˜\\nx) = 0\\n恒成立，所以\\nm\\nX\\ni=1\\nµigi(˜\\nx) +\\nn\\nX\\nj=1\\nλjhj(˜\\nx) ⩽0\\n根据上述不等式可以推得\\nL(˜\\nx, µ, λ) = f(˜\\nx) +\\nm\\nX\\ni=1\\nµigi(˜\\nx) +\\nn\\nX\\nj=1\\nλjhj(˜\\nx) ⩽f(˜\\nx)\\n又\\nΓ(µ, λ) = inf\\nx∈D L(x, µ, λ) ⩽L(˜\\nx, µ, λ)\\n所以\\nΓ(µ, λ) ⩽L(˜\\nx, µ, λ) ⩽f(˜\\nx)\\n进一步地\\nΓ(µ, λ) ⩽min{f(˜\\nx)} = p∗\\n6.2.4\\n拉格朗日对偶问题\\n在µ ⪰0 的约束下求对偶函数最大值的优化问题称为拉格朗日对偶问题（简称对偶问题）\\nmax\\nΓ(µ, λ)\\ns.t.\\nµ ⪰0\\n上一节的优化问题称为主问题或原问题。\\n设对偶问题的最优值为d∗= max{Γ(µ, λ)}, µ ⪰0，根据对偶函数的性质（2）可知d∗⩽p∗，此时称\\n为“弱对偶性”成立，若d∗= p∗，则称为“强对偶性”成立。由此可以看出，当主问题较难求解时，如', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 62, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='为“弱对偶性”成立，若d∗= p∗，则称为“强对偶性”成立。由此可以看出，当主问题较难求解时，如\\n果强对偶性成立，则可以通过求解对偶问题来间接求解主问题。由于约束条件µ ⪰0 是凸集，且根据对偶\\n函数的性质（1）可知Γ(µ, λ) 恒为凹函数，其加个负号即为凸函数，所以无论主问题是否为凸优化问题，\\n对偶问题恒为凸优化问题。\\n一般情况下，强对偶性并不成立，只有当主问题满足特定的约束限制条件（不同于KKT 条件中的约\\n束限制条件）时，强对偶性才成立，常见的有“Slater 条件”\\n。Slater 条件指出，当主问题是凸优化问题，\\n且存在一点x ∈relint D 能使得所有等式约束成立，除仿射函数以外的不等式约束严格成立，则强对偶性\\n成立。由于式(6.6) 是凸优化问题，且不等式约束均为仿射函数，所以式(6.6) 强对偶性成立。\\n对于凸优化问题，还可以通过KKT 条件来间接推导出强对偶性，并同时求解出主问题和对偶问题的\\n最优解。具体地，若主问题为凸优化问题，目标函数f(x) 和约束函数gi(x), hj(x) 的一阶偏导连续，主问\\n→_→', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 62, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='→_→\\n配套视频教程：https://www.bilibili.com/video/BV1Mh411e7VU\\n←_←', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 62, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='→_→\\n欢迎去各大电商平台选购纸质版南瓜书《机器学习公式详解第2 版》\\n←_←\\n题满足KKT 条件中任何一个特定的约束限制条件，则满足KKT 条件的点x∗和(µ∗, λ∗) 分别是主问题\\n和对偶问题的最优解，且此时强对偶性成立。下面给出具体的推导过程。\\n设x∗, µ∗, λ∗是任意满足KKT 条件的点，即\\n\\uf8f1\\n\\uf8f4\\n\\uf8f4\\n\\uf8f4\\n\\uf8f4\\n\\uf8f4\\n\\uf8f4\\n\\uf8f4\\n\\uf8f2\\n\\uf8f4\\n\\uf8f4\\n\\uf8f4\\n\\uf8f4\\n\\uf8f4\\n\\uf8f4\\n\\uf8f4\\n\\uf8f3\\n∇xL(x∗, µ∗, λ∗) = ∇f(x∗) + Pm\\ni=1 µ∗\\ni ∇gi(x∗) + Pn\\nj=1 λ∗\\nj∇hj(x∗) = 0\\nhj(x∗) = 0,\\nj = 1, 2, ..., n\\ngi(x∗) ⩽0,\\ni = 1, 2, ..., m\\nµ∗\\ni ⩾0,\\ni = 1, 2, ..., m\\nµ∗\\ni gi(x∗) = 0,\\ni = 1, 2, ..., m\\n由于主问题是凸优化问题，所以f(x) 和gi(x) 是凸函数，hj(x) 是仿射函数，又因为此时µ∗\\ni ⩾0，所以', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 63, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='i ⩾0，所以\\nL(x, µ∗, λ∗) 是关于x 的凸函数。根据∇xL(x∗, µ∗, λ∗) = 0 可知，此时x∗是L(x, µ∗, λ∗) 的极值点，而\\n凸函数的极值点也是最值点，所以x∗是最小值点，因此可以进一步推得\\nL(x∗, µ∗, λ∗) = min{L(x, µ∗, λ∗)}\\n= inf\\nx∈D\\n \\nf(x) +\\nm\\nX\\ni=1\\nµ∗\\ni gi(x) +\\nn\\nX\\nj=1\\nλ∗\\njhj(x)\\n!\\n= Γ(µ∗, λ∗)\\n= f(x∗) +\\nm\\nX\\ni=1\\nµ∗\\ni gi(x∗) +\\nn\\nX\\nj=1\\nλ∗\\njhj(x∗)\\n= f(x∗)\\n其中第二个等式是根据下确界函数的性质推得，第三个等式是根据对偶函数的定义推得，第四个等式是\\nL(x∗, µ∗, λ∗) 的展开形式，最后一个等式是因为µ∗\\ni gi(x∗) = 0, hj(x∗) = 0。\\n由于x∗和(µ∗, λ∗) 仅是满足KKT 条件的点，并不一定是f(x) 和Γ(µ, λ) 的最值点，所以f(x∗) ⩾', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 63, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='p∗⩾d∗⩾Γ(µ∗, λ∗)，但是上式又推得f(x∗) = Γ(µ∗, λ∗)，所以p∗= d∗，因此推得强对偶性成立，且x∗\\n和(µ∗, λ∗) 分别是主问题和对偶问题的最优解。\\nSlater 条件恰巧也是KKT 条件中特定的约束限制条件之一，所以式(6.6) 不仅强对偶性成立，而且\\n可以通过求解满足KKT 条件的点来求解出最优解。\\nKKT 条件除了可以作为凸优化问题强对偶性成立的充分条件以外，其实对于任意优化问题（并不一\\n定是凸优化问题）\\n，若其强对偶性成立，KKT 条件也是主问题和对偶问题最优解的必要条件，而且此时并\\n不要求主问题满足KKT 条件中任何一个特定的约束限制条件。下面同样给出具体的推导过程。\\n设主问题的最优解为x∗，对偶问题的最优解为(µ∗, λ∗)，目标函数f(x) 和约束函数gi(x), hj(x) 的\\n一阶偏导连续，当强对偶性成立时，可以推得\\nf(x∗) = Γ(µ∗, λ∗)\\n= inf\\nx∈D L(x, µ∗, λ∗)\\n= inf\\nx∈D\\n \\nf(x) +\\nm\\nX\\ni=1\\nµ∗\\ni gi(x) +\\nn\\nX\\nj=1\\nλ∗\\njhj(x)\\n!\\n⩽f(x∗) +', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 63, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='m\\nX\\ni=1\\nµ∗\\ni gi(x) +\\nn\\nX\\nj=1\\nλ∗\\njhj(x)\\n!\\n⩽f(x∗) +\\nm\\nX\\ni=1\\nµ∗\\ni gi(x∗) +\\nn\\nX\\nj=1\\nλ∗\\njhj(x∗)\\n⩽f(x∗)\\n其中，第一个等式是因为强对偶性成立时p∗= d∗，第二和第三个等式是对偶函数的定义，第四个不等式\\n是根据下确界的性质推得，最后一个不等式成立是因为µ∗\\ni ⩾0, gi(x∗) ⩽0, hj(x∗) = 0。\\n由于f(x∗) = f(x∗)，\\n所以上式中的不等式均可化为等式。\\n第四个不等式可化为等式，\\n说明L(x, µ∗, λ∗)\\n在x∗处取得最小值，\\n所以根据极值的性质可知在x∗处一阶导∇xL(x∗, µ∗, λ∗) = 0。\\n最后一个不等式可化\\n→_→\\n配套视频教程：https://www.bilibili.com/video/BV1Mh411e7VU\\n←_←', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 63, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='→_→\\n欢迎去各大电商平台选购纸质版南瓜书《机器学习公式详解第2 版》\\n←_←\\n为等式，说明µ∗\\ni gi(x∗) = 0。此时再结合主问题和对偶问题原有的约束条件µ∗\\ni ⩾0, gi(x∗) ⩽0, hj(x∗) = 0\\n便凑齐了KKT 条件。\\n6.2.5\\n式(6.9) 和式(6.10) 的推导\\nL(w, b, α) = 1\\n2∥w∥2 +\\nm\\nX\\ni=1\\nαi(1 −yi(wTxi + b))\\n= 1\\n2∥w∥2 +\\nm\\nX\\ni=1\\n(αi −αiyiwTxi −αiyib)\\n= 1\\n2wTw +\\nm\\nX\\ni=1\\nαi −\\nm\\nX\\ni=1\\nαiyiwTxi −\\nm\\nX\\ni=1\\nαiyib\\n对w 和b 分别求偏导数并令其为零\\n∂L\\n∂w = 1\\n2 × 2 × w + 0 −\\nm\\nX\\ni=1\\nαiyixi −0 = 0 =\\n⇒w =\\nm\\nX\\ni=1\\nαiyixi\\n∂L\\n∂b = 0 + 0 −0 −\\nm\\nX\\ni=1\\nαiyi = 0 =\\n⇒\\nm\\nX\\ni=1\\nαiyi = 0\\n6.2.6\\n式(6.11) 的推导\\n因为αi ⩾0，且1\\n2∥w∥2 和1 −yi\\n\\x00wTxi + b\\n\\x01', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 64, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='式(6.11) 的推导\\n因为αi ⩾0，且1\\n2∥w∥2 和1 −yi\\n\\x00wTxi + b\\n\\x01\\n均是关于w 和b 的凸函数，所以式(6.8) 也是关于w\\n和b 的凸函数。根据凸函数的性质可知，其极值点就是最值点，所以一阶导为零的点就是最小值点，因此\\n将式(6.9) 和式(6.10) 代入式(6.8) 后即可得式(6.8) 的最小值（等价于下确界）\\n，再根据对偶问题的定义\\n加上约束αi ⩾0，就得到了式(6.6) 的对偶问题。由于式(6.10) 也是αi 必须满足的条件，且不含有w 和\\nb，因此也需要纳入对偶问题的约束条件。根据以上思路进行推导的过程如下：\\ninf\\nw,b L(w, b, α) = 1\\n2wTw +\\nm\\nX\\ni=1\\nαi −\\nm\\nX\\ni=1\\nαiyiwTxi −\\nm\\nX\\ni=1\\nαiyib\\n= 1\\n2wT\\nm\\nX\\ni=1\\nαiyixi −wT\\nm\\nX\\ni=1\\nαiyixi +\\nm\\nX\\ni=1\\nαi −b\\nm\\nX\\ni=1\\nαiyi\\n= −1\\n2wT\\nm\\nX\\ni=1\\nαiyixi +\\nm\\nX\\ni=1\\nαi −b\\nm\\nX\\ni=1\\nαiyi\\n= −1\\n2wT\\nm\\nX\\ni=1', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 64, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='m\\nX\\ni=1\\nαi −b\\nm\\nX\\ni=1\\nαiyi\\n= −1\\n2wT\\nm\\nX\\ni=1\\nαiyixi +\\nm\\nX\\ni=1\\nαi\\n= −1\\n2(\\nm\\nX\\ni=1\\nαiyixi)T(\\nm\\nX\\ni=1\\nαiyixi) +\\nm\\nX\\ni=1\\nαi\\n= −1\\n2\\nm\\nX\\ni=1\\nαiyixT\\ni\\nm\\nX\\ni=1\\nαiyixi +\\nm\\nX\\ni=1\\nαi\\n=\\nm\\nX\\ni=1\\nαi −1\\n2\\nm\\nX\\ni=1\\nm\\nX\\nj=1\\nαiαjyiyjxT\\ni xj\\n所以\\nmax\\nα\\ninf\\nw,b L(w, b, α) = max\\nα\\nm\\nX\\ni=1\\nαi −1\\n2\\nm\\nX\\ni=1\\nm\\nX\\nj=1\\nαiαjyiyjxT\\ni xj\\n最后将αi ⩾0 和式(6.10) 作为约束条件即可得式(6.11)。\\n式(6.6) 之所以要转化为式(6.11) 来求解，其主要有以下两点理由：\\n→_→\\n配套视频教程：https://www.bilibili.com/video/BV1Mh411e7VU\\n←_←', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 64, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='→_→\\n欢迎去各大电商平台选购纸质版南瓜书《机器学习公式详解第2 版》\\n←_←\\n(1) 式(6.6) 中的未知数是w 和b，式(6.11) 中的未知数是α，w 的维度d 对应样本特征个数，α 的\\n维度m 对应训练样本个数，通常m ≪d，所以求解式(6.11) 更高效，反之求解式(6.6) 更高效；\\n(2) 式(6.11) 中有样本内积x\\nTxj\\ni\\n这一项，后续可以很自然地引入核函数，进而使得支持向量机也能\\n对在原始特征空间线性不可分的数据进行分类。\\n6.2.7\\n式(6.13) 的解释\\n因为式(6.6) 满足Slater 条件，所以强对偶性成立，进而最优解满足KKT 条件。\\n6.3\\n核函数\\n6.3.1\\n式(6.22) 的解释\\n此即核函数的定义，即核函数可以分解成两个向量的内积。要想了解某个核函数是如何将原始特征空\\n间映射到更高维的特征空间的，只需要分解为两个表达形式完全一样的向量内积即可。\\n6.4\\n软间隔与正则化\\n6.4.1\\n式(6.35) 的推导\\n令\\nmax\\n\\x000, 1 −yi\\n\\x00wTxi + b\\n\\x01\\x01\\n= ξi\\n显然ξi ≥0，且当1 −yi\\n\\x00wTxi + b\\n\\x01\\n> 0 时有', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 65, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='\\x01\\x01\\n= ξi\\n显然ξi ≥0，且当1 −yi\\n\\x00wTxi + b\\n\\x01\\n> 0 时有\\n1 −yi\\n\\x00wTxi + b\\n\\x01\\n= ξi\\n当1 −yi\\n\\x00wTxi + b\\n\\x01\\n≤0 时有\\nξi = 0\\n综上可得\\n1 −yi\\n\\x00wTxi + b\\n\\x01\\n⩽ξi ⇒yi\\n\\x00wTxi + b\\n\\x01\\n⩾1 −ξi\\n6.4.2\\n式(6.37) 和式(6.38) 的推导\\n类比式(6.9) 和式(6.10) 的推导\\n6.4.3\\n式(6.39) 的推导\\n式(6.36) 关于ξi 求偏导数并令其为零\\n∂L\\n∂ξi\\n= 0 + C × 1 −αi × 1 −µi × 1 = 0 ⇒C = αi + µi\\n→_→\\n配套视频教程：https://www.bilibili.com/video/BV1Mh411e7VU\\n←_←', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 65, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='→_→\\n欢迎去各大电商平台选购纸质版南瓜书《机器学习公式详解第2 版》\\n←_←\\n6.4.4\\n式(6.40) 的推导\\n将式(6.37)、式(6.38) 和(6.39) 代入式(6.36) 可以得到式(6.35) 的对偶问题，有\\n1\\n2 ∥w∥2 + C\\nm\\nX\\ni=1\\nξi +\\nm\\nX\\ni=1\\nαi\\n\\x001 −ξi −yi\\n\\x00wTxi + b\\n\\x01\\x01\\n−\\nm\\nX\\ni=1\\nµiξi\\n=1\\n2 ∥w∥2 +\\nm\\nX\\ni=1\\nαi\\n\\x001 −yi\\n\\x00wTxi + b\\n\\x01\\x01\\n+ C\\nm\\nX\\ni=1\\nξi −\\nm\\nX\\ni=1\\nαiξi −\\nm\\nX\\ni=1\\nµiξi\\n= −1\\n2\\nm\\nX\\ni=1\\nαiyixT\\ni\\nm\\nX\\ni=1\\nαiyixi +\\nm\\nX\\ni=1\\nαi +\\nm\\nX\\ni=1\\nCξi −\\nm\\nX\\ni=1\\nαiξi −\\nm\\nX\\ni=1\\nµiξi\\n= −1\\n2\\nm\\nX\\ni=1\\nαiyixT\\ni\\nm\\nX\\ni=1\\nαiyixi +\\nm\\nX\\ni=1\\nαi +\\nm\\nX\\ni=1\\n(C −αi −µi)ξi\\n=\\nm\\nX\\ni=1\\nαi −1\\n2\\nm\\nX\\ni=1\\nm\\nX\\nj=1\\nαiαjyiyjxT\\ni xj', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 66, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='=\\nm\\nX\\ni=1\\nαi −1\\n2\\nm\\nX\\ni=1\\nm\\nX\\nj=1\\nαiαjyiyjxT\\ni xj\\n= min\\nw,b,ξ L(w, b, α, ξ, µ)\\n所以\\nmax\\nα,µ min\\nw,b,ξ L(w, b, α, ξ, µ) = max\\nα,µ\\nm\\nX\\ni=1\\nαi −1\\n2\\nm\\nX\\ni=1\\nm\\nX\\nj=1\\nαiαjyiyjxT\\ni xj\\n= max\\nα\\nm\\nX\\ni=1\\nαi −1\\n2\\nm\\nX\\ni=1\\nm\\nX\\nj=1\\nαiαjyiyjxT\\ni xj\\n又因为αi ≥0，µi ≥0，C = αi + µi，消去µi 可得等价约束条件\\n0 ⩽αi ⩽C,\\ni = 1, 2, ..., m\\n6.4.5\\n对数几率回归与支持向量机的关系\\n在\\n“西瓜书”\\n本节的倒数第二段开头，\\n其讨论了对数几率回归与支持向量机的关系，\\n提到\\n“如果使用对\\n率损失函数ℓlog 来替代式(6.29) 中的0/1 损失函数，则几乎就得到了对率回归模型(3.27)”\\n，但式(6.29)\\n与式(3.27) 形式上相差甚远。为了更清晰的说明对数几率回归与软间隔支持向量机的关系，以下先对式', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 66, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='与式(3.27) 形式上相差甚远。为了更清晰的说明对数几率回归与软间隔支持向量机的关系，以下先对式\\n(3.27) 的形式进行变化。\\n将β = (w; b) 和ˆ\\nx = (x; 1) 代入式(3.27) 可得\\nℓ(w, b) =\\nm\\nX\\ni=1\\n\\x10\\n−yi\\n\\x00wTxi + b\\n\\x01\\n+ ln\\n\\x10\\n1 + ewTxi+b\\x11\\x11\\n=\\nm\\nX\\ni=1\\n\\x12\\nln\\n1\\neyi(wTxi+b) + ln\\n\\x10\\n1 + ewTxi+b\\x11\\x13\\n=\\nm\\nX\\ni=1\\nln 1 + ewTxi+b\\neyi(wTxi+b)\\n=\\n\\uf8f1\\n\\uf8f2\\n\\uf8f3\\nPm\\ni=1 ln\\n\\x10\\n1 + e−(wTxi+b)\\x11\\n,\\nyi = 1\\nPm\\ni=1 ln\\n\\x10\\n1 + ewTxi+b\\x11\\n,\\nyi = 0\\n上式中正例和反例分别用yi = 1 和yi = 0 表示，这是对数几率回归常用的方式，而在支持向量机中正例\\n和反例习惯用yi = +1 和yi = −1 表示。实际上，若上式也换用yi = +1 和yi = −1 分别表示正例和反\\n→_→\\n配套视频教程：https://www.bilibili.com/video/BV1Mh411e7VU', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 66, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='←_←', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 66, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='→_→\\n欢迎去各大电商平台选购纸质版南瓜书《机器学习公式详解第2 版》\\n←_←\\n例，则上式可改写为\\nℓ(w, b) =\\n\\uf8f1\\n\\uf8f2\\n\\uf8f3\\nPm\\ni=1 ln\\n\\x10\\n1 + e−(wTxi+b)\\x11\\n,\\nyi = +1\\nPm\\ni=1 ln\\n\\x10\\n1 + ewTxi+b\\x11\\n,\\nyi = −1\\n=\\nm\\nX\\ni=1\\nln\\n\\x10\\n1 + e−yi(wTxi+b)\\x11\\n此时上式的求和项正是式(6.33) 所表述的对率损失。\\n6.4.6\\n式(6.41) 的解释\\n参见式(6.13) 的解释\\n6.5\\n支持向量回归\\n6.5.1\\n式(6.43) 的解释\\n相比于线性回归用一条线来拟合训练样本，支持向量回归而是采用一个以f(x) = wTx + b 为中心，\\n宽度为2ϵ 的间隔带，来拟合训练样本。\\n落在带子上的样本不计算损失（类比线性回归在线上的点预测误差为0）\\n，不在带子上的则以偏离带\\n子的距离作为损失（类比线性回归的均方误差）\\n，然后以最小化损失的方式迫使间隔带从样本最密集的地\\n方穿过，进而达到拟合训练样本的目的。因此支持向量回归的优化问题可以写为\\nmin\\nw,b\\n1\\n2∥w∥2 + C\\nm\\nX\\ni=1', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 67, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='min\\nw,b\\n1\\n2∥w∥2 + C\\nm\\nX\\ni=1\\nℓϵ (f (xi) −yi)\\n其中ℓϵ(z) 为“ϵ 不敏感损失函数”\\n（类比线性回归的均方误差损失）\\nℓϵ(z) =\\n(\\n0,\\nif |z| ⩽ϵ\\n|z| −ϵ,\\nif |z| > ϵ\\n1\\n2∥w∥2 为L2 正则项，此处引入正则项除了起正则化本身的作用外，也是为了和软间隔支持向量机的优化\\n目标保持形式上的一致，这样就可以导出对偶问题引入核函数，C 为用来调节损失权重的正则化常数。\\n6.5.2\\n式(6.45) 的推导\\n同软间隔支持向量机，引入松弛变量ξi，令\\nℓϵ (f (xi) −yi) = ξi\\n显然ξi ⩾0，并且当|f (xi) −yi| ⩽ϵ 时，ξi = 0，当|f (xi) −yi| > ϵ 时，ξi = |f (xi) −yi| −ϵ，所以\\n|f (xi) −yi| −ϵ ⩽ξi\\n|f (xi) −yi| ⩽ϵ + ξi\\n−ϵ −ξi ⩽f (xi) −yi ⩽ϵ + ξi\\n因此支持向量回归的优化问题可以化为\\nmin\\nw,b,ξi\\n1\\n2∥w∥2 + C\\nm\\nX\\ni=1\\nξi\\ns.t.', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 67, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='min\\nw,b,ξi\\n1\\n2∥w∥2 + C\\nm\\nX\\ni=1\\nξi\\ns.t.\\n−ϵ −ξi ⩽f (xi) −yi ⩽ϵ + ξi\\nξi ⩾0, i = 1, 2, . . . , m\\n→_→\\n配套视频教程：https://www.bilibili.com/video/BV1Mh411e7VU\\n←_←', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 67, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='→_→\\n欢迎去各大电商平台选购纸质版南瓜书《机器学习公式详解第2 版》\\n←_←\\n如果考虑两边采用不同的松弛程度，则有\\nmin\\nw,b,ξi,ˆ\\nξi\\n1\\n2∥w∥2 + C\\nm\\nX\\ni=1\\n\\x10\\nξi + ˆ\\nξi\\n\\x11\\ns.t.\\n−ϵ −ˆ\\nξi ⩽f (xi) −yi ⩽ϵ + ξi\\nξi ⩾0, ˆ\\nξi ⩾0, i = 1, 2, . . . , m\\n6.5.3\\n式(6.52) 的推导\\n将式(6.45) 的约束条件全部恒等变形为小于等于0 的形式可得\\n\\uf8f1\\n\\uf8f4\\n\\uf8f4\\n\\uf8f4\\n\\uf8f4\\n\\uf8f2\\n\\uf8f4\\n\\uf8f4\\n\\uf8f4\\n\\uf8f4\\n\\uf8f3\\nf (xi) −yi −ϵ −ξi ≤0\\nyi −f (xi) −ϵ −ˆ\\nξi ≤0\\n−ξi ≤0\\n−ˆ\\nξi ≤0\\n由于以上四个约束条件的拉格朗日乘子分别为αi, ˆ\\nαi, µi, ˆ\\nµi，所以其对应的KKT 条件为\\n\\uf8f1\\n\\uf8f4\\n\\uf8f4\\n\\uf8f4\\n\\uf8f4\\n\\uf8f4\\n\\uf8f2\\n\\uf8f4\\n\\uf8f4\\n\\uf8f4\\n\\uf8f4\\n\\uf8f4\\n\\uf8f3\\nαi (f (xi) −yi −ϵ −ξi) = 0\\nˆ\\nαi\\n\\x10\\nyi −f (xi) −ϵ −ˆ\\nξi\\n\\x11\\n= 0\\n−µiξi = 0 ⇒µiξi = 0\\n−ˆ\\nµi ˆ\\nξi = 0 ⇒ˆ\\nµi ˆ\\nξi = 0', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 68, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='−µiξi = 0 ⇒µiξi = 0\\n−ˆ\\nµi ˆ\\nξi = 0 ⇒ˆ\\nµi ˆ\\nξi = 0\\n又由式(6.49) 和式(6.50) 有\\n\\uf8f1\\n\\uf8f2\\n\\uf8f3\\nµi = C −αi\\nˆ\\nµi = C −ˆ\\nαi\\n所以上述KKT 条件可以进一步变形为\\n\\uf8f1\\n\\uf8f4\\n\\uf8f4\\n\\uf8f4\\n\\uf8f4\\n\\uf8f4\\n\\uf8f2\\n\\uf8f4\\n\\uf8f4\\n\\uf8f4\\n\\uf8f4\\n\\uf8f4\\n\\uf8f3\\nαi (f (xi) −yi −ϵ −ξi) = 0\\nˆ\\nαi\\n\\x10\\nyi −f (xi) −ϵ −ˆ\\nξi\\n\\x11\\n= 0\\n(C −αi)ξi = 0\\n(C −ˆ\\nαi)ˆ\\nξi = 0\\n又因为样本(xi, yi) 只可能处在间隔带的某一侧，\\n即约束条件f (xi)−yi−ϵ−ξi = 0 和yi−f (xi)−ϵ−ˆ\\nξi = 0\\n不可能同时成立，所以αi 和ˆ\\nαi 中至少有一个为0，即αiˆ\\nαi = 0。\\n在此基础上再进一步分析可知，如果αi = 0，则根据约束(C −αi)ξi = 0 可知此时ξi = 0。同理，如\\n果ˆ\\nαi = 0，则根据约束(C −ˆ\\nαi)ˆ\\nξi = 0 可知此时ˆ\\nξi = 0。所以ξi 和ˆ\\nξi 中也是至少有一个为0，即ξi ˆ\\nξi = 0。\\n将αiˆ', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 68, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='ξi = 0。所以ξi 和ˆ\\nξi 中也是至少有一个为0，即ξi ˆ\\nξi = 0。\\n将αiˆ\\nαi = 0, ξi ˆ\\nξi = 0 整合进上述KKT 条件中即可得到式(6.52)。\\n6.6\\n核方法\\n6.6.1\\n式(6.57) 和式(6.58) 的解释\\n式(6.24) 是式(6.20) 的解；\\n式(6.56) 是式(6.43) 的解。\\n对应到表示定理式(6.57) 当中，\\n式(6.20) 和式\\n(6.43) 均为Ω(∥h∥H) = 1\\n2∥w∥2，\\n式(6.20) 的ℓ(h(x1), h(x2), ..., h(xm)) = 0，\\n而式(6.43) 的ℓ(h(x1), h(x2), ..., h(xm)) =\\nC Pm\\ni=1 ℓϵ(f(xi) −yi)，均满足式(6.57) 的要求，式(6.20) 和式(6.43) 的解均为κ(x, xi) 的线性组合，即\\n式(6.58)。\\n→_→\\n配套视频教程：https://www.bilibili.com/video/BV1Mh411e7VU\\n←_←', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 68, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='→_→\\n欢迎去各大电商平台选购纸质版南瓜书《机器学习公式详解第2 版》\\n←_←\\n6.6.2\\n式(6.65) 的推导\\n由表示定理可知，此时二分类KLDA 最终求得的投影直线方程总可以写成如下形式：\\nh(x) =\\nm\\nX\\ni=1\\nαiκ (x, xi)\\n又因为直线方程的固定形式为\\nh(x) = wTϕ(x)\\n所以\\nwTϕ(x) =\\nm\\nX\\ni=1\\nαiκ (x, xi)\\n将κ (x, xi) = ϕ(x)Tϕ(xi) 代入可得\\nwTϕ(x) =\\nm\\nX\\ni=1\\nαiϕ(x)Tϕ(xi)\\n= ϕ(x)T ·\\nm\\nX\\ni=1\\nαiϕ(xi)\\n由于wTϕ(x) 的计算结果为标量，而标量的转置等于其本身，所以\\nwTϕ(x) =\\n\\x00wTϕ(x)\\n\\x01T = ϕ(x)Tw = ϕ(x)T\\nm\\nX\\ni=1\\nαiϕ(xi)\\n即\\nw =\\nm\\nX\\ni=1\\nαiϕ(xi)\\n6.6.3\\n式(6.66) 和式(6.67) 的解释\\n为了详细地说明此式的计算原理，下面首先举例说明，然后再在例子的基础上延展出其一般形式。假\\n设此时仅有4 个样本，其中第1 和第3 个样本的标记为0，第2 和第4 个样本的标记为1，那么此时有', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 69, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='m = 4\\nm0 = 2, m1 = 2\\nX0 = {x1, x3}, X1 = {x2, x4}\\nK =\\n\\uf8ee\\n\\uf8ef\\n\\uf8ef\\n\\uf8ef\\n\\uf8ef\\n\\uf8f0\\nκ (x1, x1)\\nκ (x1, x2)\\nκ (x1, x3)\\nκ (x1, x4)\\nκ (x2, x1)\\nκ (x2, x2)\\nκ (x2, x3)\\nκ (x2, x4)\\nκ (x3, x1)\\nκ (x3, x2)\\nκ (x3, x3)\\nκ (x3, x4)\\nκ (x4, x1)\\nκ (x4, x2)\\nκ (x4, x3)\\nκ (x4, x4)\\n\\uf8f9\\n\\uf8fa\\n\\uf8fa\\n\\uf8fa\\n\\uf8fa\\n\\uf8fb\\n∈R4×4\\n10 =\\n\\uf8ee\\n\\uf8ef\\n\\uf8ef\\n\\uf8ef\\n\\uf8ef\\n\\uf8f0\\n1\\n0\\n1\\n0\\n\\uf8f9\\n\\uf8fa\\n\\uf8fa\\n\\uf8fa\\n\\uf8fa\\n\\uf8fb\\n∈R4×1\\n11 =\\n\\uf8ee\\n\\uf8ef\\n\\uf8ef\\n\\uf8ef\\n\\uf8ef\\n\\uf8f0\\n0\\n1\\n0\\n1\\n\\uf8f9\\n\\uf8fa\\n\\uf8fa\\n\\uf8fa\\n\\uf8fa\\n\\uf8fb\\n∈R4×1\\n→_→\\n配套视频教程：https://www.bilibili.com/video/BV1Mh411e7VU\\n←_←', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 69, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='→_→\\n欢迎去各大电商平台选购纸质版南瓜书《机器学习公式详解第2 版》\\n←_←\\n所以\\nˆ\\nµ0 = 1\\nm0\\nK10 = 1\\n2\\n\\uf8ee\\n\\uf8ef\\n\\uf8ef\\n\\uf8ef\\n\\uf8ef\\n\\uf8f0\\nκ (x1, x1) + κ (x1, x3)\\nκ (x2, x1) + κ (x2, x3)\\nκ (x3, x1) + κ (x3, x3)\\nκ (x4, x1) + κ (x4, x3)\\n\\uf8f9\\n\\uf8fa\\n\\uf8fa\\n\\uf8fa\\n\\uf8fa\\n\\uf8fb\\n∈R4×1\\nˆ\\nµ1 = 1\\nm1\\nK11 = 1\\n2\\n\\uf8ee\\n\\uf8ef\\n\\uf8ef\\n\\uf8ef\\n\\uf8ef\\n\\uf8f0\\nκ (x1, x2) + κ (x1, x4)\\nκ (x2, x2) + κ (x2, x4)\\nκ (x3, x2) + κ (x3, x4)\\nκ (x4, x2) + κ (x4, x4)\\n\\uf8f9\\n\\uf8fa\\n\\uf8fa\\n\\uf8fa\\n\\uf8fa\\n\\uf8fb\\n∈R4×1\\n根据此结果易得ˆ\\nµ0, ˆ\\nµ1 的一般形式为\\nˆ\\nµ0 = 1\\nm0\\nK10 = 1\\nm0\\n\\uf8ee\\n\\uf8ef\\n\\uf8ef\\n\\uf8ef\\n\\uf8ef\\n\\uf8ef\\n\\uf8f0\\nP\\nx∈X0 κ (x1, x)\\nP\\nx∈X0 κ (x2, x)\\n.\\n.\\n.\\nP\\nx∈X0 κ (xm, x)\\n\\uf8f9\\n\\uf8fa\\n\\uf8fa\\n\\uf8fa\\n\\uf8fa\\n\\uf8fa\\n\\uf8fb\\n∈Rm×1\\nˆ\\nµ1 = 1\\nm1\\nK11 = 1\\nm1', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 70, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='\\uf8f9\\n\\uf8fa\\n\\uf8fa\\n\\uf8fa\\n\\uf8fa\\n\\uf8fa\\n\\uf8fb\\n∈Rm×1\\nˆ\\nµ1 = 1\\nm1\\nK11 = 1\\nm1\\n\\uf8ee\\n\\uf8ef\\n\\uf8ef\\n\\uf8ef\\n\\uf8ef\\n\\uf8ef\\n\\uf8f0\\nP\\nx∈X1 κ (x1, x)\\nP\\nx∈X1 κ (x2, x)\\n.\\n.\\n.\\nP\\nx∈X1 κ (xm, x)\\n\\uf8f9\\n\\uf8fa\\n\\uf8fa\\n\\uf8fa\\n\\uf8fa\\n\\uf8fa\\n\\uf8fb\\n∈Rm×1\\n6.6.4\\n式(6.70) 的推导\\n此式是将式(6.65) 代入式(6.60) 后推得而来的，下面给出详细地推导过程。\\n首先将式(6.65) 代入式(6.60) 的分子可得\\nwTSϕ\\nb w =\\n m\\nX\\ni=1\\nαiϕ (xi)\\n!T\\n· Sϕ\\nb ·\\nm\\nX\\ni=1\\nαiϕ (xi)\\n=\\nm\\nX\\ni=1\\nαiϕ (xi)T · Sϕ\\nb ·\\nm\\nX\\ni=1\\nαiϕ (xi)\\n其中\\nSϕ\\nb =\\n\\x10\\nµϕ\\n1 −µϕ\\n0\\n\\x11 \\x10\\nµϕ\\n1 −µϕ\\n0\\n\\x11T\\n=\\n \\n1\\nm1\\nX\\nx∈X1\\nϕ(x) −1\\nm0\\nX\\nx∈X0\\nϕ(x)\\n!  \\n1\\nm1\\nX\\nx∈X1\\nϕ(x) −1\\nm0\\nX\\nx∈X0\\nϕ(x)\\n!T\\n=\\n \\n1\\nm1\\nX\\nx∈X1\\nϕ(x) −1\\nm0\\nX\\nx∈X0\\nϕ(x)\\n!  \\n1\\nm1\\nX', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 70, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='=\\n \\n1\\nm1\\nX\\nx∈X1\\nϕ(x) −1\\nm0\\nX\\nx∈X0\\nϕ(x)\\n!  \\n1\\nm1\\nX\\nx∈X1\\nϕ(x)T −1\\nm0\\nX\\nx∈X0\\nϕ(x)T\\n!\\n将其代入上式可得\\nwTSϕ\\nb w =\\nm\\nX\\ni=1\\nαiϕ (xi)T ·\\n \\n1\\nm1\\nX\\nx∈X1\\nϕ(x) −1\\nm0\\nX\\nx∈X0\\nϕ(x)\\n!\\n·\\n \\n1\\nm1\\nX\\nx∈X1\\nϕ(x)T −1\\nm0\\nX\\nx∈X0\\nϕ(x)T\\n!\\n·\\nm\\nX\\ni=1\\nαiϕ (xi)\\n=\\n \\n1\\nm1\\nX\\nx∈X1\\nm\\nX\\ni=1\\nαiϕ (xi)T ϕ(x) −1\\nm0\\nX\\nx∈X0\\nm\\nX\\ni=1\\nαiϕ (xi)T ϕ(x)\\n!\\n·\\n \\n1\\nm1\\nX\\nx∈X1\\nm\\nX\\ni=1\\nαiϕ(x)Tϕ (xi) −1\\nm0\\nX\\nx∈X0\\nm\\nX\\ni=1\\nαiϕ(x)Tϕ (xi)\\n!\\n→_→\\n配套视频教程：https://www.bilibili.com/video/BV1Mh411e7VU\\n←_←', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 70, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='→_→\\n欢迎去各大电商平台选购纸质版南瓜书《机器学习公式详解第2 版》\\n←_←\\n由于κ (xi, x) = ϕ(xi)Tϕ(x) 为标量，\\n所以其转置等于本身，\\n即κ (xi, x) = ϕ(xi)Tϕ(x) =\\n\\x00ϕ(xi)Tϕ(x)\\n\\x01T =\\nϕ(x)Tϕ(xi) = κ (xi, x)T，将其代入上式可得\\nwTSϕ\\nb w =\\n \\n1\\nm1\\nm\\nX\\ni=1\\nX\\nx∈X1\\nαiκ (xi, x) −1\\nm0\\nm\\nX\\ni=1\\nX\\nx∈X0\\nαiκ (xi, x)\\n!\\n·\\n \\n1\\nm1\\nm\\nX\\ni=1\\nX\\nx∈X1\\nαiκ (xi, x) −1\\nm0\\nm\\nX\\ni=1\\nX\\nx∈X0\\nαiκ (xi, x)\\n!\\n设α = (α1; α2; ...; αm)T ∈Rm×1，同时结合式(6.66) 的解释可得到ˆ\\nµ0, ˆ\\nµ1 的一般形式，上式可以化简为\\nwTSϕ\\nb w =\\n\\x00αT ˆ\\nµ1 −αT ˆ\\nµ0\\n\\x01\\n·\\n\\x10\\nˆ\\nµT\\n1 α −ˆ\\nµT\\n0 α\\n\\x11\\n= αT · (ˆ\\nµ1 −ˆ\\nµ0) ·\\n\\x10\\nˆ\\nµT\\n1 −ˆ\\nµT\\n0\\n\\x11\\n· α\\n= αT · (ˆ\\nµ1 −ˆ', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 71, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='µ0) ·\\n\\x10\\nˆ\\nµT\\n1 −ˆ\\nµT\\n0\\n\\x11\\n· α\\n= αT · (ˆ\\nµ1 −ˆ\\nµ0) · (ˆ\\nµ1 −ˆ\\nµ0)T · α\\n= αTMα\\n以上便是式(6.70) 分子部分的推导，下面继续推导式(6.70) 的分母部分。将式(6.65) 代入式(6.60) 的分\\n母可得：\\nwTSϕ\\nww =\\n m\\nX\\ni=1\\nαiϕ (xi)\\n!T\\n· Sϕ\\nw ·\\nm\\nX\\ni=1\\nαiϕ (xi)\\n=\\nm\\nX\\ni=1\\nαiϕ (xi)T · Sϕ\\nw ·\\nm\\nX\\ni=1\\nαiϕ (xi)\\n其中\\nSϕ\\nw =\\n1\\nX\\ni=0\\nX\\nx∈Xi\\n\\x10\\nϕ(x) −µϕ\\ni\\n\\x11 \\x10\\nϕ(x) −µϕ\\ni\\n\\x11T\\n=\\n1\\nX\\ni=0\\nX\\nx∈Xi\\n\\x10\\nϕ(x) −µϕ\\ni\\n\\x11 \\x12\\nϕ(x)T −\\n\\x10\\nµϕ\\ni\\n\\x11T\\x13\\n=\\n1\\nX\\ni=0\\nX\\nx∈Xi\\n\\x12\\nϕ(x)ϕ(x)T −ϕ(x)\\n\\x10\\nµϕ\\ni\\n\\x11T\\n−µϕ\\ni ϕ(x)T + µϕ\\ni\\n\\x10\\nµϕ\\ni\\n\\x11T\\x13\\n=\\n1\\nX\\ni=0\\nX\\nx∈Xi\\nϕ(x)ϕ(x)T −\\n1\\nX\\ni=0\\nX\\nx∈Xi\\nϕ(x)\\n\\x10\\nµϕ\\ni\\n\\x11T\\n−\\n1\\nX\\ni=0', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 71, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='1\\nX\\ni=0\\nX\\nx∈Xi\\nϕ(x)\\n\\x10\\nµϕ\\ni\\n\\x11T\\n−\\n1\\nX\\ni=0\\nX\\nx∈Xi\\nµϕ\\ni ϕ(x)T +\\n1\\nX\\ni=0\\nX\\nx∈Xi\\nµϕ\\ni\\n\\x10\\nµϕ\\ni\\n\\x11T\\n由于\\n1\\nX\\ni=0\\nX\\nx∈Xi\\nϕ(x)\\n\\x10\\nµϕ\\ni\\n\\x11T\\n=\\nX\\nx∈X0\\nϕ(x)\\n\\x10\\nµϕ\\n0\\n\\x11T\\n+\\nX\\nx∈X1\\nϕ(x)\\n\\x10\\nµϕ\\n1\\n\\x11T\\n= m0µϕ\\n0\\n\\x10\\nµϕ\\n0\\n\\x11T\\n+ m1µϕ\\n1\\n\\x10\\nµϕ\\n1\\n\\x11T\\n且\\n1\\nX\\ni=0\\nX\\nx∈Xi\\nµϕ\\ni ϕ(x)T =\\n1\\nX\\ni=0\\nµϕ\\ni\\nX\\nx∈Xi\\nϕ(x)T\\n= µϕ\\n0\\nX\\nx∈X0\\nϕ(x)T + µϕ\\n1\\nX\\nx∈X1\\nϕ(x)T\\n= m0µϕ\\n0\\n\\x10\\nµϕ\\n0\\n\\x11T\\n+ m1µϕ\\n1\\n\\x10\\nµϕ\\n1\\n\\x11T\\n所以\\nSϕ\\nw =\\nX\\nx∈D\\nϕ(x)ϕ(x)T −2\\n\\x14\\nm0µϕ\\n0\\n\\x10\\nµϕ\\n0\\n\\x11T\\n+ m1µϕ\\n1\\n\\x10\\nµϕ\\n1\\n\\x11T\\x15\\n+ m0µϕ\\n0\\n\\x10\\nµϕ\\n0\\n\\x11T\\n+ m1µϕ\\n1\\n\\x10\\nµϕ\\n1\\n\\x11T\\n=\\nX\\nx∈D\\nϕ(x)ϕ(x)T −m0µϕ\\n0\\n\\x10\\nµϕ\\n0\\n\\x11T\\n−m1µϕ\\n1\\n\\x10\\nµϕ\\n1', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 71, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='X\\nx∈D\\nϕ(x)ϕ(x)T −m0µϕ\\n0\\n\\x10\\nµϕ\\n0\\n\\x11T\\n−m1µϕ\\n1\\n\\x10\\nµϕ\\n1\\n\\x11T\\n→_→\\n配套视频教程：https://www.bilibili.com/video/BV1Mh411e7VU\\n←_←', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 71, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='→_→\\n欢迎去各大电商平台选购纸质版南瓜书《机器学习公式详解第2 版》\\n←_←\\n再将此式代回wTSϕ\\nb w 可得\\nwTSϕ\\nww =\\nm\\nX\\ni=1\\nαiϕ (xi)T · Sϕ\\nw ·\\nm\\nX\\ni=1\\nαiϕ (xi)\\n=\\nm\\nX\\ni=1\\nαiϕ (xi)T ·\\n X\\nx∈D\\nϕ(x)ϕ(x)T −m0µϕ\\n0\\n\\x10\\nµϕ\\n0\\n\\x11T\\n−m1µϕ\\n1\\n\\x10\\nµϕ\\n1\\n\\x11T!\\n·\\nm\\nX\\ni=1\\nαiϕ (xi)\\n=\\nm\\nX\\ni=1\\nm\\nX\\nj=1\\nX\\nx∈D\\nαiϕ (xi)T ϕ(x)ϕ(x)Tαjϕ (xj) −\\nm\\nX\\ni=1\\nm\\nX\\nj=1\\nαiϕ (xi)T m0µϕ\\n0\\n\\x10\\nµϕ\\n0\\n\\x11T\\nαjϕ (xj)\\n−\\nm\\nX\\ni=1\\nm\\nX\\nj=1\\nαiϕ (xi)T m1µϕ\\n1\\n\\x10\\nµϕ\\n1\\n\\x11T\\nαjϕ (xj)\\n其中，第1 项\\nm\\nX\\ni=1\\nm\\nX\\nj=1\\nX\\nx∈D\\nαiϕ (xi)T ϕ(x)ϕ(x)Tαjϕ (xj) =\\nm\\nX\\ni=1\\nm\\nX\\nj=1\\nX\\nx∈D\\nαiαjκ (xi, x) κ (xj, x)\\n= αTKKTα\\n第2 项\\nm\\nX\\ni=1\\nm\\nX', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 72, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='αiαjκ (xi, x) κ (xj, x)\\n= αTKKTα\\n第2 项\\nm\\nX\\ni=1\\nm\\nX\\nj=1\\nαiϕ (xi)T m0µϕ\\n0\\n\\x10\\nµϕ\\n0\\n\\x11T\\nαjϕ (xj) = m0\\nm\\nX\\ni=1\\nm\\nX\\nj=1\\nαiαjϕ (xi)T µϕ\\n0\\n\\x10\\nµϕ\\n0\\n\\x11T\\nϕ (xj)\\n= m0\\nm\\nX\\ni=1\\nm\\nX\\nj=1\\nαiαjϕ (xi)T\\n\"\\n1\\nm0\\nX\\nx∈X0\\nϕ(x)\\n# \"\\n1\\nm0\\nX\\nx∈X0\\nϕ(x)\\n#T\\nϕ (xj)\\n= m0\\nm\\nX\\ni=1\\nm\\nX\\nj=1\\nαiαj\\n\"\\n1\\nm0\\nX\\nx∈X0\\nϕ (xi)T ϕ(x)\\n# \"\\n1\\nm0\\nX\\nx∈X0\\nϕ(x)Tϕ (xj)\\n#\\n= m0\\nm\\nX\\ni=1\\nm\\nX\\nj=1\\nαiαj\\n\"\\n1\\nm0\\nX\\nx∈X0\\nκ (xi, x)\\n# \"\\n1\\nm0\\nX\\nx∈X0\\nκ (xj, x)\\n#\\n= m0αT ˆ\\nµ0 ˆ\\nµT\\n0 α\\n同理，有第3 项\\nm\\nX\\ni=1\\nm\\nX\\nj=1\\nαiϕ (xi)T m1µϕ\\n1\\n\\x10\\nµϕ\\n1\\n\\x11T\\nαjϕ (xj) = m1αT ˆ\\nµ1 ˆ\\nµT\\n1 α', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 72, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='1\\n\\x10\\nµϕ\\n1\\n\\x11T\\nαjϕ (xj) = m1αT ˆ\\nµ1 ˆ\\nµT\\n1 α\\n将上述三项的化简结果代回再将此式代回wTSϕ\\nb w 可得\\nwTSϕ\\nb w = αTKKTα −m0αT ˆ\\nµ0 ˆ\\nµT\\n0 α −m1αT ˆ\\nµ1 ˆ\\nµT\\n1 α\\n= αT ·\\n\\x10\\nKKT −m0 ˆ\\nµ0 ˆ\\nµT\\n0 −m1 ˆ\\nµ1 ˆ\\nµT\\n1\\n\\x11\\n· α\\n= αT ·\\n \\nKKT −\\n1\\nX\\ni=0\\nmi ˆ\\nµi ˆ\\nµT\\ni\\n!\\n· α\\n= αTNα\\n6.6.5\\n核对数几率回归\\n将“对数几率回归与支持向量机的关系”中最后得到的对数几率回归重写为如下形式\\nmin\\nw,b\\n1\\nm\\nm\\nX\\ni=1\\nlog\\n\\x10\\n1 + e−yi(wTxi+b)\\x11\\n+ λ\\n2m∥w∥2\\n→_→\\n配套视频教程：https://www.bilibili.com/video/BV1Mh411e7VU\\n←_←', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 72, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='→_→\\n欢迎去各大电商平台选购纸质版南瓜书《机器学习公式详解第2 版》\\n←_←\\n其中λ 是用来调整正则项权重的正则化常数。假设zi = ϕ(xi) 是由原始空间经核函数映射到高维空间的\\n特征向量，则\\nmin\\nw,b\\n1\\nm\\nm\\nX\\ni=1\\nlog\\n\\x10\\n1 + e−yi(wTzi+b)\\x11\\n+ λ\\n2m∥w∥2\\n注意，\\n以上两式中的w 维度是不同的，\\n其分别与xi 和zi 的维度一致。\\n根据表示定理，\\n上式的解可以写为\\nw =\\nm\\nX\\nj=1\\nαjzj\\n将w 代入对数几率回归可得\\nmin\\nw,b\\n1\\nm\\nm\\nX\\ni=1\\nlog\\n\\x10\\n1 + e−yi(\\n∑m\\nj=1 αjzT\\nj zi+b)\\x11\\n+ λ\\n2m\\nm\\nX\\ni=1\\nm\\nX\\nj=1\\nαiαjzT\\ni zj\\n用核函数κ (xi, xj) = zT\\ni zj = ϕ (xi)T ϕ (xj) 替换上式中的内积运算\\nmin\\nw,b\\n1\\nm\\nm\\nX\\ni=1\\nlog\\n\\x10\\n1 + e−yi(\\n∑m\\nj=1 αjκ(xi,xj)+b)\\x11\\n+ λ\\n2m\\nm\\nX\\ni=1\\nm\\nX\\nj=1\\nαiαjκ (xi, xj)', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 73, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='+ λ\\n2m\\nm\\nX\\ni=1\\nm\\nX\\nj=1\\nαiαjκ (xi, xj)\\n解出α = (α1, α2, ..., αm) 和b 后，即可得f(x) = Pm\\ni=1 αiκ(x, xi) + b。\\n参考文献\\n[1] 王燕军. 最优化基础理论与方法. 复旦大学出版社, 2011.\\n[2] 王书宁. 凸优化. 清华大学出版社, 2013.\\n→_→\\n配套视频教程：https://www.bilibili.com/video/BV1Mh411e7VU\\n←_←', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 73, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='→_→\\n欢迎去各大电商平台选购纸质版南瓜书《机器学习公式详解第2 版》\\n←_←\\n第7 章\\n贝叶斯分类器\\n本章是从概率框架下的贝叶斯视角给出机器学习问题的建模方法，不同于前几章着重于算法具体实\\n现，本章的理论性会更强。朴素贝叶斯算法常用于文本分类，例如用于广告邮件检测，贝叶斯网和EM 算\\n法均属于概率图模型的范畴，因此可合并至第14 章一起学习。\\n7.1\\n贝叶斯决策论\\n7.1.1\\n式(7.5) 的推导\\n由式(7.1) 和式(7.4) 可得\\nR(ci|x) = 1 ∗P(c1|x) + ... + 1 ∗P(ci−1|x) + 0 ∗P(ci|x) + 1 ∗P(ci+1|x) + ... + 1 ∗P(cN|x)\\n又PN\\nj=1 P(cj|x) = 1，则\\nR(ci|x) = 1 −P(ci|x)\\n此即式(7.5）\\n。\\n7.1.2\\n式(7.6) 的推导\\n将式(7.5) 代入式(7.3) 即可推得此式\\n7.1.3\\n判别式模型与生成式模型\\n对于判别式模型来说，就是在已知x 的条件下判别其类别标记c，即求后验概率P(c|x)，前几章介绍', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 74, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='对于判别式模型来说，就是在已知x 的条件下判别其类别标记c，即求后验概率P(c|x)，前几章介绍\\n的模型都属于判别式模型的范畴，尤其是对数几率回归最为直接明了，式(3.23) 和式(3.24) 直接就是后\\n验概率的形式。\\n对于生成式模型来说，理解起来比较抽象，但是可通过思考以下两个问题来理解。\\n(1) 对于数据集来说，其中的样本是如何生成的？通常假设数据集中的样本服从独立同分布，即每个\\n样本都是按照联合概率分布P(x, c) 采样而得，也可以描述为根据P(x, c) 生成的。\\n(2) 若已知样本x 和联合概率分布P(x, c)，如何预测类别呢？若样本x 和联合概率分布P(x, c) 已\\n知，则可以分别求出x 属于各个类别的概率，即P(x, c1), P(x, c2), ..., P(x, cN)，然后选择概率最大的类\\n别作为样本x 的预测结果。\\n因此，之所以称为“生成式”模型，是因为所求的概率P(x, c) 是生成样本x 的概率。\\n7.2\\n极大似然估计\\n7.2.1\\n式(7.12) 和(7.13) 的推导\\n根据式(7.11) 和式(7.10) 可知参数求解式为\\nˆ', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 74, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='式(7.12) 和(7.13) 的推导\\n根据式(7.11) 和式(7.10) 可知参数求解式为\\nˆ\\nθc = arg max\\nθc\\nLL (θc)\\n= arg min\\nθc\\n−LL (θc)\\n= arg min\\nθc\\n−\\nX\\nx∈Dc\\nlog P (x|θc)\\n由“西瓜书”上下文可知，此时假设概率密度函数p(x|c) ∼N (µc, σ2\\nc)，其等价于假设\\nP (x|θc) = P\\n\\x00x|µc, σ2\\nc\\n\\x01\\n=\\n1\\np\\n(2π)d|Σc|\\nexp\\n\\x12\\n−1\\n2(x −µc)TΣ−1\\nc (x −µc)\\n\\x13\\n→_→\\n配套视频教程：https://www.bilibili.com/video/BV1Mh411e7VU\\n←_←', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 74, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='→_→\\n欢迎去各大电商平台选购纸质版南瓜书《机器学习公式详解第2 版》\\n←_←\\n其中，d 表示x 的维数，Σc = σ2\\nc 为对称正定协方差矩阵，|Σc| 表示Σc 的行列式。将其代入参数求解式\\n可得\\n(ˆ\\nµc, ˆ\\nΣc) = arg min\\n(µc,Σc)\\n−\\nX\\nx∈Dc\\nlog\\n\"\\n1\\np\\n(2π)d|Σc|\\nexp\\n\\x12\\n−1\\n2(x −µc)TΣ−1\\nc (x −µc)\\n\\x13#\\n= arg min\\n(µc,Σc)\\n−\\nX\\nx∈Dc\\n\\x14\\n−d\\n2 log(2π) −1\\n2 log |Σc| −1\\n2(x −µc)TΣ−1\\nc (x −µc)\\n\\x15\\n= arg min\\n(µc,Σc)\\nX\\nx∈Dc\\n\\x14d\\n2 log(2π) + 1\\n2 log |Σc| + 1\\n2(x −µc)TΣ−1\\nc (x −µc)\\n\\x15\\n= arg min\\n(µc,Σc)\\nX\\nx∈Dc\\n\\x141\\n2 log |Σc| + 1\\n2(x −µc)TΣ−1\\nc (x −µc)\\n\\x15\\n假设此时数据集Dc 中的样本个数为n，即|Dc| = n，则上式可以改写为\\n(ˆ\\nµc, ˆ\\nΣc) = arg min\\n(µc,Σc)', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 75, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='(ˆ\\nµc, ˆ\\nΣc) = arg min\\n(µc,Σc)\\nn\\nX\\ni=1\\n\\x141\\n2 log |Σc| + 1\\n2(xi −µc)TΣ−1\\nc (xi −µc)\\n\\x15\\n= arg min\\n(µc,Σc)\\nn\\n2 log |Σc| +\\nn\\nX\\ni=1\\n1\\n2(xi −µc)TΣ−1\\nc (xi −µc)\\n为了便于分别求解ˆ\\nµc 和ˆ\\nΣc，在这里我们根据式xTAx = tr(AxxT), ¯\\nx = 1\\nn\\nPn\\ni=1 xi 将上式中的最后一项\\n作如下恒等变形：\\nn\\nX\\ni=1\\n1\\n2(xi −µc)TΣ−1\\nc (xi −µc)\\n=1\\n2 tr\\n\"\\nΣ−1\\nc\\nn\\nX\\ni=1\\n(xi −µc)(xi −µc)T\\n#\\n=1\\n2 tr\\n\"\\nΣ−1\\nc\\nn\\nX\\ni=1\\n\\x00xixT\\ni −xiµT\\nc −µcxT\\ni + µcµT\\nc\\n\\x01\\n#\\n=1\\n2 tr\\n\"\\nΣ−1\\nc\\n n\\nX\\ni=1\\nxixT\\ni −n¯\\nxµT\\nc −nµc¯\\nxT + nµcµT\\nc\\n!#\\n=1\\n2 tr\\n\"\\nΣ−1\\nc\\n n\\nX\\ni=1\\nxixT\\ni −2n¯\\nxµT\\nc + nµcµT\\nc + 2n¯', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 75, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='Σ−1\\nc\\n n\\nX\\ni=1\\nxixT\\ni −2n¯\\nxµT\\nc + nµcµT\\nc + 2n¯\\nx¯\\nxT −2n¯\\nx¯\\nxT\\n!#\\n=1\\n2 tr\\n\"\\nΣ−1\\nc\\n  n\\nX\\ni=1\\nxixT\\ni −2n¯\\nx¯\\nxT + n¯\\nx¯\\nxT\\n!\\n+\\n\\x00nµcµT\\nc −2n¯\\nxµT\\nc + n¯\\nx¯\\nxT\\x01\\n!#\\n=1\\n2 tr\\n\"\\nΣ−1\\nc\\n n\\nX\\ni=1\\n(xi −¯\\nx)(xi −¯\\nx)T +\\nn\\nX\\ni=1\\n(µc −¯\\nx)(µc −¯\\nx)T\\n!#\\n=1\\n2 tr\\n\"\\nΣ−1\\nc\\nn\\nX\\ni=1\\n(xi −¯\\nx)(xi −¯\\nx)T\\n#\\n+ 1\\n2 tr\\n\"\\nΣ−1\\nc\\nn\\nX\\ni=1\\n(µc −¯\\nx)(µc −¯\\nx)T\\n#\\n=1\\n2 tr\\n\"\\nΣ−1\\nc\\nn\\nX\\ni=1\\n(xi −¯\\nx)(xi −¯\\nx)T\\n#\\n+ 1\\n2 tr\\n\\x02\\nn · Σ−1\\nc (µc −¯\\nx)(µc −¯\\nx)T\\x03\\n=1\\n2 tr\\n\"\\nΣ−1\\nc\\nn\\nX\\ni=1\\n(xi −¯\\nx)(xi −¯\\nx)T\\n#\\n+ n\\n2 tr\\n\\x02\\nΣ−1\\nc (µc −¯', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 75, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='i=1\\n(xi −¯\\nx)(xi −¯\\nx)T\\n#\\n+ n\\n2 tr\\n\\x02\\nΣ−1\\nc (µc −¯\\nx)(µc −¯\\nx)T\\x03\\n=1\\n2 tr\\n\"\\nΣ−1\\nc\\nn\\nX\\ni=1\\n(xi −¯\\nx)(xi −¯\\nx)T\\n#\\n+ n\\n2 (µc −¯\\nx)TΣ−1\\nc (µc −¯\\nx)\\n所以\\n(ˆ\\nµc, ˆ\\nΣc) = arg min\\n(µc,Σc)\\nn\\n2 log |Σc| + 1\\n2 tr\\n\"\\nΣ−1\\nc\\nn\\nX\\ni=1\\n(xi −¯\\nx)(xi −¯\\nx)T\\n#\\n+ n\\n2 (µc −¯\\nx)TΣ−1\\nc (µc −¯\\nx)\\n→_→\\n配套视频教程：https://www.bilibili.com/video/BV1Mh411e7VU\\n←_←', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 75, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='→_→\\n欢迎去各大电商平台选购纸质版南瓜书《机器学习公式详解第2 版》\\n←_←\\n观察上式可知，由于此时Σ−1\\nc\\n和Σc 一样均为正定矩阵，所以当µc −¯\\nx ̸= 0 时，上式最后一项为正定二\\n次型。\\n根据正定二次型的性质可知，\\n此时上式最后一项的取值仅与µc −¯\\nx 相关，\\n并有当且仅当µc −¯\\nx = 0\\n时，上式最后一项取最小值0，此时可以解得\\nˆ\\nµc = ¯\\nx = 1\\nn\\nn\\nX\\ni=1\\nxi\\n将求解出来的ˆ\\nµc 代回参数求解式可得新的参数求解式，有\\nˆ\\nΣc = arg min\\nΣc\\nn\\n2 log |Σc| + 1\\n2 tr\\n\"\\nΣ−1\\nc\\nn\\nX\\ni=1\\n(xi −¯\\nx)(xi −¯\\nx)T\\n#\\n此时的参数求解式是仅与Σc 相关的函数。\\n为了求解ˆ\\nΣc，在这里我们不加证明地给出一个引理：设B 为p 阶正定矩阵，n > 0 为实数，在对所\\n有p 阶正定矩阵Σ 有\\nn\\n2 log |Σ| + 1\\n2 tr\\n\\x02\\nΣ−1B\\n\\x03\\n≥n\\n2 log |B| + pn\\n2 (1 −log n)\\n当且仅当Σ = 1\\nnB 时等号成立。', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 76, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='2 log |B| + pn\\n2 (1 −log n)\\n当且仅当Σ = 1\\nnB 时等号成立。\\n（引理的证明可搜索张伟平老师的“多元正态分布参数的估计和数据的清洁与变换”课件）\\n根据此引理可知，当且仅当Σc = 1\\nn\\nPn\\ni=1(xi −¯\\nx)(xi −¯\\nx)T 时，上述参数求解式中arg min 后面的式\\n子取到最小值，那么此时的Σc 即我们想要求解的ˆ\\nΣc。\\n7.3\\n朴素贝叶斯分类器\\n7.3.1\\n式(7.16) 和式(7.17) 的解释\\n该式是基于大数定律的频率近似概率的思路，而该思路的本质仍然是极大似然估计，下面举例说明。\\n以掷硬币为例，假设投掷硬币5 次，结果依次是正面、正面、反面、正面、反面，试基于此观察结果估计\\n硬币正面朝上的概率。\\n设硬币正面朝上的概率为θ，其服从伯努利分布，因此反面朝上的概率为1 −θ，同时设每次投掷结果\\n相互独立，即独立同分布，则似然为\\nL(θ) = θ · θ · (1 −θ) · θ · (1 −θ)\\n= θ3(1 −θ)2\\n对数似然为\\nLL(θ) = ln L(θ) = 3 ln θ + 2 ln(1 −θ)', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 76, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='对数似然为\\nLL(θ) = ln L(θ) = 3 ln θ + 2 ln(1 −θ)\\n易证LL(θ) 是关于θ 的凹函数，因此对其求一阶导并令导数等于零即可求出最大值点，具体地\\n∂LL(θ)\\n∂θ\\n= ∂(3 ln θ + 2 ln(1 −θ))\\n∂θ\\n= 3\\nθ −\\n2\\n1 −θ\\n= 3 −5θ\\nθ(1 −θ)\\n令上式等于0 可解得θ = 3\\n5，显然3\\n5 也是正面出现的频率。\\n7.3.2\\n式(7.18) 的解释\\n该式所表示的正态分布并不一定是标准正态分布，因此p(xi|c) 的取值并不一定在(0, 1) 之间，但是\\n仍然不妨碍其用作“概率”\\n，因为根据朴素贝叶斯的算法原理可知，只需p(xi|c) 的值仅仅是用来比大小，\\n因此只关心相对值而不关心绝对值。\\n→_→\\n配套视频教程：https://www.bilibili.com/video/BV1Mh411e7VU\\n←_←', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 76, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='→_→\\n欢迎去各大电商平台选购纸质版南瓜书《机器学习公式详解第2 版》\\n←_←\\n7.3.3\\n贝叶斯估计\\n[1]\\n贝叶斯学派视角下的一类点估计法称为贝叶斯估计，常用的贝叶斯估计有最大后验估计（Maximum\\nA Posteriori Estimation，简称MAP）\\n、后验中位数估计和后验期望值估计这3 种参数估计方法，下面给\\n出这3 种方法的具体定义。\\n设总体的概率质量函数（若总体的分布为连续型时则改为概率密度函数，此处以离散型为例）为\\nP(x|θ)，从该总体中抽取出的n 个独立同分布的样本构成样本集D = {x1, x2, · · · , xn}，则根据贝叶斯\\n式可得，在给定样本集D 的条件下，θ 的条件概率为\\nP(θ|D) = P(D|θ)P(θ)\\nP(D)\\n=\\nP(D|θ)P(θ)\\nP\\nθ P(D|θ)P(θ)\\n其中P(D|θ) 为似然函数，由于样本集D 中的样本是独立同分布的，所以似然函数可以进一步展开，有\\nP(θ|D) =\\nP(D|θ)P(θ)\\nP\\nθ P(D|θ)P(θ) =\\nQn\\ni=1 P(xi|θ)P(θ)\\nP\\nθ\\nQn\\ni=1 P(xi|θ)P(θ)', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 77, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='Qn\\ni=1 P(xi|θ)P(θ)\\nP\\nθ\\nQn\\ni=1 P(xi|θ)P(θ)\\n根据贝叶斯学派的观点，此条件概率代表了我们在已知样本集D 后对θ 产生的新的认识，它综合了\\n我们对θ 主观预设的先验概率P(θ) 和样本集D 带来的信息，通常称其为θ 的后验概率。\\n贝叶斯学派认为，在得到P(θ|D) 以后，对参数θ 的任何统计推断，都只能基于P(θ|D)。至于具体\\n如何去使用它，可以结合某种准则一起去进行，统计学家也有一定的自由度。对于点估计来说，求使得\\nP(θ|D) 达到最大值的ˆ\\nθMAP 作为θ 的估计称为最大后验估计，求P(θ|D) 的中位数ˆ\\nθMedian 作为θ 的估计\\n称为后验中位数估计，求P(θ|D) 的期望值（均值）ˆ\\nθMean 作为θ 的估计称为后验期望值估计。\\n7.3.4\\nCategorical 分布\\nCategorical 分布又称为广义伯努利分布，是将伯努利分布中的随机变量可取值个数由两个泛化为多\\n个得到的分布。具体地，设离散型随机变量X 共有k 种可能的取值{x1, x2, · · · , xk}，且X 取到每个值', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 77, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='的概率分别为P(X = x1) = θ1, P(X = x2) = θ2, · · · , P(X = xk) = θk，则称随机变量X 服从参数为\\nθ1, θ2, · · · , θk 的Categorical 分布，其概率质量函数为\\nP(X = xi) = p(xi) = θi\\n7.3.5\\nDirichlet 分布\\n类似于Categorical 分布是伯努利分布的泛化形式，Dirichlet 分布是Beta 分布的泛化形式。对于一\\n个k 维随机变量x = (x1, x2, · · · , xk) ∈Rk，其中xi(i = 1, 2, · · · , k) 满足0 ⩽xi ⩽1, Pk\\ni=1 xi = 1，若x\\n服从参数为α = (α1, α2, · · · , αk) ∈Rk 的Dirichlet 分布，则其概率密度函数为\\np(x; α) =\\nΓ\\n\\x10Pk\\ni=1 αi\\n\\x11\\nQk\\ni=1 Γ(αi)\\nk\\nY\\ni=1\\nxαi−1\\ni\\n其中Γ(z) =\\nR ∞', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 77, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='\\x11\\nQk\\ni=1 Γ(αi)\\nk\\nY\\ni=1\\nxαi−1\\ni\\n其中Γ(z) =\\nR ∞\\n0 xz−1e−xdx 为Gamma 函数，当α = (1, 1, · · · , 1) 时，Dirichlet 分布等价于均匀分布。\\n7.3.6\\n式(7.19) 和式(7.20) 的推导\\n从贝叶斯估计的角度来说，拉普拉斯修正就等价于先验概率为Dirichlet 分布的后验期望值估计。为\\n了接下来的叙述方便，我们重新定义一下相关数学符号。\\n设有包含m 个独立同分布样本的训练集D，D 中可能的类别数为k，其类别的具体取值范围为\\n{c1, c2, ..., ck}。若令随机变量C 表示样本所属的类别，且C 取到每个值的概率分别为P(C = c1) =\\n→_→\\n配套视频教程：https://www.bilibili.com/video/BV1Mh411e7VU\\n←_←', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 77, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='→_→\\n欢迎去各大电商平台选购纸质版南瓜书《机器学习公式详解第2 版》\\n←_←\\nθ1, P(C = c2) = θ2, ..., P(C = ck) = θk，那么显然C 服从参数为θ = (θ1, θ2, ..., θk) ∈Rk 的Categorical\\n分布，其概率质量函数为\\nP(C = ci) = P(ci) = θi\\n其中P(ci) = θi 就是式(7.9) 所要求解的ˆ\\nP(c)，下面我们用贝叶斯估计中的后验期望值估计来估计\\nθi。根据贝叶斯估计的原理可知，在进行参数估计之前，需要先主观预设一个先验概率P(θ)，通常为了方\\n便计算后验概率P(θ|D)，我们会用似然函数P(D|θ) 的共轭先验作为我们的先验概率。显然，此时的似\\n然函数P(D|θ) 是一个基于Categorical 分布的似然函数，而Categorical 分布的共轭先验为Dirichlet 分\\n布，所以只需要预设先验概率P(θ) 为Dirichlet 分布，然后使用后验期望值估计就能估计出θi。\\n具体地，记D 中样本类别取值为ci 的样本个数为yi，则似然函数P(D|θ) 可展开为\\nP(D|θ) = θy1', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 78, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='P(D|θ) = θy1\\n1 ...θyk\\nk =\\nk\\nY\\ni=1\\nθyi\\ni\\n则有后验概率\\nP(θ|D) = P(D|θ)P(θ)\\nP(D)\\n=\\nP(D|θ)P(θ)\\nP\\nθ P(D|θ)P(θ)\\n=\\nQk\\ni=1 θyi\\ni · P(θ)\\nP\\nθ\\nhQk\\ni=1 θyi\\ni · P(θ)\\ni\\n假设此时先验概率P(θ) 是参数为α = (α1, α2, ..., αk) ∈Rk 的Dirichlet 分布，则P(θ) 可写为\\nP(θ; α) =\\nΓ\\n\\x10Pk\\ni=1 αi\\n\\x11\\nQk\\ni=1 Γ(αi)\\nk\\nY\\ni=1\\nθαi−1\\ni\\n将其代入P(D|θ) 可得\\nP(θ|D) =\\nQk\\ni=1 θyi\\ni · P(θ)\\nP\\nθ\\nhQk\\ni=1 θyi\\ni · P(θ)\\ni\\n=\\nQk\\ni=1 θyi\\ni ·\\nΓ\\n\\x10Pk\\ni=1 αi\\n\\x11\\nQk\\ni=1 Γ(αi)\\nQk\\ni=1 θαi−1\\ni\\nP\\nθ\\n\\uf8ee\\n\\uf8f0Qk\\ni=1 θyi\\ni ·\\nΓ\\n\\x10Pk\\ni=1 αi\\n\\x11\\nQk\\ni=1 Γ(αi)\\nQk\\ni=1 θαi−1\\ni\\n\\uf8f9\\n\\uf8fb\\n=\\nQk\\ni=1 θyi\\ni ·\\nΓ\\n\\x10Pk\\ni=1 αi', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 78, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='Qk\\ni=1 θαi−1\\ni\\n\\uf8f9\\n\\uf8fb\\n=\\nQk\\ni=1 θyi\\ni ·\\nΓ\\n\\x10Pk\\ni=1 αi\\n\\x11\\nQk\\ni=1 Γ(αi)\\nQk\\ni=1 θαi−1\\ni\\nP\\nθ\\nhQk\\ni=1 θyi\\ni · Qk\\ni=1 θαi−1\\ni\\ni\\n·\\nΓ\\n\\x10Pk\\ni=1 αi\\n\\x11\\nQk\\ni=1 Γ(αi)\\n=\\nQk\\ni=1 θyi\\ni · Qk\\ni=1 θαi−1\\ni\\nP\\nθ\\nhQk\\ni=1 θyi\\ni · Qk\\ni=1 θαi−1\\ni\\ni\\n=\\nQk\\ni=1 θαi+yi−1\\ni\\nP\\nθ\\nhQk\\ni=1 θαi+yi−1\\ni\\ni\\n→_→\\n配套视频教程：https://www.bilibili.com/video/BV1Mh411e7VU\\n←_←', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 78, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='→_→\\n欢迎去各大电商平台选购纸质版南瓜书《机器学习公式详解第2 版》\\n←_←\\n此时若设α + y = (α1 + y1, α2 + y2, ..., αk + yk) ∈Rk，则根据Dirichlet 分布的定义可知\\nP(θ; α + y) =\\nΓ\\n\\x10Pk\\ni=1(αi + yi)\\n\\x11\\nQk\\ni=1 Γ(αi + yi)\\nk\\nY\\ni=1\\nθαi+yi−1\\ni\\nX\\nθ\\nP(θ; α + y) =\\nX\\nθ\\nΓ\\n\\x10Pk\\ni=1(αi + yi)\\n\\x11\\nQk\\ni=1 Γ(αi + yi)\\nk\\nY\\ni=1\\nθαi+yi−1\\ni\\n1 =\\nX\\nθ\\nΓ\\n\\x10Pk\\ni=1(αi + yi)\\n\\x11\\nQk\\ni=1 Γ(αi + yi)\\nk\\nY\\ni=1\\nθαi+yi−1\\ni\\n1 =\\nΓ\\n\\x10Pk\\ni=1(αi + yi)\\n\\x11\\nQk\\ni=1 Γ(αi + yi)\\nX\\nθ\\n\" k\\nY\\ni=1\\nθαi+yi−1\\ni\\n#\\n1\\nP\\nθ\\nhQk\\ni=1 θαi+yi−1\\ni\\ni =\\nΓ\\n\\x10Pk\\ni=1(αi + yi)\\n\\x11\\nQk\\ni=1 Γ(αi + yi)\\n将此结论代入P(D|θ) 可得\\nP(θ|D) =\\nQk', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 79, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='\\x11\\nQk\\ni=1 Γ(αi + yi)\\n将此结论代入P(D|θ) 可得\\nP(θ|D) =\\nQk\\ni=1 θαi+yi−1\\ni\\nP\\nθ\\nhQk\\ni=1 θαi+yi−1\\ni\\ni\\n=\\nΓ\\n\\x10Pk\\ni=1(αi + yi)\\n\\x11\\nQk\\ni=1 Γ(αi + yi)\\nk\\nY\\ni=1\\nθαi+yi−1\\ni\\n= P(θ; α + y)\\n综上可知，对于服从Categorical 分布的θ 来说，假设其先验概率P(θ) 是参数为α 的Dirichlet 分布时，\\n得到的后验概率P(θ|D) 是参数为α + y 的Dirichlet 分布，通常我们称这种先验概率分布和后验概率分\\n布形式相同的这对分布为共轭分布。在推得后验概率P(θ|D) 的具体形式以后，根据后验期望值估计可得\\nθi 的估计值为\\nθi = EP (θ|D)[θi]\\n= EP (θ;α+y)[θi]\\n=\\nαi + yi\\nPk\\nj=1(αj + yj)\\n=\\nαi + yi\\nPk\\nj=1 αj + Pk\\nj=1 yj\\n=\\nαi + yi\\nPk\\nj=1 αj + m', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 79, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='Pk\\nj=1 αj + Pk\\nj=1 yj\\n=\\nαi + yi\\nPk\\nj=1 αj + m\\n显然，式(7.9) 是当α = (1, 1, ..., 1) 时推得的具体结果，此时等价于我们主观预设的先验概率P(θ) 服从\\n均匀分布，此即拉普拉斯修正。同理，当我们调整α 的取值后，即可推得其他数据平滑的公式。\\n7.4\\n半朴素贝叶斯分类器\\n7.4.1\\n式(7.21) 的解释\\n在朴素贝叶斯中求解P(xi|c) 时，先挑出类别为c 的样本，若是离散属性则按大数定律估计P(xi|c)，\\n若是连续属性则求这些样本的均值和方差，接着按正态分布估计P(xi|c)。现在估计P(xi|c, pai)，则是先\\n挑出类别为c 且属性xi 所依赖的属性为pai 的样本，剩下步骤与估计P(xi|c) 时相同。\\n→_→\\n配套视频教程：https://www.bilibili.com/video/BV1Mh411e7VU\\n←_←', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 79, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='→_→\\n欢迎去各大电商平台选购纸质版南瓜书《机器学习公式详解第2 版》\\n←_←\\n7.4.2\\n式(7.22) 的解释\\n该式写为如下形式可能更容易理解：\\nI(xi, xj|y) =\\nN\\nX\\nn=1\\nP(xi, xj|cn) log\\nP(xi, xj|cn)\\nP(xi|cn)P(xj|cn)\\n其中i, j = 1, 2, ..., d 且i ̸= j，N 为类别个数。该式共可得到d(d−1)\\n2\\n个I(xi, xj|y)，即每对(xi, xj) 均有\\n一个条件互信息I(xi, xj|y)。\\n7.4.3\\n式(7.23) 的推导\\n基于贝叶斯定理，式(7.8) 将联合概率P(x, c) 写为等价形式P(x|c)P(c)，实际上，也可将向量x 拆\\n开，把P(x, c) 写为P(x1, x2, ..., xd, c) 形式，然后利用概率公式P(A, B) = P(A|B)P(B) 对其恒等变形\\nP(x, c) = P (x1, x2, . . . , xd, c)\\n= P (x1, x2, . . . , xd | c) P(c)', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 80, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='= P (x1, x2, . . . , xd | c) P(c)\\n= P (x1, . . . , xi−1, xi+1, . . . , xd | c, xi) P (c, xi)\\n类似式(7.14) 采用属性条件独立性假设，则\\nP(x1, ..., xi−1, xi+1, ..., xd|c, xi) =\\nd\\nY\\nj=1j̸=i\\nP(xj|c, xi)\\n根据式(7.25) 可知，当j = i 时，|Dc,xi| = |Dc,xi,xj|，若不考虑平滑项，则此时P(xj|c, xi) = 1，因此在\\n上式的连乘项中可放开j ̸= i 的约束，即\\nP(x1, ..., xi−1, xi+1, ..., xd|c, xi) =\\nd\\nY\\nj=1\\nP(xj|c, xi)\\n综上可得：\\nP(c|x) = P(x, c)\\nP(x)\\n= P (c, xi) P (x1, . . . , xi−1, xi+1, . . . , xd | c, xi)\\nP(x)\\n∝P (c, xi) P (x1, . . . , xi−1, xi+1, . . . , xd | c, xi)\\n= P (c, xi)', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 80, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='= P (c, xi)\\nd\\nY\\nj=1\\nP(xj|c, xi)\\n上式是将属性xi 作为超父属性的，AODE 尝试将每个属性作为超父来构建SPODE，然后将那些具\\n有足够训练数据支撑的SPODE 集成起来作为最终结果。具体来说，对于总共d 个属性来说，共有d 个\\n不同的上式，集成直接求和即可，因为对于不同的类别标记c 均有d 个不同的上式，至于如何满足“足够\\n训练数据支撑的SPODE”这个条件，注意式(7.24) 和式(7.25) 均使用到了|Dc,xi| 和|Dc,xi,xj|，若集合\\nDxi 中样本数量过少，则|Dc,xi| 和|Dc,xi,xj| 将会更小，因此在式(7.23) 中要求集合Dxi 中样本数量不少\\n于m′。\\n7.4.4\\n式(7.24) 和式(7.25) 的推导\\n类比式(7.19) 和式(7.20) 的推导。\\n→_→\\n配套视频教程：https://www.bilibili.com/video/BV1Mh411e7VU\\n←_←', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 80, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='→_→\\n欢迎去各大电商平台选购纸质版南瓜书《机器学习公式详解第2 版》\\n←_←\\n7.5\\n贝叶斯网\\n7.5.1\\n式(7.27) 的解释\\n在这里补充一下同父结构和顺序结构的推导。同父结构：在给定父节点x1 的条件下x3, x4 独立\\nP(x3, x4|x1) = P(x1, x3, x4)\\nP(x1)\\n= P(x1)P(x3|x1)P(x4|x1)\\nP(x1)\\n= P(x3|x1)P(x4|x1)\\n顺序结构：在给定节点x 的条件下y, z 独立\\nP(y, z|x) = P(x, y, z)\\nP(x)\\n= P(z)P(x|z)P(y|x)\\nP(x)\\n= P(z, x)P(y|x)\\nP(x)\\n= P(z|x)P(y|x)\\n7.6\\nEM 算法\\n“西瓜书”中仅给出了EM 算法的运算步骤，其原理并未展开讲解，下面补充EM 算法的推导原理，\\n以及所用到的相关数学知识。\\n7.6.1\\nJensen 不等式\\n若f 是凸函数，则下式恒成立\\nf (tx1 + (1 −t)x2) ⩽tf(x1) + (1 −t)f(x2)\\n其中t ∈[0, 1]，若将x 推广到n 个时同样成立，即', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 81, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='其中t ∈[0, 1]，若将x 推广到n 个时同样成立，即\\nf(t1x1 + t2x2 + ... + tnxn) ⩽t1f(x1) + t2f(x2) + ... + tnf(tn)\\n其中t1, t2, ..., tn ∈[0, 1], Pn\\ni=1 ti = 1。此不等式在概率论中通常以如下形式出现\\nφ(E[X]) ⩽E[φ(X)]\\n其中X 是随机变量，φ 为凸函数，E[X] 为随机变量X 的期望。显然，若f 和φ 是凹函数，则上述不等\\n式中的⩽换成⩾也恒成立。\\n7.6.2\\nEM 算法的推导\\n假设现有一批独立同分布的样本{x1, x2, ..., xm}，它们是由某个含有隐变量的概率分布p(x, z; θ) 生\\n成，现尝试用极大似然估计法估计此概率分布的参数。为了便于讨论，此处假设z 为离散型随机变量，则\\n对数似然函数为\\nLL(θ) =\\nm\\nX\\ni=1\\nln p(xi; θ)\\n=\\nm\\nX\\ni=1\\nln\\nX\\nzi\\np(xi, zi; θ)\\n→_→\\n配套视频教程：https://www.bilibili.com/video/BV1Mh411e7VU\\n←_←', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 81, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='→_→\\n欢迎去各大电商平台选购纸质版南瓜书《机器学习公式详解第2 版》\\n←_←\\n显然，此时LL(θ) 里含有未知的隐变量z 以及求和项的对数，相比于不含隐变量的对数似然函数，显然\\n该似然函数的极大值点较难求解，而EM 算法则给出了一种迭代的方法来完成对LL(θ) 的极大化。\\n下面给出两种推导方法，\\n一个是出自李航老师的\\n《统计学习方法》\\n[2]，\\n一个是出自吴恩达老师的CS229，\\n两种推导方式虽然形式上有差异，但最终的Q 函数相等，接下来先讲述两种推导方法，最后会给出Q 函\\n数是相等的证明。\\n首先给出《统计学习方法》中的推导方法，设X = {x1, x2, ..., xm}, Z = {z1, z2, ..., zm}，则对数似然\\n函数可以改写为\\nLL(θ) = ln P(X|θ)\\n= ln\\nX\\nZ\\nP(X, Z|θ)\\n= ln\\n X\\nZ\\nP(X|Z, θ)P(Z|θ)\\n!\\nEM 算法采用的是通过迭代逐步近似极大化L(θ)：假设第t 次迭代时θ 的估计值是θ(t)，我们希望第t + 1\\n次迭代时的θ 能使LL(θ) 增大，即LL(θ) > LL(θ(t))。为此，考虑两者的差', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 82, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='次迭代时的θ 能使LL(θ) 增大，即LL(θ) > LL(θ(t))。为此，考虑两者的差\\nLL(θ) −LL(θ(t)) = ln\\n X\\nZ\\nP(X|Z, θ)P(Z|θ)\\n!\\n−ln P(X|θ(t))\\n= ln\\n\\uf8eb\\n\\uf8edX\\nZ\\nP(Z|X, θ(t))\\nP(X|Z, θ)P(Z|θ)\\nP(Z|X, θ(t))\\n\\uf8f6\\n\\uf8f8−ln P(X|θ(t))\\n由上述Jensen 不等式可得\\nLL(θ) −LL(θ(t)) ⩾\\nX\\nZ\\nP(Z|X, θ(t)) ln\\nP(X|Z, θ)P(Z|θ)\\nP(Z|X, θ(t))\\n−ln P(X|θ(t))\\n=\\nX\\nZ\\nP(Z|X, θ(t)) ln\\nP(X|Z, θ)P(Z|θ)\\nP(Z|X, θ(t))\\n−1 · ln P(X|θ(t))\\n=\\nX\\nZ\\nP(Z|X, θ(t)) ln\\nP(X|Z, θ)P(Z|θ)\\nP(Z|X, θ(t))\\n−\\nX\\nZ\\nP(Z|X, θ(t)) · ln P(X|θ(t))\\n=\\nX\\nZ\\nP(Z|X, θ(t))\\n\\uf8eb\\n\\uf8edln\\nP(X|Z, θ)P(Z|θ)\\nP(Z|X, θ(t))\\n−ln P(X|θ(t))\\n\\uf8f6\\n\\uf8f8\\n=', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 82, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='P(X|Z, θ)P(Z|θ)\\nP(Z|X, θ(t))\\n−ln P(X|θ(t))\\n\\uf8f6\\n\\uf8f8\\n=\\nX\\nZ\\nP(Z|X, θ(t)) ln\\nP(X|Z, θ)P(Z|θ)\\nP(Z|X, θ(t))P(X|θ(t))\\n令\\nB(θ, θ(t)) = LL(θ(t)) +\\nX\\nZ\\nP(Z|X, θ(t)) ln\\nP(X|Z, θ)P(Z|θ)\\nP(Z|X, θ(t))P(X|θ(t))\\n则\\nLL(θ) ⩾B(θ, θ(t))\\n即B(θ, θ(t)) 是LL(θ) 的下界，此时若设θ(t+1) 能使得B(θ, θ(t)) 达到极大，即\\nB(θ(t+1), θ(t)) ⩾B(θ, θ(t))\\n由于LL(θ(t)) = B(θ(t), θ(t))，那么可以进一步推得\\nLL(θ(t+1)) ⩾B(θ(t+1), θ(t)) ⩾B(θ(t), θ(t)) = LL(θ(t))\\n→_→\\n配套视频教程：https://www.bilibili.com/video/BV1Mh411e7VU\\n←_←', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 82, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='→_→\\n欢迎去各大电商平台选购纸质版南瓜书《机器学习公式详解第2 版》\\n←_←\\nLL(θ(t+1)) ⩾LL(θ(t))\\n因此，\\n任何能使得B(θ, θ(t)) 增大的θ，\\n也可以使得LL(θ) 增大，\\n于是问题就转化为了求解能使得B(θ, θ(t))\\n达到极大的θ(t+1)，即\\nθ(t+1) = arg max\\nθ\\nB(θ, θ(t))\\n= arg max\\nθ\\n\\uf8eb\\n\\uf8edLL(θ(t)) +\\nX\\nZ\\nP(Z|X, θ(t)) ln\\nP(X|Z, θ)P(Z|θ)\\nP(Z|X, θ(t))P(X|θ(t))\\n\\uf8f6\\n\\uf8f8\\n略去对θ 极大化而言是常数的项\\nθ(t+1) = arg max\\nθ\\n X\\nZ\\nP(Z|X, θ(t)) ln (P(X|Z, θ)P(Z|θ))\\n!\\n= arg max\\nθ\\n X\\nZ\\nP(Z|X, θ(t)) ln P(X, Z|θ)\\n!\\n= arg max\\nθ\\nQ(θ, θ(t))\\n到此即完成了EM 算法的一次迭代，求出的θ(t+1) 作为下一次迭代的初始θ(t)。综上，EM 算法的“E 步”\\n和“M 步”可总结为以下两步。', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 83, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='和“M 步”可总结为以下两步。\\nE 步：计算完全数据的对数似然函数ln P(X, Z|θ) 关于在给定观测数据X 和当前参数θ(t) 下对未观\\n测数据Z 的条件概率分布P(Z|X, θ(t)) 的期望Q(θ, θ(t))：\\nQ(θ, θ(t)) = EZ[ln P(X, Z|θ)|X, θ(t)] =\\nX\\nZ\\nP(Z|X, θ(t)) ln P(X, Z|θ)\\nM 步：求使得Q(θ, θ(t)) 达到极大的θ(t+1)。\\n接下来给出CS229 中的推导方法，设zi 的概率质量函数为Qi(zi)，则LL(θ) 可以作如下恒等变形\\nLL(θ) =\\nm\\nX\\ni=1\\nln p(xi; θ)\\n=\\nm\\nX\\ni=1\\nln\\nX\\nzi\\np(xi, zi; θ)\\n=\\nm\\nX\\ni=1\\nln\\nX\\nzi\\nQi(zi)\\np(xi, zi; θ)\\nQi(zi)\\n其中P\\nzi Qi(zi)\\np(xi, zi; θ)\\nQi(zi)\\n可以看做是对\\np(xi, zi; θ)\\nQi(zi)\\n关于zi 求期望，即\\nX\\nzi\\nQi(zi)\\np(xi, zi; θ)\\nQi(zi)\\n= Ezi\\n\\uf8ee\\n\\uf8f0p(xi, zi; θ)', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 83, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='Qi(zi)\\np(xi, zi; θ)\\nQi(zi)\\n= Ezi\\n\\uf8ee\\n\\uf8f0p(xi, zi; θ)\\nQi(zi)\\n\\uf8f9\\n\\uf8fb\\n由Jensen 不等式可得\\nln\\n\\uf8eb\\n\\uf8edEzi\\n\\uf8ee\\n\\uf8f0p(xi, zi; θ)\\nQi(zi)\\n\\uf8f9\\n\\uf8fb\\n\\uf8f6\\n\\uf8f8⩾Ezi\\n\\uf8ee\\n\\uf8f0ln\\n\\uf8eb\\n\\uf8edp(xi, zi; θ)\\nQi(zi)\\n\\uf8f6\\n\\uf8f8\\n\\uf8f9\\n\\uf8fb\\nln\\nX\\nzi\\nQi(zi)\\np(xi, zi; θ)\\nQi(zi)\\n⩾\\nX\\nzi\\nQi(zi) ln\\np(xi, zi; θ)\\nQi(zi)\\n将此式代入LL(θ) 可得\\nLL(θ) =\\nm\\nX\\ni=1\\nln\\nX\\nzi\\nQi(zi)\\np(xi, zi; θ)\\nQi(zi)\\n⩾\\nm\\nX\\ni=1\\nX\\nzi\\nQi(zi) ln\\np(xi, zi; θ)\\nQi(zi)\\n1\\n→_→\\n配套视频教程：https://www.bilibili.com/video/BV1Mh411e7VU\\n←_←', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 83, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='→_→\\n欢迎去各大电商平台选购纸质版南瓜书《机器学习公式详解第2 版》\\n←_←\\n若令B(θ) =\\nm\\nP\\ni=1\\nP\\nzi\\nQi(zi) ln\\np(xi, zi; θ)\\nQi(zi)\\n，则此时B(θ) 为LL(θ) 的下界函数，那么这个下界函数所能构成\\n的最优下界是多少？即B(θ) 的最大值是多少？显然，B(θ) 是LL(θ) 的下界函数，反过来LL(θ) 是其上\\n界函数，所以如果能使得B(θ) = LL(θ)，则此时的B(θ) 就取到了最大值。根据Jensen 不等式的性质\\n可知，如果能使得\\np(xi, zi; θ)\\nQi(zi)\\n恒等于某个常量c，大于等于号便可以取到等号。因此，只需任意选取满足\\np(xi, zi; θ)\\nQi(zi)\\n= c 的Qi(zi) 就能使得B(θ) 达到最大值。由于Qi(zi) 是zi 的概率质量函数，所以Qi(zi) 同\\n时也满足约束0 ⩽Qi(zi) ⩽1, P\\nzi Qi(zi) = 1，结合Qi(zi) 的所有约束可以推得\\np(xi, zi; θ)\\nQi(zi)\\n= c\\np(xi, zi; θ) = c · Qi(zi)\\nX\\nzi', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 84, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='Qi(zi)\\n= c\\np(xi, zi; θ) = c · Qi(zi)\\nX\\nzi\\np(xi, zi; θ) = c ·\\nX\\nzi\\nQi(zi)\\nX\\nzi\\np(xi, zi; θ) = c\\np(xi, zi; θ)\\nQi(zi)\\n=\\nX\\nzi\\np(xi, zi; θ)\\nQi(zi) =\\np(xi, zi; θ)\\nP\\nzi\\np(xi, zi; θ) =\\np(xi, zi; θ)\\np(xi; θ)\\n= p(zi|xi; θ)\\n所以，当且仅当Qi(zi) = p(zi|xi; θ) 时B(θ) 取到最大值，将Qi(zi) = p(zi|xi; θ) 代回LL(θ) 和B(θ) 可\\n以推得\\nLL(θ) =\\nm\\nX\\ni=1\\nln\\nX\\nzi\\nQi(zi)\\np(xi, zi; θ)\\nQi(zi)\\n2\\n=\\nm\\nX\\ni=1\\nln\\nX\\nzi\\np(zi|xi; θ)\\np(xi, zi; θ)\\np(zi|xi; θ)\\n3\\n=\\nm\\nX\\ni=1\\nX\\nzi\\np(zi|xi; θ) ln\\np(xi, zi; θ)\\np(zi|xi; θ)\\n4\\n= max{B(θ)}\\n5', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 84, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='p(xi, zi; θ)\\np(zi|xi; θ)\\n4\\n= max{B(θ)}\\n5\\n其中式4 是式1 中不等式取等号时的情形。由以上推导可知，此时对数似然函数LL(θ) 等价于其下界\\n函数的最大值max{B(θ)}，所以要想极大化LL(θ) 可以通过极大化max{B(θ)} 来间接极大化LL(θ)，因\\n此，下面考虑如何极大化max{B(θ)}。假设已知第t 次迭代的参数为θ(t)，而第t + 1 次迭代的参数θ(t+1)\\n可通过如下方式求得\\nθ(t+1) = arg max\\nθ\\nmax{B(θ)}\\n6\\n= arg max\\nθ\\nm\\nX\\ni=1\\nX\\nzi\\np(zi|xi; θ(t)) ln\\np(xi, zi; θ)\\np(zi|xi; θ(t))\\n7\\n= arg max\\nθ\\nm\\nX\\ni=1\\nX\\nzi\\np(zi|xi; θ(t)) ln p(xi, zi; θ)\\n8\\n→_→\\n配套视频教程：https://www.bilibili.com/video/BV1Mh411e7VU\\n←_←', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 84, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='→_→\\n欢迎去各大电商平台选购纸质版南瓜书《机器学习公式详解第2 版》\\n←_←\\n此时将θ(t+1) 代入LL(θ) 可推得\\nLL(θ(t+1)) = max{B(θ(t+1))}\\n9\\n=\\nm\\nX\\ni=1\\nX\\nzi\\np(zi|xi; θ(t+1)) ln\\np(xi, zi; θ(t+1))\\np(zi|xi; θ(t+1))\\n10\\n⩾\\nm\\nX\\ni=1\\nX\\nzi\\np(zi|xi; θ(t)) ln\\np(xi, zi; θ(t+1))\\np(zi|xi; θ(t))\\n11\\n⩾\\nm\\nX\\ni=1\\nX\\nzi\\np(zi|xi; θ(t)) ln\\np(xi, zi; θ(t))\\np(zi|xi; θ(t))\\n12\\n= max{B(θ(t))}\\n13\\n= LL(θ(t))\\n14\\n其中，式9 和式10 分别由式5 和式4 推得，式11 由式1 推得，式12 由式7 推得，式13 和式\\n14 由式2 至式5 推得。此时若令\\nQ(θ, θ(t)) =\\nm\\nX\\ni=1\\nX\\nzi\\np(zi|xi; θ(t)) ln p(xi, zi; θ)', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 85, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='m\\nX\\ni=1\\nX\\nzi\\np(zi|xi; θ(t)) ln p(xi, zi; θ)\\n由式9 至式14 可知，凡是能使得Q(θ, θ(t)) 达到极大的θ(t+1) 一定能使得LL(θ(t+1)) ⩾LL(θ(t))。综上，\\nEM 算法的“E 步”和“M 步”可总结为以下两步。\\nE 步：令Qi(zi) = p(zi|xi; θ) 并写出Q(θ, θ(t))；\\nM 步：求使得Q(θ, θ(t)) 到达极大的θ(t+1)。\\n以上便是EM 算法的两种推导方法，下面证明两种推导方法中的Q 函数相等。\\nQ(θ|θ(t)) =\\nX\\nZ\\nP(Z|X, θ(t)) ln P(X, Z|θ)\\n=\\nX\\nz1,z2,...,zm\\n( m\\nY\\ni=1\\nP(zi|xi, θ(t)) ln\\n\" m\\nY\\ni=1\\nP(xi, zi|θ)\\n#)\\n=\\nX\\nz1,z2,...,zm\\n( m\\nY\\ni=1\\nP(zi|xi, θ(t))\\n\" m\\nX\\ni=1\\nln P(xi, zi|θ)\\n#)\\n=\\nX\\nz1,z2,...,zm\\n( m\\nY\\ni=1', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 85, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='i=1\\nln P(xi, zi|θ)\\n#)\\n=\\nX\\nz1,z2,...,zm\\n( m\\nY\\ni=1\\nP(zi|xi, θ(t)) [ln P(x1, z1|θ) + ln P(x2, z2|θ) + ... + ln P(xm, zm|θ)]\\n)\\n=\\nX\\nz1,z2,...,zm\\n\" m\\nY\\ni=1\\nP(zi|xi, θ(t)) · ln P(x1, z1|θ)\\n#\\n+ ... +\\nX\\nz1,z2,...,zm\\n\" m\\nY\\ni=1\\nP(zi|xi, θ(t)) · ln P(xm, zm|θ)\\n#\\n→_→\\n配套视频教程：https://www.bilibili.com/video/BV1Mh411e7VU\\n←_←', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 85, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='→_→\\n欢迎去各大电商平台选购纸质版南瓜书《机器学习公式详解第2 版》\\n←_←\\n其中\\nP\\nz1,z2,...,zm\\n\\x14 m\\nQ\\ni=1\\nP(zi|xi, θ(t)) · ln P(x1, z1|θ)\\n\\x15\\n可作如下恒等变形：\\nX\\nz1,z2,...,zm\\n\" m\\nY\\ni=1\\nP(zi|xi, θ(t)) · ln P(x1, z1|θ)\\n#\\n=\\nX\\nz1,z2,...,zm\\n\" m\\nY\\ni=2\\nP(zi|xi, θ(t)) · P(z1|x1, θ(t)) · ln P(x1, z1|θ)\\n#\\n=\\nX\\nz1\\nX\\nz2,...,zm\\n\" m\\nY\\ni=2\\nP(zi|xi, θ(t)) · P(z1|x1, θ(t)) · ln P(x1, z1|θ)\\n#\\n=\\nX\\nz1\\nP(z1|x1, θ(t)) ln P(x1, z1|θ)\\nX\\nz2,...,zm\\n\" m\\nY\\ni=2\\nP(zi|xi, θ(t))\\n#\\n=\\nX\\nz1\\nP(z1|x1, θ(t)) ln P(x1, z1|θ)\\nX\\nz2,...,zm\\n\" m\\nY\\ni=3\\nP(zi|xi, θ(t)) · P(z2|x2, θ(t))\\n#\\n=', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 86, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='\" m\\nY\\ni=3\\nP(zi|xi, θ(t)) · P(z2|x2, θ(t))\\n#\\n=\\nX\\nz1\\nP(z1|x1, θ(t)) ln P(x1, z1|θ)\\n(X\\nz2\\nX\\nz3,...,zm\\n\" m\\nY\\ni=3\\nP(zi|xi, θ(t)) · P(z2|x2, θ(t))\\n#)\\n=\\nX\\nz1\\nP(z1|x1, θ(t)) ln P(x1, z1|θ)\\n(X\\nz2\\nP(z2|x2, θ(t))\\nX\\nz3,...,zm\\n\" m\\nY\\ni=3\\nP(zi|xi, θ(t))\\n#)\\n=\\nX\\nz1\\nP(z1|x1, θ(t)) ln P(x1, z1|θ)\\n(X\\nz2\\nP(z2|x2, θ(t)) ×\\nX\\nz3\\nP(z3|x3, θ(t)) × ... ×\\nX\\nzm\\nP(zm|xm, θ(t))\\n)\\n=\\nX\\nz1\\nP(z1|x1, θ(t)) ln P(x1, z1|θ) × {1 × 1 × ... × 1}\\n=\\nX\\nz1\\nP(z1|x1, θ(t)) ln P(x1, z1|θ)\\n所以\\nX\\nz1,z2,...,zm\\n\" m\\nY\\ni=1', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 86, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='所以\\nX\\nz1,z2,...,zm\\n\" m\\nY\\ni=1\\nP(zi|xi, θ(t)) · ln P(x1, z1|θ)\\n#\\n=\\nX\\nz1\\nP(z1|x1, θ(t)) ln P(x1, z1|θ)\\n同理可得\\nX\\nz1,z2,...,zm\\n\" m\\nY\\ni=1\\nP(zi|xi, θ(t)) · ln P(x2, z2|θ)\\n#\\n=\\nX\\nz2\\nP(z2|x2, θ(t)) ln P(x2, z2|θ)\\n.\\n.\\n.\\nX\\nz1,z2,...,zm\\n\" m\\nY\\ni=1\\nP(zi|xi, θ(t)) · ln P(xm, zm|θ)\\n#\\n=\\nX\\nzm\\nP(zm|xm, θ(t)) ln P(xm, zm|θ)\\n将上式代入Q(θ|θ(t)) 可得\\nQ(θ|θ(t)) =\\nX\\nz1,z2,...,zm\\n\" m\\nY\\ni=1\\nP(zi|xi, θ(t)) · ln P(x1, z1|θ)\\n#\\n+ ... +\\nX\\nz1,z2,...,zm\\n\" m\\nY\\ni=1\\nP(zi|xi, θ(t)) · ln P(xm, zm|θ)\\n#\\n=\\nX\\nz1', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 86, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='Y\\ni=1\\nP(zi|xi, θ(t)) · ln P(xm, zm|θ)\\n#\\n=\\nX\\nz1\\nP(z1|x1, θ(t)) ln P(x1, z1|θ) + ... +\\nX\\nzm\\nP(zm|xm, θ(t)) ln P(xm, zm|θ)\\n=\\nm\\nX\\ni=1\\nX\\nzi\\nP(zi|xi, θ(t)) ln P(xi, zi|θ)\\n参考文献\\n[1] 陈希孺. 概率论与数理统计. 中国科学技术大学出版社, 2009.\\n→_→\\n配套视频教程：https://www.bilibili.com/video/BV1Mh411e7VU\\n←_←', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 86, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='→_→\\n欢迎去各大电商平台选购纸质版南瓜书《机器学习公式详解第2 版》\\n←_←\\n[2] 李航. 统计学习方法. 清华大学出版社, 2012.\\n→_→\\n配套视频教程：https://www.bilibili.com/video/BV1Mh411e7VU\\n←_←', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 87, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='→_→\\n欢迎去各大电商平台选购纸质版南瓜书《机器学习公式详解第2 版》\\n←_←\\n第8 章\\n集成学习\\n集成学习(ensemble learning) 描述的是组合多个基础的学习器（模型）的结果以达到更加鲁棒、效果\\n更好的学习器。在“西瓜书”作者周志华教授的谷歌学术主页的top 引用文章（图8-1）中，很大一部分都\\n和集成学习有关。\\n图8-1 周志华教授谷歌学术top10 引用文章(截止到2023-02-19)\\n在引用次数前10 的文章中，第1 名“Top 10 algorithms in data mining”是在ICDM’06 中投票选\\n出的数据挖掘十大算法，每个提名算法均由业内专家代表去阐述，然后进行投票，其中最终得票排名第7\\n位的“Adaboost”即由周志华教授作为代表进行阐述；第2 名“Isolation forest”是通过集成学习的技术\\n用来做异常检测。第3 名的“Ensemble Methods: Foundations and Algorithms”则是周志华教授所著的', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 88, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='集成学习专著。第6 名“Ensembing neural networks: many could be better than all”催生了基于优化的\\n集成修剪(ensemble pruning) 技术；第7 名的“Exploratory undersampling for class-imbalance learning”\\n是以集成学习技术解决类别不平衡问题。\\n毫不夸张的说，周志华教授在集成学习领域深耕了很多年，是绝对的权威。而集成学习也是经受了时\\n间考验的非常有效的算法，常常被各位竞赛同学作为涨点提分的致胜法宝。下面，让我们一起认真享受\\n“西瓜书”作者最拿手的集成学习章节吧。\\n→_→\\n配套视频教程：https://www.bilibili.com/video/BV1Mh411e7VU\\n←_←', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 88, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='→_→\\n欢迎去各大电商平台选购纸质版南瓜书《机器学习公式详解第2 版》\\n←_←\\n8.1\\n个体与集成\\n基学习器(base learner) 的概念在论文中经常出现，\\n可留意一下；\\n另外，\\n本节提到的投票法有两种，\\n除\\n了本节的多数投票(majority voting)，\\n还有概率投票(probability voting)，\\n这两点在8.4 节中均会提及，\\n即\\n硬投票和软投票。\\n8.1.1\\n式(8.1) 的解释\\nhi(x) 是编号为i 的基分类器给x 的预测标记，f(x) 是x 的真实标记，它们之间不一致的概率记为\\nϵ。\\n8.1.2\\n式(8.2) 的解释\\n注意到当前仅针对二分类问题y ∈{−1, +1}, 即预测标记hi(x) ∈{−1, +1} 。各个基分类器hi 的分\\n类结果求和之后结果的正、负或0，代表投票法产生的结果，即“少数服从多数”\\n，符号函数sign，将正数\\n变成1，负数变成-1，0 仍然是0，所以H(x) 是由投票法产生的分类结果。\\n8.1.3\\n式(8.3) 的推导\\n由基分类器相互独立，假设随机变量X 为T 个基分类器分类正确的次数，因此随机变量X 服从二项\\n分布：', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 89, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='分布：\\nX ∼B(T, 1 −ϵ)，\\n设xi 为每一个分类器分类正确的次数，\\n则xi ∼B(1, 1 −ϵ)\\uffffi = 1\\uffff2\\uffff3\\uffff...\\uffffT\\uffff，\\n那么有\\nX =\\nT\\nX\\ni=1\\nxi\\nE(X) =\\nT\\nX\\ni=1\\nE(xi) = (1 −ϵ)T\\n证明过程如下：\\nP(H(x) ̸= f(x)) =P(X ≤⌊T/2⌋)\\n⩽P(X ≤T/2)\\n= P\\n\\x14\\nX −(1 −ϵ)T ⩽T\\n2 −(1 −ϵ)T\\n\\x15\\n= P\\n\\x14\\nX −(1 −ϵ)T ⩽−T\\n2 (1 −2ϵ)]\\n\\x15\\n= P\\n\" T\\nX\\ni=1\\nxi −\\nT\\nX\\ni=1\\nE(xi) ⩽−T\\n2 (1 −2ϵ)]\\n#\\n= P\\n\"\\n1\\nT\\nT\\nX\\ni=1\\nxi −1\\nT\\nT\\nX\\ni=1\\nE(xi) ⩽−1\\n2 (1 −2ϵ)]\\n#\\n根据Hoeffding 不等式知\\nP\\n \\n1\\nm\\nm\\nX\\ni=1\\nxi −1\\nm\\nm\\nX\\ni=1\\nE (xi) ⩽−δ\\n!\\n⩽exp\\n\\x00−2mδ2\\x01\\n令δ = (1−2ϵ)\\n2\\n, m = T 得\\nP(H(x) ̸= f(x)) =\\n⌊T /2⌋\\nX\\nk=0\\n \\nT\\nk\\n!\\n(1 −ϵ)kϵT −k', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 89, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='⌊T /2⌋\\nX\\nk=0\\n \\nT\\nk\\n!\\n(1 −ϵ)kϵT −k\\n⩽exp\\n\\x12\\n−1\\n2T(1 −2ϵ)2\\n\\x13\\n→_→\\n配套视频教程：https://www.bilibili.com/video/BV1Mh411e7VU\\n←_←', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 89, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='→_→\\n欢迎去各大电商平台选购纸质版南瓜书《机器学习公式详解第2 版》\\n←_←\\n8.2\\nBoosting\\n注意8.1 节最后一段提到：根据个体学习器的生成方式，目前的集成学习方法大致可分为两大类，即\\n个体学习器间存在强依赖关系、必须串行生成的序列化方法，以及个体学习器间不存在强依赖关系、可同\\n时生成的并行化方法。\\n本节Boosting 为前者的代表，Adaboost 又是Boosting 族算法的代表。\\n8.2.1\\n式(8.4) 的解释\\n这个式子是集成学习的加性模型，加性模型不采用梯度下降的思想，而是H(x) = PT −1\\nt=1 αtht(x) +\\nαT hT (x)，\\n共迭代T 次，\\n每次更新求解一个理论上最优的hT 和αT 。\\nhT 和αT 的定义参见式(8.18) 和式(8.11)\\n8.2.2\\n式(8.5) 的解释\\n先考虑指数损失函数e−f(x)H(x) 的含义参见“西瓜书”图6.5 ：f 为真实函数，对于样本x 来说，\\nf(x) ∈{+1, −1} 只能取+1 和−1，而H(x) 是一个实数。\\n当H(x) 的符号与f(x) 一致时，\\nf(x)H(x) > 0，', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 90, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='当H(x) 的符号与f(x) 一致时，\\nf(x)H(x) > 0，\\n因此e−f(x)H(x) = e−|H(x)| < 1，\\n且|H(x)| 越大指数\\n损失函数e−f(x)H(x) 越小。这很合理：此时|H(x)| 越大意味着分类器本身对预测结果的信心越大，损失\\n应该越小；若|H(x)| 在零附近，虽然预测正确，但表示分类器本身对预测结果信心很小，损失应该较大；\\n当H(x) 的符号与f(x) 不一致时，f(x)H(x) < 0，因此e−f(x)H(x) = e|H(x)| > 1，且|H(x)| 越大指\\n数损失函数越大。这很合理：此时|H(x)| 越大意味着分类器本身对预测结果的信心越大，但预测结果是\\n错的，因此损失应该越大；若|H(x)| 在零附近，虽然预测错误，但表示分类器本身对预测结果信心很小，\\n虽然错了，损失应该较小。\\n再解释符号Ex∼D[·] 的含义：D 为概率分布，可简单理解为在数据集D 中进行一次随机抽样，每个\\n样本被取到的概率；E[·] 为经典的期望，则综合起来Ex∼D[·] 表示在概率分布D 上的期望，可简单理解为\\n对数据集D 以概率D 进行加权后的期望。', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 90, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='对数据集D 以概率D 进行加权后的期望。\\n综上所述, 若数据集D 中样本x 的权值分布为D(x), 则式(8.5) 可写为:\\nℓexp(H | D) = Ex∼D\\n\\x02\\ne−f(x)H(x)\\x03\\n=\\nX\\nx∈D\\nD(x)e−f(x)H(x)\\n=\\nX\\nx∈D\\nD(x)\\n\\x00e−H(x)I(f(x) = 1) + eH(x)I(f(x) = −1)\\n\\x01\\n特别地, 若针对任意样本x, 若分布D(x) =\\n1\\n|D|, 其中|D| 为数据集D 样本个数, 则\\nℓexp(H | D) = Ex∼D\\n\\x02\\ne−f(x)H(x)\\x03\\n=\\n1\\n|D|\\nX\\nx∈D\\ne−f(x)H(x)\\n而这就是在求传统平均值。\\n→_→\\n配套视频教程：https://www.bilibili.com/video/BV1Mh411e7VU\\n←_←', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 90, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='→_→\\n欢迎去各大电商平台选购纸质版南瓜书《机器学习公式详解第2 版》\\n←_←\\n8.2.3\\n式(8.6) 的推导\\n由式(8.5) 中对于符号Ex∼D[·] 的解释可知\\nℓexp(H|D) = Ex∼D\\n\\x02\\ne−f(x)H(x)\\x03\\n=\\nX\\nx∈D\\nD(x)e−f(x)H(x)\\n=\\n|D|\\nX\\ni=1\\nD (xi)\\n\\x00e−H(xi)I (f (xi) = 1) + eH(xi)I (f (xi) = −1)\\n\\x01\\n=\\n|D|\\nX\\ni=1\\n\\x00e−H(xi)D (xi) I (f (xi) = 1) + eH(xi)D (xi) I (f (xi) = −1)\\n\\x01\\n=\\n|D|\\nX\\ni=1\\n\\x00e−H(xi)P (f (xi) = 1 | xi) + eH(xi)P (f (xi) = −1 | xi)\\n\\x01\\n其中D (xi) I (f (xi) = 1) = P (f (xi) = 1 | xi) 可以这样理解：D(xi) 表示在数据集D 中进行一次随机抽\\n样，样本xi 被取到的概率，D (xi) I (f (xi) = 1) 表示在数据集D 中进行一次随机抽样，使得f(xi) = 1', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 91, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='的样本xi 被抽到的概率，即为P (f (xi) = 1 | xi)。\\n当对H(xi) 求导时，求和号中只有含xi 项不为0，由求导公式\\n∂e−H(x)\\n∂H(x) = −e−H(x)\\n∂eH(x)\\n∂H(x) = eH(x)\\n有\\n∂ℓexp(H|D)\\n∂H(x)\\n= −e−H(x)P(f(x) = 1|x) + eH(x)P(f(x) = −1|x)\\n8.2.4\\n式(8.7) 的推导\\n令式(8.6) 等于零:\\n−e−H(x)P(f(x) = 1 | x) + eH(x)P(f(x) = −1 | x) = 0\\n移项:\\neH(x)P(f(x) = −1 | x) = e−H(x)P(f(x) = 1 | x)\\n两边同乘\\neH(x)\\nP (f(x)=−1|x):\\ne2H(x) = P(f(x) = 1 | x)\\nP(f(x) = −1 | x)\\n取ln(·):\\n2H(x) = ln P(f(x) = 1 | x)\\nP(f(x) = −1 | x)\\n两边同除1\\n2 即得式(8.7)。\\n8.2.5\\n式(8.8) 的推导\\nsign(H(x)) = sign\\n\\x121', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 91, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='2 即得式(8.7)。\\n8.2.5\\n式(8.8) 的推导\\nsign(H(x)) = sign\\n\\x121\\n2 ln P(f(x) = 1|x)\\nP(f(x) = −1|x)\\n\\x13\\n=\\n(\\n1,\\nP(f(x) = 1|x) > P(f(x) = −1|x)\\n−1,\\nP(f(x) = 1|x) < P(f(x) = −1|x)\\n= arg max\\ny∈{−1,1}\\nP(f(x) = y|x)\\n→_→\\n配套视频教程：https://www.bilibili.com/video/BV1Mh411e7VU\\n←_←', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 91, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='→_→\\n欢迎去各大电商平台选购纸质版南瓜书《机器学习公式详解第2 版》\\n←_←\\n第一行到第二行显然成立，第二行到第三行是利用了arg max 函数的定义。arg max\\ny∈{−1,1}\\nP(f(x) = y|x) 表示使\\n得函数P(f(x) = y|x) 取得最大值的y 的值，展开刚好是第二行的式子。\\n这里解释一下贝叶斯错误率的概念。这来源于“西瓜书”P148 的式(7.6) 表示的贝叶斯最优分类器，\\n可以发现式(8.8) 的最终结果是式(7.6) 的二分类特殊形式。\\n到此为止，本节证明了指数损失函数是分类任务原本0/1 损失函数的一致的替代损失函数，而指数损\\n失函数有更好的数学性质，例如它是连续可微函数，因此接下来的式(8.9) 至式(8.19) 基于指数损失函数\\n推导AdaBoost 的理论细节。替代损失函数参见“西瓜书”P131 图6.5\\n8.2.6\\n式(8.9) 的推导\\nℓexp (αtht|Dt) = Ex∼Dt\\n\\x02\\ne−f(x)αtht(x)\\x03\\n1\\n= Ex∼Dt\\n\\x02\\ne−αtI (f(x) = ht(x)) + eαtI (f(x) ̸= ht(x))\\n\\x03\\n2', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 92, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='\\x02\\ne−αtI (f(x) = ht(x)) + eαtI (f(x) ̸= ht(x))\\n\\x03\\n2\\n= e−αtPx∼Dt (f(x) = ht(x)) + eαtPx∼Dt (f(x) ̸= ht(x))\\n3\\n= e−αt (1 −ϵt) + eαtϵt\\n4\\n乍一看本式有些问题, 为什么要最小化ℓexp (αtht | Dt) ?\\n“西瓜书”图8.3 中的第3 行的表达式\\nht = L (D, Dt) 不是代表着应该最小化ℓexp (ht | Dt) 么?\\n或者从整体来看, 第t 轮迭代也应该最小化\\nℓexp (Ht | D) = ℓexp (Ht−1 + αtht | D), 这样最终T 轮迭代结束后得到的式(8.4) 就可以最小化ℓexp(H |\\nD) 了。实际上, 理解了AdaBoost 之后就会发现, ℓexp (αtht | Dt) 与ℓexp (Ht | D) 是等价的, 详见后面的\\n“AdaBoost 的个人推导”\\n。另外, ht = L (D, Dt) 也是推导的结论之一, 即式(8.18), 而不是无缘无故靠直觉\\n用L (D, Dt) 得到ht 。', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 92, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='用L (D, Dt) 得到ht 。\\n暂且不管以上疑问, 权且按作者思路推导一下：\\n1 与式(8.5) 的区别仅在于到底针对αtht(x) 还是H(x), 代入即可;\\n2 是考虑到ht(x) 和f(x) 均只能取−1 和+1 两个值, 其中I(·) 为指示函数;\\n3 对中括号的两项分别求Ex∼Dt[·], 而eαt 和e−αt 与x 无关, 可以作为常数项拿到Ex∼Dt[.] 外面, 而\\nEx∼Dt [I (f(x) = ht(x))] 表示在数据集D 上、样本权值分布为Dt 时f(x) 和ht(x) 相等次数的期望, 即\\nPx∼Dt (f(x) = ht(x)), 也就是正确率, 即(1 −ϵt); 同理, Ex∼Dt [I (f(x) ̸= ht(x))] 表示在数据集D 上、样\\n本权值分布为Dt 时f(x) 和ht(x) 不相等次数的期望, 即Px∼Dt (f(x) ̸= ht(x)), 也就是错误率ϵt;\\n4 即为将Px∼Dt (f(x) = ht(x)) 替换为(1 −ϵt) 、将Px∼Dt (f(x) ̸= ht(x)) 替换为ϵt 的结果。', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 92, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='注意本节符号略有混乱, 如前所述式(8.4) 的H(x) 是连续实值函数, 但在“西瓜书”图8.3 最后一行\\n的输出H(x) 明显只能取−1 和+1 两个值(与式(8.2) 相同), 本节除了“西瓜书”图8.3 最后一行的输出\\n之外, H(x) 均以式(8.4) 的连续实值函数为准。\\n8.2.7\\n式(8.10) 的解释\\n指数损失函数对αt 求偏导，为了得到使得损失函数取最小值时αt 的值。\\n8.2.8\\n式(8.11) 的推导\\n令公式(8.10) 等于0 移项即得到的该式。此时αt 的取值使得该基分类器经αt 加权后的损失函数最\\n小。\\n→_→\\n配套视频教程：https://www.bilibili.com/video/BV1Mh411e7VU\\n←_←', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 92, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='→_→\\n欢迎去各大电商平台选购纸质版南瓜书《机器学习公式详解第2 版》\\n←_←\\n8.2.9\\n式(8.12) 的解释\\n本式的推导和原始论文[1] 的推导略有差异，虽然并不影响后面式(8.18) 以及式(8.19) 的推导结果。\\nAdaBoost 第t 轮迭代应该求解如下优化问题从而得到αt 和ht(x) :\\n(αt, ht(x)) = arg min\\nα,h\\nℓexp (Ht−1 + αh | D)\\n对于该问题, 先对于固定的任意α > 0, 求解ht(x); 得到ht(x) 后再求αt◦。\\n在原始论文的第346 页，对式(8.12) 的推导如图8-2所示，可以发现原文献中保留了参数c (即α )。\\n当然, 对于任意α > 0, 并不影响推导结果。\\n图8-2 原始论文对式(8.12) 的相关推导\\n如果暂且不管以上的差异，我们按照作者的思路推导的话，将Ht(x) = Ht−1(x) + ht(x) 带入公式\\n(8.5) 即可，因为理想的ht 可以纠正Ht−1 的全部错误，所以这里指定ht 其权重系数αt 为1。如果权重\\n系数αt 是个常数的话，对后续结果也没有影响。\\n8.2.10', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 93, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='系数αt 是个常数的话，对后续结果也没有影响。\\n8.2.10\\n式(8.13) 的推导\\n由ex 的二阶泰勒展开为1 + x + x2\\n2 + o(x2) 得:\\nℓexp (Ht−1 + ht|D) = Ex∼D\\n\\x02\\ne−f(x)Ht−1(x)e−f(x)ht(x)\\x03\\n≃Ex∼D\\n\\x14\\ne−f(x)Ht−1(x)\\n\\x12\\n1 −f(x)ht(x) + f 2(x)h2\\nt(x)\\n2\\n\\x13\\x15\\n因为f(x) 与ht(x) 取值都为1 或-1，所以f 2(x) = h2\\nt(x) = 1，所以得:\\nℓexp (Ht−1 + ht|D) = Ex∼D\\n\\x14\\ne−f(x)Ht−1(x)\\n\\x12\\n1 −f(x)ht(x) + 1\\n2\\n\\x13\\x15\\n实际上，此处保留一阶泰勒展开项即可，后面提到的Gradient Boosting 理论框架就是只使用了一阶\\n泰勒展开；当然二阶项为常数，也并不影响推导结果，原文献[1] 中也保留了二阶项。\\n→_→\\n配套视频教程：https://www.bilibili.com/video/BV1Mh411e7VU\\n←_←', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 93, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='→_→\\n欢迎去各大电商平台选购纸质版南瓜书《机器学习公式详解第2 版》\\n←_←\\n8.2.11\\n式(8.14) 的推导\\nht(x) = arg min\\nh\\nℓexp (Ht−1 + h|D)\\n1\\n= arg min\\nh\\nEx∼D\\n\\x14\\ne−f(x)Ht−1(x)\\n\\x12\\n1 −f(x)h(x) + 1\\n2\\n\\x13\\x15\\n2\\n= arg max\\nh\\nEx∼D\\n\\x02\\ne−f(x)Ht−1(x)f(x)h(x)\\n\\x03\\n3\\n= arg max\\nh\\nEx∼D\\n\\x14\\ne−f(x)Ht−1(x)\\nEx∼D [e−f(x)Ht−1(x)]f(x)h(x)\\n\\x15\\n4\\n理想的ht(x) 是使得Ht(x) 的指数损失函数取得最小值时的ht(x)，该式将此转化成某个期望的最大值，\\n其中：\\n2 是将式(8.13) 代入；\\n3 是因为\\nEx∼D\\n\\x14\\ne−f(x)Ht−1(x)\\n\\x12\\n1 −f(x)h(x) + 1\\n2\\n\\x13\\x15\\n=Ex∼D\\n\\x143\\n2e−f(x)Ht−1(x) −e−f(x)Ht−1(x)f(x)h(x)\\n\\x15\\n=Ex∼D\\n\\x143\\n2e−f(x)Ht−1(x)\\n\\x15\\n−Ex∼D\\n\\x02\\ne−f(x)Ht−1(x)f(x)h(x)\\n\\x03', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 94, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='2e−f(x)Ht−1(x)\\n\\x15\\n−Ex∼D\\n\\x02\\ne−f(x)Ht−1(x)f(x)h(x)\\n\\x03\\n本式自变量为h(x), 而Ex∼D\\n\\x02 3\\n2e−f(x)Ht−1(x)\\x03\\n与h(x) 无关, 也就是一个常数，因此只需最小大化第二项\\n−Ex∼D\\n\\x02\\ne−f(x)Ht−1(x)f(x)h(x)\\n\\x03\\n将负号去掉, 原最小化问题变为最大化问题；\\n4 是因为Ex∼D\\n\\x02\\ne−f(x)Ht−1(x)\\x03\\n是与自变量h(x) 无关的正常数(因为指数函数与原问题等价，例如\\narg maxx (1 −x2) 与arg maxx 2 (1 −x2) 的结果均为x = 0)。\\n8.2.12\\n式(8.16) 的推导\\n首先解释下符号Ex∼D 的含义，注意在本章中有两个符号D 和D，其中D 表示数据集，而D 表示\\n数据集D 的样本分布，可以理解为在数据集D 上进行一次随机采样，样本x 被抽到的概率是D(x)，那\\n么符号Ex∼D 表示的是在概率分布D 上的期望，可以简单地理解为对数据及D 以概率D 加权之后的期\\n望，因此有：\\nE(g(x)) =\\n|D|\\nX\\ni=1\\nf(xi)g(xi)\\n故可得\\nEx∼D\\n\\x02', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 94, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='望，因此有：\\nE(g(x)) =\\n|D|\\nX\\ni=1\\nf(xi)g(xi)\\n故可得\\nEx∼D\\n\\x02\\ne−f(x)H(x)\\x03\\n=\\n|D|\\nX\\ni=1\\nD (xi) e−f(xi)H(xi)\\n由式(8.15) 可知\\nDt (xi) = D (xi)\\ne−f(xi)Ht−1(xi)\\nEx∼D [e−f(x)Ht−1(x)]\\n→_→\\n配套视频教程：https://www.bilibili.com/video/BV1Mh411e7VU\\n←_←', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 94, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='→_→\\n欢迎去各大电商平台选购纸质版南瓜书《机器学习公式详解第2 版》\\n←_←\\n所以式(8.16) 可以表示为\\nEx∼D\\n\\x14\\ne−f(x)Ht−1(x)\\nEx∼D [e−f(x)Ht−1(x)]f(x)h(x)\\n\\x15\\n=\\n|D|\\nX\\ni=1\\nD (xi)\\ne−f(xi)Ht−1(xi)\\nEx∼D [e−f(x)Ht−1(x)] f(xi)h(xi)\\n=\\n|D|\\nX\\ni=1\\nDt (xi) f (xi) h (xi)\\n=Ex∼Dt[f(x)h(x)]\\n8.2.13\\n式(8.17) 的推导\\n当f(x) = h(x) 时，I(f(x) ̸= h(x)) = 0，f(x)h(x) = 1，1 −2I(f(x) ̸= h(x)) = 1；\\n当f(x) ̸= h(x) 时，I(f(x) ̸= h(x)) = 1，f(x)h(x) = −1，1 −2I(f(x) ̸= h(x)) = −1。\\n综上，左右两式相等。\\n8.2.14\\n式(8.18) 的推导\\n本式基于式(8.17) 的恒等关系，由式(8.16) 推导而来。', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 95, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='式(8.18) 的推导\\n本式基于式(8.17) 的恒等关系，由式(8.16) 推导而来。\\nEx∼Dt[f(x)h(x)] = Ex∼Dt[1 −2I(f(x) ̸= h(x))]\\n= Ex∼Dt[1] −2Ex∼Dt[I(f(x) ̸= h(x))]\\n= 1 −2Ex∼Dt[I(f(x) ̸= h(x))]\\n类似于式(8.14) 的第3 个和第4 个等号，由式(8.16) 的结果开始推导：\\nht(x) = arg max\\nh\\nEx∼Dt[f(x)h(x)]\\n= arg max\\nh\\n(1 −2Ex∼Dt[I(f(x) ̸= h(x))])\\n= arg max\\nh\\n(−2Ex∼Dt[I(f(x) ̸= h(x))])\\n= arg min Ex∼Dt[I(f(x) ̸= h(x))]\\n此式表示理想的ht(x) 在分布Dt 下最小化分类误差, 因此有\\n“西瓜书”\\n图8.3 第3 行ht(x) = L (D, Dt),\\n即分类器ht(x) 可以基于分布Dt 从数据集D 中训练而得, 而我们在训练分类器时, 一般来说最小化的损\\n失函数就是分类误差。\\n8.2.15\\n式(8.19) 的推导', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 95, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='失函数就是分类误差。\\n8.2.15\\n式(8.19) 的推导\\nDt+1(x) =\\nD(x)e−f(x)Ht(x)\\nEx∼D [e−f(x)Ht(x)]\\n= D(x)e−f(x)Ht−1(x)e−f(x)αtht(x)\\nEx∼D [e−f(x)Ht(x)]\\n= Dt(x) · e−f(x)αtht(x) Ex∼D\\n\\x02\\ne−f(x)Ht−1(x)\\x03\\nEx∼D [e−f(x)Ht(x)]\\n第1 个等号是将式(8.15) 中的t 换为t + 1 (同时t −1 换为t);\\n第2 个等号是将Ht(x) = Ht−1(x) + αtht(x) 代入分子即可;\\n第3 个等号是乘以\\nEx∼D[e−f(x)Ht−1(x)]\\nEx∼D[e−f(x)Ht−1(x)] 后, 凑出式(8.15) 的Dt(x) 表达式, 以符号Dt(x) 替换即得。到\\n此之后, 得到Dt+1(x) 与Dt(x) 的关系, 但为了确保Dt+1(x) 是一个分布, 需要对得到的Dt+1(x) 进行规\\n范化, 即“西瓜书”图8.3 第7 行的Zt 。式(8.19) 第3 行最后一个分式将在规范化过程被吸收。\\n→_→', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 95, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='→_→\\n配套视频教程：https://www.bilibili.com/video/BV1Mh411e7VU\\n←_←', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 95, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='→_→\\n欢迎去各大电商平台选购纸质版南瓜书《机器学习公式详解第2 版》\\n←_←\\nboosting 算法是根据调整后的样本再去训练下一个基分类器，这就是“重赋权法”的样本分布的调整\\n公式。\\n8.2.16\\nAdaBoost 的个人推导\\n西瓜书中对AdaBoost 的推导和原论文[1] 上有些地方有差异，综合原论文和一些参考资料，这里给\\n出一版更易于理解的推导，亦可参见我们的视频教程。\\nAdaBoost 的目标是学得T 个ht(x) 和相应的T 个αt, 得到式(8.4) 的H(x), 使式(8.5) 指数损失函\\n数ℓexp(H | D) 最小, 这就是求解所谓的“加性模型”\\n。特别强调一下, 分类器ht(x) 如何得到及其相应的\\n权重αt 等于多少都是需要求解的(ht(x) = L (D, Dt), 即基于分布Dt 从数据集D 中经过最小化训练误差\\n训练出分类器ht, 也就是式(8.18), αt 参见式(8.11)。\\n“通常这是一个复杂的优化问题（同时学得T 个ht(x) 和相应的T 个αt 很困难)。前向分步算法求解', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 96, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='这一优化问题的想法是：因为学习的是加法模型, 如果能够从前向后, 每一步只学习一个基函数ht(x) 及\\n其系数αt, 逐步逼近最小化指数损失函数ℓexp(H | D), 那么就可以简化优化的复杂度。\\n”\\n摘自李航《统计学习方法》[2] 第144 页，略有改动\\n因此, AdaBoost 每轮迭代只需要得到一个基分类器和其投票权重, 设第t 轮迭代需得到基分类器\\nht(x), 对应的投票权重为αt, 则集成分类器Ht(x) = Ht−1(x) + αtht(x), 其中H0(x) = 0 。为表达式简\\n洁, 常常将ht(x) 简写为ht, Ht(x) 简写为Ht 。则第t 轮实际为如下优化问题（本节式(8.4) 到式(8.8)\\n已经证明了指数损失函数是分类任务原本0/1 损失函数的一致替代损失函数):\\n(αt, ht) = arg min\\nα,h\\nℓexp (Ht−1 + αh | D)\\n表示每轮得到的基分类器ht(x) 和对应的权重αt 是最小化集成分类器Ht = Ht−1 + αtht 在数据集D 上、', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 96, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='样本权值分布为D (即初始化样本权值分布, 也就是D1 ) 时的指数损失函数ℓexp (Ht−1 + αh | D) 的结果。\\n这就是前向分步算法求解加性模型的思路。根据式(8.5) 将指数损失函数表达式代入, 则\\nℓexp (Ht−1 + αh | D) = Ex∼D\\n\\x02\\ne−f(x)(Ht−1(x)+αh(x))\\x03\\n=\\n|D|\\nX\\ni=1\\nD (xi) e−f(xi)(Ht−1(xi)+αh(xi))\\n=\\n|D|\\nX\\ni=1\\nD (xi) e−f(xi)Ht−1(xi)e−f(xi)αh(xi)\\n=\\n|D|\\nX\\ni=1\\nD (xi) e−f(xi)Ht−1(xi) \\x00e−αI (f (xi) = h (xi)) + eαI (f (xi) ̸= h (xi))\\n\\x01\\n上式推导中, 由于f (xi) 和h (xi) 均只能取−1, +1 两个值, 因此当f (xi) = h (xi) 时, f (xi) h (xi) = 1,', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 96, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='当f (xi) ̸= h (xi) 时, f (xi) h (xi) = −1 。另外, f (xi) 和h (xi) 要么相等, 要么不相等, 二者只能有一个\\n为真, 因此以下等式恒成立:\\nI (f (xi) = h (xi)) + I (f (xi) ̸= h (xi)) = 1\\n所以\\ne−αI (f (xi) = h (xi)) + eαI (f (xi) ̸= h (xi))\\n=e−αI (f (xi) = h (xi)) + e−αI (f (xi) ̸= h (xi)) −e−αI (f (xi) ̸= h (xi)) + eαI (f (xi) ̸= h (xi))\\n=e−α (I (f (xi) = h (xi)) + I (f (xi) ̸= h (xi))) +\\n\\x00eα −e−α\\x01\\nI (f (xi) ̸= h (xi))\\n=e−α +\\n\\x00eα −e−α\\x01\\nI (f (xi) ̸= h (xi))\\n→_→\\n配套视频教程：https://www.bilibili.com/video/BV1Mh411e7VU\\n←_←', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 96, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='→_→\\n欢迎去各大电商平台选购纸质版南瓜书《机器学习公式详解第2 版》\\n←_←\\n将此结果代入ℓexp (Ht−1 + αh | D), 得注: 以下表达式后面求解权重αt 时仍会使用\\nℓexp (Ht−1 + αh | D) =\\n|D|\\nX\\ni=1\\nD (xi) e−f(xi)Ht−1(xi) \\x00e−α +\\n\\x00eα −e−α\\x01\\nI (f (xi) ̸= h (xi))\\n\\x01\\n=\\n|D|\\nX\\ni=1\\nD (xi) e−f(xi)Ht−1(xi)e−α +\\n|D|\\nX\\ni=1\\nD (xi) e−f(xi)Ht−1(xi) \\x00eα −e−α\\x01\\nI (f (xi) ̸= h (xi))\\n= e−α\\n|D|\\nX\\ni=1\\nD′\\nt (xi) +\\n\\x00eα −e−α\\x01 |D|\\nX\\ni=1\\nD′\\nt (xi) I (f (xi) ̸= h (xi))\\n外面; 第一项e−α P|D|\\ni=1 D′\\nt (xi) 与h(x) 无关, 因此对于任意α > 0, 使ℓexp (Ht−1 + αh | D) 最小的h(x)\\n只需要使第二项最小即可, 即\\nht = arg min\\nh\\n\\x00eα −e−α\\x01 |D|\\nX', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 97, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='只需要使第二项最小即可, 即\\nht = arg min\\nh\\n\\x00eα −e−α\\x01 |D|\\nX\\ni=1\\nD′\\nt (xi) I (f (xi) ̸= h (xi))\\n对于任意α > 0, 有eα −e−α > 0, 所以上式中与h(x) 无关的正系数可以省略:\\nht = arg min\\nh\\n|D|\\nX\\ni=1\\nD′\\nt (xi) I (f (xi) ̸= h (xi))\\n此即式(8.18) 另一种表达形式。\\n注意, 为了确保D′\\nt(x) 是一个分布, 需要对其进行规范化, 即Dt(x) = D′\\nt(x)\\nZt ,\\n然而规范化因子Zt = P|D|\\ni=1 D′\\nt (xi) 为常数, 并不影响最小化的求解。正是基于此结论, AdaBoost 通过\\nht = L (D, Dt ) 得到第t 轮的基分类器。“西瓜书”图8.3 的第3 行\\nDt+1 (xi) = D (xi) e−f(xi)Ht(xi)\\n= D (xi) e−f(xi)(Ht−1(xi)+αtht(xi))\\n= D (xi) e−f(xi)Ht−1(xi)e−f(xi)αtht(xi)', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 97, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='= D (xi) e−f(xi)Ht−1(xi)e−f(xi)αtht(xi)\\n= Dt (xi) e−f(xi)αtht(xi)\\n此即类似式(8.19) 的分布权重更新公式。\\n现在只差权重αt 表达式待求。对指数损失函数ℓexp (Ht−1 + αht | D) 求导, 得\\n∂ℓexp (Ht−1 + αht | D)\\n∂α\\n=\\n∂\\n\\x10\\ne−α P|D|\\ni=1 D′\\nt (xi) + (eα −e−α) P|D|\\ni=1 D′\\nt (xi) I (f (xi) ̸= h (xi))\\n\\x11\\n∂α\\n= −e−α\\n|D|\\nX\\ni=1\\nD′\\nt (xi) +\\n\\x00eα + e−α\\x01 |D|\\nX\\ni=1\\nD′\\nt (xi) I (f (xi) ̸= h (xi))\\n令导数等于零, 得\\ne−α\\neα + e−α =\\nP|D|\\ni=1 D′\\nt (xi) I (f (xi) ̸= h (xi))\\nP|D|\\ni=1 D′\\nt (xi)\\n=\\n|D|\\nX\\ni=1\\nD′\\nt (xi)\\nZt\\nI (f (xi) ̸= h (xi))\\n=\\n|D|\\nX\\ni=1', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 97, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='i=1\\nD′\\nt (xi)\\nZt\\nI (f (xi) ̸= h (xi))\\n=\\n|D|\\nX\\ni=1\\nDt (xi) I (f (xi) ̸= h (xi)) = Ex∼Dt [I (f (xi) ̸= h (xi))]\\n= ϵt\\n对上述等式化简, 得\\ne−α\\neα + e−α =\\n1\\ne2α + 1 ⇒e2α + 1 = 1\\nϵt\\n⇒e2α = 1 −ϵt\\nϵt\\n⇒2α = ln\\n\\x121 −ϵt\\nϵt\\n\\x13\\n⇒αt = 1\\n2 ln\\n\\x121 −ϵt\\nϵt\\n\\x13\\n→_→\\n配套视频教程：https://www.bilibili.com/video/BV1Mh411e7VU\\n←_←', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 97, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='→_→\\n欢迎去各大电商平台选购纸质版南瓜书《机器学习公式详解第2 版》\\n←_←\\n即式(8.11)。从该式可以发现, 当ϵt = 1 时, αt →∞, 此时集成分类器将由基分类器ht 决定, 而这很可能\\n是由于过拟合产生的结果, 例如不前枝决策树, 如果一直分下去, 一般情况下总能得到在训练集上分类误差\\n很小甚至为0 的分类器, 但这并没有什么意义。所以一般在AdaBoost 中使用弱分类器, 如决策树桩(即单\\n层决策树)。\\n另外, 由以上指数损失函数ℓexp (Ht−1 + αh | D) 的推导可以发现\\nℓexp (Ht−1 + αh | D) =\\n|D|\\nX\\ni=1\\nD (xi) e−f(xi)Ht−1(xi)e−f(xi)αh(xi)\\n=\\n|D|\\nX\\ni=1\\nD′\\nt (xi) e−f(xi)αh(xi)\\n这与指数损失函数ℓexp (αtht | Dt) 的表达式基本一致:\\nℓexp (αtht | Dt) = Ex∼Dt\\n\\x02\\ne−f(x)αtht(x)\\x03\\n=\\n|D|\\nX\\ni=1\\nDt (xi) e−f(xi)αtht(xt)\\n而D′', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 98, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='=\\n|D|\\nX\\ni=1\\nDt (xi) e−f(xi)αtht(xt)\\n而D′\\nt(x) 的规范化过程并不影响对ℓexp (Ht−1 + αh | D) 求最小化操作, 因此最小化式(8.9) 等价于最小化\\nℓexp (Ht−1 + αh | D), 这就是式(8.9) 的来历，故并无问题。\\n到此为止, 就逐一完成了“西瓜书”图8.3 中第3 行的ht 的训练(并计算训练误差)、第6 行的权重\\nαt 计算公式以及第7 行的分布Dt 更新公式来历的理论推导。\\n8.2.17\\n进一步理解权重更新公式\\nAdaboost 原始文献[1] 第12 页(pdf 显示第348 页) 有如下推论，如图8-3所示:\\n图8-3 Adaboost 原始文献推论2\\n即Px∼Dt (ht−1(x) ̸= f(x)) = 0.5 。用通俗的话来说就是, ht−1 在数据集D 上、分布为Dt 时的分类\\n误差为0.5, 即相当于随机猜测(最糟糕的二分类器是分类误差为0.5, 当二分类器分类误差为1 时相当于\\n分类误差为0 , 因为将预测结果反过来用就是了)。而ht 由式(8.18) 得到\\nht = arg min\\nh', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 98, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='ht = arg min\\nh\\nEx∼Dt[I(f(x) ̸= h(x))] = arg min\\nh\\nPx∼Dt(h(x) ̸= f(x))\\n即ht 是在数据集D 上、分布为Dt 时分类误差最小的分类器, 因此在数据集D 上、分布为Dt 时, ht 是\\n最好的分类器, 而ht−1 是最差的分类器, 故二者差别最大。\\n“西瓜书”第8.1 节的图8.2 形象的说明了“集\\n成个体应‘好而不同’\\n”, 此时可以说ht−1 和ht 非常“不同”\\n。证明如下:\\n对于ht−1 来说, 分类误差ϵt−1 为\\nϵt−1 = Px∼Dt−1 (ht−1(x) ̸= f(x)) = Ex∼Dt−1 [I (ht−1(x) ̸= f(x))]\\n=\\n|D|\\nX\\ni=1\\nDt−1 (xi) I (ht−1(x) ̸= f(x))\\n=\\nP|D|\\ni=1 Dt−1 (xi) I (ht−1(x) ̸= f(x))\\nP|D|\\ni=1 Dt−1 (xi) I (ht−1(x) = f(x)) + P|D|\\ni=1 Dt−1 (xi) I (ht−1(x) ̸= f(x))\\n→_→', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 98, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='i=1 Dt−1 (xi) I (ht−1(x) ̸= f(x))\\n→_→\\n配套视频教程：https://www.bilibili.com/video/BV1Mh411e7VU\\n←_←', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 98, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='→_→\\n欢迎去各大电商平台选购纸质版南瓜书《机器学习公式详解第2 版》\\n←_←\\n在第t 轮, 根据分布更新公式(8.19) 或“西瓜书”图8.3 第7 行(规范化因子Zt−1 为常量):\\nDt = Dt−1\\nZt−1\\ne−f(x)αt−1ht−1(x)\\n其中根据式(8.11), 第t −1 轮的权重\\nαt−1 = 1\\n2 ln 1 −ϵt−1\\nϵt−1\\n= ln\\ns\\n1 −ϵt−1\\nϵt−1\\n代入Dt 的表达式, 则\\nDt =\\n\\uf8f1\\n\\uf8f4\\n\\uf8f2\\n\\uf8f4\\n\\uf8f3\\nDt−1\\nZt−1 ·\\nq\\nϵt−1\\n1−ϵt−1\\n, if ht−1(x) = f(x)\\nDt−1\\nZt−1 ·\\nq\\n1−ϵt−1\\nϵt−1\\n, if ht−1(x) ̸= f(x)\\n那么ht−1 在数据集D 上、分布为Dt 时的分类误差Px∼Dt (ht−1(x) ̸= f(x) ) 为(注意, 下式第二行的分\\n母等于1, 因为I (ht−1(x) = f(x)) + I (ht−1(x) ̸= f(x)) = 1 )\\nPx∼Dt (ht−1(x) ̸= f(x)) = Ex∼Dt [I (ht−1(x) ̸= f(x))]\\n=\\nP|D|', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 99, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='=\\nP|D|\\ni=1 Dt (xi) I (ht−1 (xi) ̸= f (xi))\\nP|D|\\ni=1 Dt (xi) I (ht−1 (xi) = f (xi)) + P|D|\\ni=1 Dt (xi) I (ht−1 (xi) ̸= f (xi))\\n=\\nP|D|\\ni=1\\nDt−1(xi)\\nZt−1\\n·\\nq\\n1−ϵt−1\\nϵt−1 I (ht−1 (xi) ̸= f (xi))\\nP|D|\\ni=1\\nDt−1(xi)\\nZt−1\\n·\\nq\\nϵt−1\\n1−ϵt−1 I (ht−1 (xi) = f (xi)) + P|D|\\ni=1\\nDt−1(xi)\\nZt−1\\n·\\nq\\n1−ϵt−1\\nϵt−1 I (ht−1 (xi) ̸= f (xi))\\n=\\nq\\n1−ϵt−1\\nϵt−1\\n· P|D|\\ni=1 Dt−1 (xi) I (ht−1 (xi) ̸= f (xi))\\nq\\nϵt−1\\n1−ϵt−1 · P|D|\\ni=1 Dt−1 (xi) I (ht−1 (xi) = f (xi)) +\\nq\\n1−ϵt−1\\nϵt−1\\n· P|D|\\ni=1 Dt−1 (xi) I (ht−1 (xi) ̸= f (xi))\\n=', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 99, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='· P|D|\\ni=1 Dt−1 (xi) I (ht−1 (xi) ̸= f (xi))\\n=\\nq\\n1−ϵt−1\\nϵt−1\\n· ϵt−1\\nq\\nϵt−1\\n1−ϵt−1 · (1 −ϵt−1) +\\nq\\n1−ϵt−1\\nϵt−1\\n· ϵt−1\\n= 1\\n2\\n8.2.18\\n能够接受带权样本的基学习算法\\n在Adaboost 算法的推导过程中，我们发现能够接受并利用带权样本的算法才能很好的嵌入到Ad-\\naboost 的框架中作为基学习器。因此这里举一些能够接受带权样本的基学习算法的例子，分别是SVM 和\\n基于随机梯度下降(SGD) 的对率回归：\\n其实原理很简单: 对于SVM 来说, 针对“西瓜书”P130 页的优化目标式(6.29) 来说, 第二项为损失\\n项, 此时每个样本的损失ℓ0/1\\n\\x00yi\\n\\x00wTxi + b\\n\\x01\\n−1\\n\\x01\\n直接相加, 即样本权值分布为D (xi) =\\n1\\nm, 其中m 为数\\n据集D 样本个数; 若样本权值更新为Dt (xi), 则此时损失求和项应该变为\\nm\\nX\\ni=1\\nmDt (xi) · ℓ0/1\\n\\x00yi\\n\\x00wTxi + b\\n\\x01\\n−1\\n\\x01\\n若将D (xi) =\\n1', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 99, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='mDt (xi) · ℓ0/1\\n\\x00yi\\n\\x00wTxi + b\\n\\x01\\n−1\\n\\x01\\n若将D (xi) =\\n1\\nm 替换Dt (xi), 则就是每个样本的损失ℓ0/1\\n\\x00yi\\n\\x00wTxi + b\\n\\x01\\n−1\\n\\x01\\n直接相加。如此更改后, 最\\n后推导结果影响的是式(6.39), 将由C = αi + µi 变为\\nC · mDt (xi) = αi + µi\\n进而由αi, µi ≥0 导出0 ≤αi ≤C · mDt (xi) 。\\n对于基于随机梯度下降(SGD) 的对率回归, 每次随机选择一个样本进行梯度下降, 总体上的期望损失\\n即为式(3.27), 此时每个样本被选到的概率相同, 相当于D (xi) =\\n1\\nm 。若样本权值更新为Dt (xi), 则类似\\n→_→\\n配套视频教程：https://www.bilibili.com/video/BV1Mh411e7VU\\n←_←', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 99, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='→_→\\n欢迎去各大电商平台选购纸质版南瓜书《机器学习公式详解第2 版》\\n←_←\\n于SVM, 针对式(3.27) 只需要给第i 项乘以mDt (xi) 即可, 相当于每次随机梯度下降选择样本时以概率\\nDt (xi) 选择样本xi 即可。\\n注意, 这里总的损失中出现了样本个数m 。这是因为在定义损失时末求均值, 若对式(6.29) 的第二\\n项和式(3.27) 乘以\\n1\\nm 则可以将m 抵消掉。然而常数项在最小化式(3.27) 实际上并不影响什么, 对于式\\n(6.29) 来说只要选择平衡参数C 时选为原来的m 倍即可。\\n当然, 正如“西瓜书”P177 第三段中所说, “对无法接受带权样本的基学习算法, 则可通过“重采样\\n法’ 来处理, 即在每一轮学习中, 根据样本分布对训练集重新进行采样, 再用重采样而得的样本集对基学习\\n器进行训练”\\n。\\n8.3\\nBagging 与随机森林\\n8.3.1\\n式(8.20) 的解释\\nI (ht(x) = y) 表示对T 个基学习器，每一个都判断结果是否与y 一致，y 的取值一般是−1 和1，如', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 100, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='果基学习器结果与y 一致，则I (ht(x) = y) = 1，如果样本不在训练集内，则I (x /\\n∈Dt) = 1，综合起来\\n看就是，对包外的数据，用“投票法”选择包外估计的结果，即1 或-1。\\n8.3.2\\n式(8.21) 的推导\\n由式(8.20) 知，Hoob(x) 是对包外的估计，该式表示估计错误的个数除以总的个数，得到泛化误差的\\n包外估计。注意在本式直接除以D | (训练集D 样本个数), 也就是说此处假设T 个基分类器的各自的包\\n外样本的并集一定为训练集D 。实际上, 这个事实成立的概率也是比较大的, 可以计算一下: 样本属于包\\n内的概率为0.632, 那么T 次独立的随机采样均属于包内的概率为0.632T , 当T = 5 时, 0.632T ≈0.1, 当\\nT = 10 时, 0.632T ≈0.01, 这么来看的话T 个基分类器的各自的包外样本的并集为训练集D 的概率的确\\n实比较大。\\n8.3.3\\n随机森林的解释\\n在8.3.2 节开篇第一句话就解释了随机森林的概念：随机森林是Bagging 的一个扩展变体，是以决策', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 100, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='树为基学习器构建Bagging 集成的基础上，进一步在决策树的训练过程中引入了随机属性选择。\\n完整版随机森林当然更复杂，这时只须知道两个重点：(1) 以决策树为基学习器；(2) 在基学习器训练\\n过程中，选择划分属性时只使用当前结点属性集合的一个子集。\\n8.4\\n结合策略\\n8.4.1\\n式(8.22) 的解释\\nH(x) = 1\\nT\\nT\\nX\\ni=1\\nhi(x)\\n对基分类器的结果进行简单的平均。\\n8.4.2\\n式(8.23) 的解释\\nH(x) =\\nT\\nX\\ni=1\\nwihi(x)\\n对基分类器的结果进行加权平均。\\n→_→\\n配套视频教程：https://www.bilibili.com/video/BV1Mh411e7VU\\n←_←', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 100, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='→_→\\n欢迎去各大电商平台选购纸质版南瓜书《机器学习公式详解第2 版》\\n←_←\\n8.4.3\\n硬投票和软投票的解释\\n“西瓜书”\\n中第183 页提到了硬投票(hard voting) 和软投票(soft voting)，\\n本页左侧注释也提到多数投\\n票法的英文术语使用不太一致，有文献称为majority voting。本人看到有些文献中，硬投票使用majority\\nvoting（多数投票）\\n，软投票使用probability voting（概率投票）\\n，所以还是具体问题具体分析比较稳妥。\\n8.4.4\\n式(8.24) 的解释\\nH(x) =\\n(\\ncj,\\nif PT\\ni=1 hj\\ni(x) > 0.5 PN\\nk=1\\nPT\\ni=1 hk\\ni (x)\\nreject,\\notherwise.\\n当某一个类别j 的基分类器的结果之和，大于所有结果之和的1\\n2，则选择该类别j 为最终结果。\\n8.4.5\\n式(8.25) 的解释\\nH(x) = carg max\\nj\\n∑T\\ni=1 hj\\ni (x)\\n相比于其他类别，该类别j 的基分类器的结果之和最大，则选择类别j 为最终结果。\\n8.4.6\\n式(8.26) 的解释', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 101, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='8.4.6\\n式(8.26) 的解释\\nH(x) = carg max\\nj\\n∑T\\ni=1 wihj\\ni (x)\\n相比于其他类别，该类别j 的基分类器的结果之和最大，则选择类别j 为最终结果，与式(8.25) 不同的\\n是，该式在基分类器前面乘上一个权重系数，该系数大于等于0，且T 个权重之和为1。\\n8.4.7\\n元学习器(meta-learner) 的解释\\n书中第183 页最后一行提到了元学习器(meta-learner)，简单解释一下，因为理解meta 的含义有时\\n对于理解论文中的核心思想很有帮助。\\n元(meta)，非常抽象，例如此处的含义，即次级学习器，或者说基于学习器结果的学习器；另外还有\\n元语言，就是描述计算机语言的语言，还有元数学，研究数学的数学等等；\\n另外，\\n论文中经常出现的还有meta-strategy，\\n即元策略或元方法，\\n比如说你的研究问题是多分类问题，\\n那么你提出了一种方法，例如对输入特征进行变换（或对输出类别做某种变换）\\n，然后再基于普通的多分\\n类方法进行预测，这时你的方法可以看成是一种通用的框架，它虽然针对多分类问题开发，但它需要某个', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 101, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='类方法进行预测，这时你的方法可以看成是一种通用的框架，它虽然针对多分类问题开发，但它需要某个\\n具体多分类方法配合才能实现，那么这样的方法是一种更高层级的方法，可以称为是一种meta-strategy。\\n8.4.8\\nStacking 算法的解释\\n该算法其实非常简单，对于数据集，试想你现在有了个基分类器预测结果，也就是说数据集中的每个\\n样本均有个预测结果，那么怎么结合这个预测结果呢？\\n本节名为“结合策略”\\n，告诉你各种结合方法，但其实最简单的方法就是基于这个预测结果再进行一\\n次学习，即针对每个样本，将这个预测结果作为输入特征，类别仍为原来的类别，既然无法抉择如何将这\\n些结果进行结合，那么就“学习”一下吧。\\n“西瓜书”图8.9 伪代码第9 行中将第个样本进行变换，特征为个基学习器的输出，类别标记仍为原\\n来的，将所有训练集中的样本进行转换得到新的数据集后，再基于进行一次学习即可，也就是Stacking 算\\n法。\\n→_→\\n配套视频教程：https://www.bilibili.com/video/BV1Mh411e7VU\\n←_←', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 101, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='→_→\\n欢迎去各大电商平台选购纸质版南瓜书《机器学习公式详解第2 版》\\n←_←\\n至于说“西瓜书”图8.9 中伪代码第1 行到第3 行使用的数据集与第5 行到第10 行使用的数据集之\\n间的关系，在“西瓜书”图8.9 下方的一段话有详细的讨论，不再赘述。\\n8.5\\n多样性\\n8.5.1\\n式(8.27) 的解释\\nA (hi|x) = (hi(x) −H(x))2\\n该式表示个体学习器结果与预测结果的差值的平方，即为个体学习器的“分歧”\\n。\\n8.5.2\\n式(8.28) 的解释\\n¯\\nA(h|x) =\\nT\\nX\\ni=1\\nwiA (hi|x)\\n=\\nT\\nX\\ni=1\\nwi (hi(x) −H(x))2\\n该式表示对各个个体学习器的“分歧”加权平均的结果，即集成的“分歧”\\n。\\n8.5.3\\n式(8.29) 的解释\\nE (hi|x) = (f(x) −hi(x))2\\n该式表示个体学习器与真实值之间差值的平方，即个体学习器的平方误差。\\n8.5.4\\n式(8.30) 的解释\\nE(H|x) = (f(x) −H(x))2\\n该式表示集成与真实值之间差值的平方，即集成的平方误差。\\n8.5.5\\n式(8.31) 的推导', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 102, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='该式表示集成与真实值之间差值的平方，即集成的平方误差。\\n8.5.5\\n式(8.31) 的推导\\n由(8.28) 知\\n¯\\nA(h|x) =\\nT\\nX\\ni=1\\nwi (hi(x) −H(x))2\\n=\\nT\\nX\\ni=1\\nwi(hi(x)2 −2hi(x)H(x) + H(x)2)\\n=\\nT\\nX\\ni=1\\nwihi(x)2 −H(x)2\\n→_→\\n配套视频教程：https://www.bilibili.com/video/BV1Mh411e7VU\\n←_←', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 102, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='→_→\\n欢迎去各大电商平台选购纸质版南瓜书《机器学习公式详解第2 版》\\n←_←\\n又因为\\nT\\nX\\ni=1\\nwiE (hi|x) −E(H|x)\\n=\\nT\\nX\\ni=1\\nwi (f(x) −hi(x))2 −(f(x) −H(x))2\\n=\\nT\\nX\\ni=1\\nwihi(x)2 −H(x)2\\n所以\\n¯\\nA(h|x) =\\nT\\nX\\ni=1\\nwiE (hi|x) −E(H|x)\\n8.5.6\\n式(8.32) 的解释\\nT\\nX\\ni=1\\nwi\\nZ\\nA (hi|x) p(x)dx =\\nT\\nX\\ni=1\\nwi\\nZ\\nE (hi|x) p(x)dx −\\nZ\\nE(H|x)p(x)dx\\nR\\nA (hi|x) p(x)dx 表示个体学习器在全样本上的“分歧”\\n，PT\\ni=1 wi\\nR\\nA (hi|x) p(x)dx 表示集成在全样本上\\n的“分歧”\\n。式(8.31) 的意义在于, 对于示例x 有¯\\nA(h | x) = ¯\\nE(h | x) −E(H | x) 成立, 即个体学习器分\\n歧的加权均值等于个体学习器误差的加权均值减去集成H(x) 的误差。\\n将这个结论应用于全样本上, 即为式(8.32)。\\n例如Ai =\\nR', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 103, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='将这个结论应用于全样本上, 即为式(8.32)。\\n例如Ai =\\nR\\nA (hi | x) p(x)dx, 这是将x 作为连续变量来处理的, 所以这里是概率密度p(x) 和积分\\n号; 若按离散变量来处理, 则变为Ai = P\\nx∈D A (hi | x) px; 其实高等数学中讲过, 积分就是连续求和。\\n8.5.7\\n式(8.33) 的解释\\nEi =\\nZ\\nE (hi|x) p(x)dx\\n表示个体学习器在全样本上的泛化误差。\\n8.5.8\\n式(8.34) 的解释\\nAi =\\nZ\\nA (hi|x) p(x)dx\\n表示个体学习器在全样本上的分歧。\\n8.5.9\\n式(8.35) 的解释\\nE =\\nZ\\nE(H|x)p(x)dx\\n表示集成在全样本上的泛化误差。\\n8.5.10\\n式(8.36) 的解释\\nE = ¯\\nE −¯\\nA\\n¯\\nE 表示个体学习器泛化误差的加权均值，¯\\nA 表示个体学习器分歧项的加权均值，该式称为“误差-分歧分\\n解”\\n。\\n→_→\\n配套视频教程：https://www.bilibili.com/video/BV1Mh411e7VU\\n←_←', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 103, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='→_→\\n欢迎去各大电商平台选购纸质版南瓜书《机器学习公式详解第2 版》\\n←_←\\n8.5.11\\n式(8.40) 的解释\\n当p1 = p2 时, κ = 0; 当p1 = 1 时, κ = 1; 一般来说p1 ⩾p2, 即κ ⩾0, 但偶尔也有p1 < p2 的情况,\\n此时κ < 0 。有关p1, p2 的意义参见式(8.41) 和式(8.42) 的解释。\\n8.5.12\\n式(8.41) 的解释\\n分子a + d 为分类器hi 与hj 在数据集D 上预测结果相同的样本数目, 分母为数据集D 总样本数目,\\n因此p1 为两个分类器hi 与hj 预测结果相同的概率。若a + d = m, 即分类器hi 与hj 对数据集D 所有\\n样本预测结果均相同, 此时p1 = 1 。\\n8.5.13\\n式(8.42) 的解释\\n将式(8.42) 拆分为如下形式，将会很容易理解其含义：\\np2 = a + b\\nm\\n· a + c\\nm\\n+ c + d\\nm\\n· b + d\\nm\\n其中a+b\\nm\\n为分类器hi 将样本预测为+1 的概率,\\na+c\\nm\\n为分类器hj 将样本预测为+1 的概率, 二者相乘\\na+b\\nm · a+c', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 104, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='a+c\\nm\\n为分类器hj 将样本预测为+1 的概率, 二者相乘\\na+b\\nm · a+c\\nm 可理解为分类器hi 与hj 将样本预测为+1 的概率; c+d\\nm 为分类器hi 将样本预测为−1 的概率,\\nb+d\\nm 为分类器hj 将样本预测为−1 的概率, 二者相乘c+d\\nm · b+d\\nm 可理解为分类器hi 与hj 将样本预测为−1\\n的概率。\\n注意a+b\\nm · a+c\\nm 与\\na\\nm 的不同, c+d\\nm · b+d\\nm 与\\nd\\nm 的不同:\\na + b\\nm\\n· a + c\\nm\\n= p (hi = +1) p (hj = +1) , a\\nm = p (hi = +1, hj = +1)\\nc + d\\nm\\n· b + d\\nm\\n= p (hi = −1) p (hj = −1) , d\\nm = p (hi = −1, hj = −1)\\n即a+b\\nm · a+c\\nm\\n和c+d\\nm · b+d\\nm 是分别考虑分类器hi 与hj 时的概率( hi 与hj 独立), 而\\na\\nm 和\\nd\\nm 是同时考虑\\nhi 与hj 时的概率(联合概率)。\\n8.5.14\\n多样性增强的解释', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 104, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='m 和\\nd\\nm 是同时考虑\\nhi 与hj 时的概率(联合概率)。\\n8.5.14\\n多样性增强的解释\\n在8.5.3 节介绍了四种多样性增强的方法, 通俗易懂, 几乎不需要什么注解, 仅强调几个概念:\\n(1) 数据样本扰动中提到了“不稳定基学习器”(例如决策树、神经网络等) 和“稳定基学习器”(例\\n如线性学习器、支持向量机、朴素贝叶斯、k 近邻学习器等), 对稳定基学习器进行集成时数据样本扰动技\\n巧效果有限。这也就可以解释为什么随机森林和GBDT 等以决策树为基分学习器的集成方法很成功吧,\\nGradient Boosting 和Bagging 都是以数据样本扰动来增强多样性的; 而且, 掌握这个经验后在实际工程应\\n用中就可以排除一些候选基分类器, 但论文中的确经常见到以支持向量机为基分类器Bagging 实现, 这可\\n能是由于LIBSVM 简单易用的原因吧。\\n(2)“西瓜书”图8.11 随机子空间算法, 针对每个基分类器ht 在训练时使用了原数据集的部分输入属\\n性（末必是初始属性, 详见第189 页左上注释), 因此在最终集成时“西瓜书”图8.11 最后一行也要使用\\n相同的部分属性。', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 104, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='相同的部分属性。\\n(3) 输出表示扰动中提到了“翻转法”(Flipping Output), 看起来是一个并没有道理的技巧, 为什么要\\n将训练样本的标记改变呢? 若认为原训练样本标记是完全可靠的, 这不是人为地加入噪声么? 但西瓜书作\\n者2017 年提出的深度森林[3] 模型中也用到了该技巧, 正如本小节名为“多样性增强”, 虽然从局部来看\\n引入了标记噪声, 但从模型集成的角度来说却是有益的。\\n8.6\\nGradient Boosting/GBDT/XGBoost 联系与区别\\n在集成学习中，梯度提升(Gradient Boosting, GB)、梯度提升树(GB Decision Tree, GBDT) 很常见，\\n尤其是近几年非常流行的XGBoost 很是耀眼，此处单独介绍对比这些概念。\\n→_→\\n配套视频教程：https://www.bilibili.com/video/BV1Mh411e7VU\\n←_←', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 104, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='→_→\\n欢迎去各大电商平台选购纸质版南瓜书《机器学习公式详解第2 版》\\n←_←\\n8.6.1\\n梯度下降法\\n本部分内容参考了孙文瑜教授的最优化方法[4] 设目标函数f(x) 在xk 附近连续可微, 且∇f (xk) =\\n∇f(x)\\n∇x\\n\\x0c\\n\\x0c\\n\\x0c\\nx=xk ̸= 0 。将f(x) 在xk 处进行一阶Taylor 展开\\nf(x) ≈f (xk) + ∇f (xk)T (x −xk)\\n记x −xk = ∆x, 则上式可写为\\nf (xk + ∆x) ≈f (xk) + ∇f (xk)T ∆x\\n显然, 若∇f (xk)T ∆x < 0 则有f (xk + ∆x) < f (xk), 即相比于f (xk), 自变量增量∆x 会使f(x)\\n函数值下降; 若要使f(x) = f (xk + ∆x) 下降最快, 只要选择∆x 使∇f (xk)T ∆x 最小即可, 而此时\\n∇f (xk)T ∆x < 0, 因此使绝对值|f (xk)T ∆x| 最大即可。将∆x 分成两部分: ∆x = αkdk, 其中dk 为待', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 105, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='求单位向量, αk > 0 为待解常量; dk 表示往哪个方向改变x 函数值下降最快, 而αk 表示沿这个方向的步\\n长。因此, 求解∆x 的问题变为\\n(αk, dk) = arg min\\nα,d\\n∇f (xk)T αd\\n将以上优化问题分为两步求解, 即\\ndk = arg min\\nd\\n∇f (xk)T d\\ns.t. ∥d∥2 = 1\\nαk = arg min\\nα\\n∇f (xk)T dkα\\n以上求解αk 的优化问题明显有问题, 因为对于∇f (xk)T dk < 0 来说, 显然αk = +∞时取的最小值, 求\\n解αk 应该求解如下优化问题:\\nαk = arg min\\nα\\nf (xk + αdk)\\n对于凸函数来说, 以上两步可以得到最优解; 但对于非凸函数来说, 联合求解得到dk 和αk, 与先求dk 然\\n后基于此再求αk 的结果应该有时是不同的。由Cauchy-Schwartz 不等式\\n\\x0c\\n\\x0c\\n\\x0c∇f (xk)T dk\\n\\x0c\\n\\x0c\\n\\x0c ≤∥∇f (xk)∥2 ∥dk∥2\\n可知, 当且仅当dk = −\\n∇f(xk)', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 105, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='≤∥∇f (xk)∥2 ∥dk∥2\\n可知, 当且仅当dk = −\\n∇f(xk)\\n∥∇f(xk)∥2 时, ∇f (xk)T dk 最小, −∇f (xk)T dk 最大。对于αk, 若f (xk + αdk) 对\\nα 的导数存在, 则可简单求解如下单变量方程即可:\\n∂f (xk + αdk)\\n∂α\\n= 0\\n例1: 试求f(x) = x2 在xk = 2 处的梯度方向dk 和步长αk 。解: 对f(x) 在xk = 2 处进行一阶Taylor\\n展开:\\nf(x) = f (xk) + f ′ (xk) (x −xk)\\n= x2\\nk + 2xk (x −xk)\\n= x2\\nk + 2xkαd\\n由于此时自变量为一维, 因此只有两个方向可选, 要么正方向, 要么负方向。此时f ′ (xk) = 4, 因此dk =\\n−f ′(xk)\\n|f ′(xk)| = −1 。接下来求αk, 将xk 和dk 代入:\\nf (xk + αdk) = f(2 −α) = (2 −α)2\\n进而有\\n∂f (xk + αdk)\\n∂α\\n= −2(2 −α)\\n→_→', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 105, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='进而有\\n∂f (xk + αdk)\\n∂α\\n= −2(2 −α)\\n→_→\\n配套视频教程：https://www.bilibili.com/video/BV1Mh411e7VU\\n←_←', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 105, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='→_→\\n欢迎去各大电商平台选购纸质版南瓜书《机器学习公式详解第2 版》\\n←_←\\n令导数等于0 , 得αk = 2 。此时\\n∆x = αkdk = −2\\n则xk + ∆x = 0, 函数值f (xk + ∆x) = 0 。例2: 试求f(x) = ∥x∥2\\n2 = xTx 在xk = [x1\\nk, x2\\nk]\\nT = [3, 4]T 处\\n的梯度方向dk 和步长αk 。解: 对f(x) 在xk = [x1\\nk, x2\\nk]\\nT = [3, 4]T 处进行一阶Taylor 展开:\\nf(x) = f (xk) + ∇f (xk)T (x −xk)\\n= ∥x∥2\\n2 + 2xT\\nk (x −xk)\\n= ∥x∥2\\n2 + 2xT\\nk αd\\n此时∇f (xk) = [6, 8]T, 因此dk = −\\n∇f(xk)\\n∥∇f(xk)∥2 = [−0.6, −0.8]T 。接下来求αk, 将xk 和dk 代入:\\nf (xk + αdk) = (3 −0.6α)2 + (4 −0.8α)2\\n= α2 −10α + 25\\n= (α −5)2\\n因此可得αk = 5 (或对α 求导, 再令导数等于0 )。此时', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 106, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='= (α −5)2\\n因此可得αk = 5 (或对α 求导, 再令导数等于0 )。此时\\n∆x = αkdk = [−3, −4]T\\n则xk + ∆x = [0, 0]T, 函数值f (xk + ∆x) = 0 。通过以上分析, 只想强调两点: (1) 梯度下降法求解下降\\n最快的方向dk 时应该求解如下优化问题:\\ndk = arg min\\nd\\n∇f (xk)T d s.t. ∥d∥2 = C\\n其中C 为常量, 即不必严格限定∥dk∥2 = 1, 只要固定向量长度, 与αk 搭配即可。(2) 梯度下降法求解步\\n长αk 应该求解如下优化问题:\\nαk = arg min\\nα\\nf (xk + αdk)\\n实际应用中, 很多时候不会去求最优的αk, 而是靠经验设置一个步长。\\n8.6.2\\n从梯度下降的角度解释AdaBoost\\nAdaBoost 第t 轮迭代时最小化式(8.5) 的指数损失函数\\nℓexp (Ht | D) = Ex∼D\\n\\x02\\ne−f(x)Ht(x)\\x03\\n=\\nX\\nx∈D\\nD(x)e−f(x)Ht(x)\\n对ℓexp (Ht | D) 每一项在Ht−1 处泰勒展开\\nℓexp (Ht | D) ≈', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 106, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='对ℓexp (Ht | D) 每一项在Ht−1 处泰勒展开\\nℓexp (Ht | D) ≈\\nX\\nx∈D\\nD(x)\\n\\x00e−f(x)Ht−1(x) −f(x)e−f(x)Ht−1(x) (Ht(x) −Ht−1(x))\\n\\x01\\n=\\nX\\nx∈D\\nD(x)\\n\\x00e−f(x)Ht−1(x) −e−f(x)Ht−1(x)f(x)αtht(x)\\n\\x01\\n= Ex∼D\\n\\x02\\ne−f(x)Ht−1(x) −e−f(x)Ht−1(x)f(x)αtht(x)\\n\\x03\\n其中Ht = Ht−1 +αtht 。\\n注意: αt, ht 是第t 轮待解的变量。\\n另外补充一下, 在上式展开中的变量为Ht(x),\\n在Ht−1 处一阶导数为\\n∂e−f(x)Ht(x)\\n∂Ht(x)\\n\\x0c\\n\\x0c\\n\\x0c\\n\\x0c\\nHt(x)=Ht−1(x)\\n= −f(x)e−f(x)Ht−1(x)\\n→_→\\n配套视频教程：https://www.bilibili.com/video/BV1Mh411e7VU\\n←_←', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 106, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='→_→\\n欢迎去各大电商平台选购纸质版南瓜书《机器学习公式详解第2 版》\\n←_←\\n如果看不习惯上述泰勒展开过程, 可令变量z = Ht(x) 和函数g(z) = e−f(x)z, 对g(z) 在z0 = Ht−1(x) 处\\n泰勒展开, 得\\ng(z) ≈g (z0) + g′ (z0) (z −z0)\\n= g (z0) −f(x)e−f(x)z0 (z −z0)\\n= e−f(x)Ht−1(x) −e−f(x)Ht−1(x)f(x) (Ht(x) −Ht−1(x))\\n= e−f(x)Ht−1(x) −e−f(x)Ht−1(x)f(x)αtht(x)\\n注意此处ht(x) ∈{−1, +1}, 类似于3.3.2 节梯度下降法中的约束\\n\\r\\n\\rdt\\r\\n\\r = 1 。类似于使用梯度下降法\\n求解下降最快的方向dt, 此处先求ht (先不管αt ):\\nht = arg min\\nh\\nX\\nx∈D\\nD(x)\\n\\x00−e−f(x)Ht−1(x)f(x)h(x)\\n\\x01\\ns.t. h(x) ∈{−1, +1}\\n将负号去掉, 最小化变为最大化问题\\nht = arg max\\nh\\nX\\nx∈D\\nD(x)', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 107, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='将负号去掉, 最小化变为最大化问题\\nht = arg max\\nh\\nX\\nx∈D\\nD(x)\\n\\x00e−f(x)Ht−1(x)f(x)h(x)\\n\\x01\\n= arg max\\nh\\nEx∼D\\n\\x02\\ne−f(x)Ht−1(x)f(x)h(x)\\n\\x03\\ns.t. h(x) ∈{−1, +1}\\n这就是式(8.14) 的第3 个等号的结果, 因此其余推导参见8.2.16节即可。由于这里的h(x) 约束较强, 因此\\n不能直接取负梯度方向, 书中经过推导得到了ht(x) 的表达式, 即式(8.18)。实际上, 可以将此结果理解为\\n满足约束条件的最快下降方向。求得ht(x) 之后再求αt (8.2.16节“AdaBoost 的个人推导”注解中已经写\\n过一遍, 此处仅粘贴至此, 具体参见8.2.16节注解，尤其是ℓexp (Ht−1 + αht | D) 表达式的由来):\\nαk = arg min\\nα\\nℓexp (Ht−1 + αht | D)\\n对指数损失函数ℓexp (Ht−1 + αht | D) 求导, 得\\n∂ℓexp (Ht−1 + αht | D)\\n∂α\\n=\\n∂\\n\\x10\\ne−α P|D|\\ni=1 D′', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 107, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='∂ℓexp (Ht−1 + αht | D)\\n∂α\\n=\\n∂\\n\\x10\\ne−α P|D|\\ni=1 D′\\nt (xi) + (eα −e−α) P|D|\\ni=1 D′\\nt (xi) I (f (xi) ̸= h (xi))\\n\\x11\\n∂α\\n= −e−α\\n|D|\\nX\\ni=1\\nD′\\nt (xi) +\\n\\x00eα + e−α\\x01 |D|\\nX\\ni=1\\nD′\\nt (xi) I (f (xi) ̸= h (xi))\\n令导数等于零, 得\\ne−α\\neα + e−α =\\nP|D|\\ni=1 D′\\nt (xi) I (f (xi) ̸= h (xi))\\nP|D|\\ni=1 D′\\nt (xi)\\n=\\n|D|\\nX\\ni=1\\nD′\\nt (xi)\\nZt\\nI (f (xi) ̸= h (xi))\\n=\\n|D|\\nX\\ni=1\\nDt (xi) I (f (xi) ̸= h (xi)) = Ex∼Dt [I (f (xi) ̸= h (xi))]\\n= ϵt\\n对上述等式化简, 得\\ne−α\\neα + e−α =\\n1\\ne2α + 1 ⇒e2α + 1 = 1\\nϵt\\n⇒e2α = 1 −ϵt\\nϵt\\n⇒2α = ln\\n\\x121 −ϵt\\nϵt\\n\\x13\\n⇒αt = 1', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 107, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='ϵt\\n⇒e2α = 1 −ϵt\\nϵt\\n⇒2α = ln\\n\\x121 −ϵt\\nϵt\\n\\x13\\n⇒αt = 1\\n2 ln\\n\\x121 −ϵt\\nϵt\\n\\x13\\n即式(8.11)。通过以上推导可以发现: AdaBoost 每一轮的迭代就是基于梯度下降法求解损失函数为指数\\n损失函数的二分类问题约束条件ht(x) ∈{−1, +1} 。\\n→_→\\n配套视频教程：https://www.bilibili.com/video/BV1Mh411e7VU\\n←_←', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 107, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='→_→\\n欢迎去各大电商平台选购纸质版南瓜书《机器学习公式详解第2 版》\\n←_←\\n8.6.3\\n梯度提升(Gradient Boosting)\\n将AdaBoost 的问题一般化, 即不限定损失函数为指数损失函数, 也不局限于二分类问题, 则可以将式\\n(8.5) 写为更一般化的形式\\nℓ(Ht | D) = Ex∼D [err (Ht(x), f(x))]\\n= Ex∼D [err (Ht−1(x) + αtht(x), f(x))]\\n问题时, f(x) ∈R, 损失函数可使用平方损失err (Ht(x), f(x)) = (Ht(x) −f(x))2 。针对该一般化的损失\\n函数和一般的学习问题, 要通过T 轮迭代得到学习器\\nH(x) =\\nT\\nX\\nt=1\\nαtht(x)\\n类似于AdaBoost, 第t 轮得到αt, ht(x), 可先对损失函数在Ht−1(x) 处进行泰勒展开:\\nℓ(Ht | D) ≈Ex∼D\\n\"\\nerr (Ht−1(x), f(x)) + ∂err (Ht(x), f(x))\\n∂Ht(x)\\n\\x0c\\n\\x0c\\n\\x0c\\n\\x0c\\nHt(x)=Ht−1(x)\\n(Ht(x) −Ht−1(x))\\n#', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 108, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='∂Ht(x)\\n\\x0c\\n\\x0c\\n\\x0c\\n\\x0c\\nHt(x)=Ht−1(x)\\n(Ht(x) −Ht−1(x))\\n#\\n= Ex∼D\\n\"\\nerr (Ht−1(x), f(x)) + ∂err (Ht(x), f(x))\\n∂Ht(x)\\n\\x0c\\n\\x0c\\n\\x0c\\n\\x0c\\nHt(x)=Ht−1(x)\\nαtht(x)\\n#\\n= Ex∼D [err (Ht−1(x), f(x))] + Ex∼D\\n\"\\n∂err (Ht(x), f(x))\\n∂Ht(x)\\n\\x0c\\n\\x0c\\n\\x0c\\n\\x0c\\nHt(x)=Ht−1(x)\\nαtht(x)\\n#\\n注意, 在上式展开中的变量为Ht(x), 且有Ht(x) = Ht−1(x)+αtht(x) (类似于梯度下降法中x = xk + αkdk)\\n。上式中括号内第1 项为常量ℓ(Ht−1 | D), 最小化ℓ(Ht | D) 只须最小化第2 项即可。先不考虑权重αt,\\n求解如下优化问题可得ht(x) :\\nht(x) = arg min\\nh\\nEx∼D\\n\"\\n∂err (Ht(x), f(x))\\n∂Ht(x)\\n\\x0c\\n\\x0c\\n\\x0c\\n\\x0c\\nHt(x)=Ht−1(x)\\nh(x)\\n#\\ns.t. constraints for h(x)', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 108, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='Ht(x)=Ht−1(x)\\nh(x)\\n#\\ns.t. constraints for h(x)\\n解得ht(x) 之后, 再求解如下优化问题可得权重αt :\\nαt = arg min\\nα\\nEx∼D [err (Ht−1(x) + αht(x), f(x))]\\n以上就是梯度提升(Gradient Boosting) 的理论框架, 即每轮通过梯度(Gradient) 下降的方式将T 个弱学\\n习器提升(Boosting) 为强学习器。可以看出AdaBoost 是其特殊形式。\\nGradient Boosting 算法的官方版本参见[5] 第5-6 页(第1193-1194 页)，其中算法部分见算法1\\nAlgorithm 1 Gradient_Boost(A, p, r)\\n1: F0(x) = arg minρ\\nPN\\ni=1 L (yi, ρ)\\n2: for m = 1 doM\\n3:\\n˜\\nyi = −\\nh\\n∂L(yi,F (xj))\\n∂F (xi)\\ni\\nF (x)=Fm−1(x) , i = 1, N\\n4:\\nam = arg mina,β\\nPN\\ni=1 [˜\\nyi −βh (xi; a)]2', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 108, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='4:\\nam = arg mina,β\\nPN\\ni=1 [˜\\nyi −βh (xi; a)]2\\n5:\\nρm = arg minρ\\nPN\\ni=1 L (yi, Fm−1 (xi) + ρh (xi; am))\\n6:\\nFm(x) = Fm−1(x) + ρmh (x; am)\\n感觉该伪代码针对的还是在任意损失函数L (yi, F (xi)) 下的回归问题。Algorithm 1 中第3 步和第4\\n步意思是用βh (xi, a) 拟合F(x) = Fm−1(x) 处负梯度, 但第4 步表示只求参数am, 第5 步单独求解参数\\nρm, 这里的疑问是为什么第4 步要用最小二乘法（即3.2 节的线性回归）去拟合负梯度（又称伪残差）?\\n→_→\\n配套视频教程：https://www.bilibili.com/video/BV1Mh411e7VU\\n←_←', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 108, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='→_→\\n欢迎去各大电商平台选购纸质版南瓜书《机器学习公式详解第2 版》\\n←_←\\n简单理解如下: 第4 步要解的h (xi, a) 相当于梯度下降法中的待解的下降方向d, 在梯度下降法中也\\n已提到不必严格限制∥d∥2 = 1, 长度可以由步长α 调节(例如前面梯度下降方解释中的例1 , 若直接取\\ndk = −f ′ (xk) = −4, 则可得αk = 0.5, 仍有∆x = αkdk = −2), 因此第4 步直接用h (xi, a) 拟合负梯度,\\n与梯度下降中约束∥d∥2 = 1 的区别在于末对负梯度除以其模值进行归一化而已。\\n那为什么不是直接令h (xi, a) 等于负梯度呢? 因为这里实际是求假设函数h, 将数据集中所有的xi\\n经假设函数h 映射到对应的伪残差(负梯度) ˜\\nyi, 所以只能做线性回归了。\\n李航《统计学习方法》[2] 第8.4.3 节中的算法8.4 并末显式体现参数ρm, 这应该是第2 步的(c) 步完\\n成的, 因为(b) 步只是拟合一棵回归树(相当于Algorithm 1 第4 步解得h (xi, a) ), 而(c) 步才确定每个', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 109, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='叶结点的取值(相当于Algorithm 1 第5 步解得ρm, 只是每个叶结点均对应一个ρm); 而且回归问题中基\\n函数为实值函数，可以将参数ρm 吸收到基函数中。\\n8.6.4\\n梯度提升树(GBDT)\\n本部分无实质GBDT 内容，仅为梳理GBDT 的概念，具体可参考给出的资源链接。\\n对于GBDT，一般资料是按Gradient Boosting+CART 处理回归问题讲解的，如林轩田《机器学习\\n技法》课程第11 讲。但是，分类问题也可以用回归来处理，例如3.3 节的对数几率回归，只需将平方损\\n失换为对率损失（参见式(3.27) 和式(6.33)，二者关系可参见第3 章注解中有关式(3.27) 的推导）即可。\\n细节可以搜索林轩田老师的《机器学习基石》和《机器学习技法》两门课程以及配套的视频。\\n8.6.5\\nXGBoost\\n本部分无实质XGBoost 内容，仅为梳理XGBoost 的概念，具体可参考给出的资源链接。\\n首先，\\nXGBoost 是eXtreme Gradient Boosting 的简称。\\n其次，\\nXGBoost 与GBDT 的关系，\\n可大致类\\n比为LIBSVM 与SVM', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 109, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='其次，\\nXGBoost 与GBDT 的关系，\\n可大致类\\n比为LIBSVM 与SVM\\n（或SMO 算法）\\n的关系。\\nLIBSVM 是SVM 算法的一种高效实现软件包，\\nXGBoost\\n是GBDT 的一种高效实现；在实现层面，LIBSVM 对SMO 算法进行了许多改进，XGBoost 也对GBDT\\n进行了许多改进；另外，LIBSVM 扩展了许多SVM 变体，XGBoost 也不再仅仅是标准的GBDT，也扩\\n展了一些其它功能。最后，XGBoost 是由陈天奇开发的；XGBoost 论文可以参考[6]，XGBoost 工具包、\\n文档和源码等均可以在Github 上搜索到。\\n参考文献\\n[1] Jerome Friedman, Trevor Hastie, and Robert Tibshirani. Additive logistic regression: a statistical view', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 109, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='of boosting (with discussion and a rejoinder by the authors). The annals of statistics, 28(2):337–407,\\n2000.\\n[2] 李航. 统计学习方法. 清华大学出版社, 2012.\\n[3] Zhi-Hua Zhou and Ji Feng. Deep forest: Towards an alternative to deep neural networks. In IJCAI,\\npages 3553–3559, 2017.\\n[4] 朱德通孙文瑜, 徐成贤. 最优化方法. 最优化方法, 2010.\\n[5] Jerome H Friedman. Greedy function approximation: a gradient boosting machine. Annals of statistics,\\npages 1189–1232, 2001.', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 109, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='pages 1189–1232, 2001.\\n[6] Tianqi Chen and Carlos Guestrin. Xgboost: A scalable tree boosting system. In Proceedings of the\\n22nd acm sigkdd international conference on knowledge discovery and data mining, pages 785–794,\\n2016.\\n→_→\\n配套视频教程：https://www.bilibili.com/video/BV1Mh411e7VU\\n←_←', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 109, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='→_→\\n欢迎去各大电商平台选购纸质版南瓜书《机器学习公式详解第2 版》\\n←_←\\n第9 章\\n聚类\\n到目前为止，\\n前面章节介绍的方法都是针对监督学习(supervised learning) 的，\\n本章介绍的聚类(clus-\\ntering) 和下一章介绍的降维属于无监督学习(unsupervised learning)。\\n9.1\\n聚类任务\\n单词“cluster”既是动词也是名词，作为名词时翻译为“簇”\\n，即聚类得到的子集；一般谈到“聚类”\\n这个概念时对应其动名词形式“clustering”\\n。\\n9.2\\n性能度量\\n本节给出了聚类性能度量的三种外部指标和两种内部指标，其中式(9.5) ~ 式(9.7) 是基于式(9.1) ~\\n式(9.4) 导出的三种外部指标，而式(9.12) 和式(9.13) 是基于式(9.8) ~ 式(9.11) 导出的两种内部指标。\\n读本节内容需要心里清楚的一点：本节给出的指标仅是该领域的前辈们定义的指标，在个人研究过程中可\\n以根据需要自己定义，说不定就会被业内同行广泛使用。\\n9.2.1\\n式(9.5) 的解释\\n给定两个集合A 和B，则Jaccard 系数定义为如下公式', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 110, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='9.2.1\\n式(9.5) 的解释\\n给定两个集合A 和B，则Jaccard 系数定义为如下公式\\nJC = |A T B|\\n|A S B| =\\n|A T B|\\n|A| + |B| −|A T B|\\nJaccard 系数可以用来描述两个集合的相似程度。推论：假设全集U 共有n 个元素，且A ⊆U，B ⊆U，\\n则每一个元素的位置共有四种情况：\\n1. 元素同时在集合A 和B 中，这样的元素个数记为M11；\\n2. 元素出现在集合A 中，但没有出现在集合B 中，这样的元素个数记为M10；\\n3. 元素没有出现在集合A 中，但出现在集合B 中，这样的元素个数记为M01；\\n4. 元素既没有出现在集合A 中，也没有出现在集合B 中，这样的元素个数记为M00。\\n根据Jaccard 系数的定义，此时的Jaccard 系数为如下公式\\nJC =\\nM11\\nM11 + M10 + M01\\n由于聚类属于无监督学习，事先并不知道聚类后样本所属类别的类别标记所代表的意义，即便参考模型的\\n类别标记意义是已知的，我们也无法知道聚类后的类别标记与参考模型的类别标记是如何对应的，况且聚', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 110, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='类别标记意义是已知的，我们也无法知道聚类后的类别标记与参考模型的类别标记是如何对应的，况且聚\\n类后的类别总数与参考模型的类别总数还可能不一样，因此只用单个样本无法衡量聚类性能的好坏。\\n由于外部指标的基本思想就是以参考模型的类别划分为参照，因此如果某一个样本对中的两个样本在\\n聚类结果中同属于一个类，\\n在参考模型中也同属于一个类，\\n或者这两个样本在聚类结果中不同属于一个类，\\n在参考模型中也不同属于一个类，那么对于这两个样本来说这是一个好的聚类结果。\\n总的来说所有样本对中的两个样本共存在四种情况：\\n1. 样本对中的两个样本在聚类结果中属于同一个类，在参考模型中也属于同一个类；\\n2. 样本对中的两个样本在聚类结果中属于同一个类，在参考模型中不属于同一个类；\\n3. 样本对中的两个样本在聚类结果中不属于同一个类，在参考模型中属于同一个类；\\n4. 样本对中的两个样本在聚类结果中不属于同一个类，在参考模型中也不属于同一个类。\\n→_→\\n配套视频教程：https://www.bilibili.com/video/BV1Mh411e7VU\\n←_←', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 110, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='→_→\\n欢迎去各大电商平台选购纸质版南瓜书《机器学习公式详解第2 版》\\n←_←\\n综上所述，即所有样本对存在着书中式(9.1) ~ 式(9.4) 的四种情况，现在假设集合A 中存放着两个样本\\n都同属于聚类结果的同一个类的样本对，即A = SS S SD，集合B 中存放着两个样本都同属于参考模型\\n的同一个类的样本对，即B = SS S DS，那么根据Jaccard 系数的定义有：\\nJC = |A T B|\\n|A S B| =\\n|SS|\\n|SS S SD S DS| =\\na\\na + b + c\\n也可直接将书中式(9.1) ~ 式(9.4) 的四种情况类比推论，即M11 = a，M10 = b，M01 = c，所以\\nJC =\\nM11\\nM11 + M10 + M01\\n=\\na\\na + b + c\\n9.2.2\\n式(9.6) 的解释\\n其中\\na\\na+b 和\\na\\na+c 为Wallace 提出的两个非对称指标，\\na 代表两个样本在聚类结果和参考模型中均属于\\n同一类的样本对的个数，a + b 代表两个样本在聚类结果中属于同一类的样本对的个数，a + c 代表两个样', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 111, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='本在参考模型中属于同一类的样本对的个数，这两个非对称指标均可理解为样本对中的两个样本在聚类结\\n果和参考模型中均属于同一类的概率。由于指标的非对称性，这两个概率值往往不一样，因此Fowlkes 和\\nMallows 提出利用几何平均数将这两个非对称指标转化为一个对称指标，即Fowlkes and Mallows Index,\\nFMI。\\n9.2.3\\n式(9.7) 的解释\\nRand Index 定义如下：\\nRI =\\na + d\\na + b + c + d =\\na + d\\nm(m −1)/2 = 2(a + d)\\nm(m −1)\\n由第一个等号可知RI 肯定不大于1。之所以a + b + c + d = m(m −1)/2, 这是因为式(9.1) ~ 式(9.4) 遍\\n历了所有(xi, xj) 组合对(i ̸= j) : 其中i = 1 时j 可以取2 到m 共m −1 个值, i = 2 时j 可以取3\\n到m 共m −2 个值, · · · · · · , i = m −1 时j 仅可以取m 共1 个值, 因此(xi, xj) 组合对的个数为从1 到', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 111, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='m −1 求和, 根据等差数列求和公式即得m(m −1)/2 。\\n这个指标可以理解为两个样本都属于聚类结果和参考模型中的同一类的样本对的个数与两个样本都\\n分别不属于聚类结果和参考模型中的同一类的样本对的个数的总和在所有样本对中出现的频率，可以简单\\n理解为聚类结果与参考模型的一致性。\\n9.2.4\\n式(9.8) 的解释\\n簇内距离的定义式：求和号左边是(xi, xj) 组合个数的倒数，求和号右边是这些组合的距离和，所以\\n两者相乘定义为平均距离。\\n9.2.5\\n式(9.12) 的解释\\n式中, k 表示聚类结果中簇的个数。该式的DBI 值越小越好, 因为我们希望“物以类聚”, 即同一簇的\\n样本尽可能彼此相似, avg (Ci) 和avg (Cj) 越小越好; 我们希望不同簇的样本尽可能不同, 即dcen (Ci, Cj)\\n越大越好。勘误: 第25 次印刷将分母dcen\\n\\x00µi, µj\\n\\x01\\n改为dcen (Ci, Cj)\\n9.3\\n距离计算\\n距离计算在各种算法中都很常见，本节介绍的距离计算方式和“西瓜书”10.6 节介绍的马氏距离基本', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 111, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='距离计算在各种算法中都很常见，本节介绍的距离计算方式和“西瓜书”10.6 节介绍的马氏距离基本\\n囊括了一般的距离计算方法。另外可能还会碰到“西瓜书”10.5 节的测地线距离。\\n本节有很多概念和名词很常见，比如本节开篇介绍的距离度量的四个基本性质、闵可夫斯基距离、欧\\n氏距离、曼哈顿距离、切比雪夫距离、数值属性、离散属性、有序属性、无序属性、非度量距离等，注意\\n对应的中文和英文。\\n→_→\\n配套视频教程：https://www.bilibili.com/video/BV1Mh411e7VU\\n←_←', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 111, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='→_→\\n欢迎去各大电商平台选购纸质版南瓜书《机器学习公式详解第2 版》\\n←_←\\n9.3.1\\n式(9.21) 的解释\\n该式符号较为抽象, 下面计算“西瓜书”第76 页表4.1 西瓜数据集2.0 属性根蒂上“蜷缩”和“稍蜷”\\n两个离散值之间的距离。\\n此时, u 为“根蒂”, a 为属性根蒂上取值为“蜷缩”, b 为属性根蒂上取值为“稍蜷”, 根据边注, 此\\n时样本类别已知(好瓜/坏瓜), 因此k = 2 。\\n从“西瓜书”表4.1 中可知, 根蒂为蜷缩的样本共有8 个(编号1 ∼5、编号12、编号16 ∼17), 即\\nmu,a = 8, 根蒂为稍蜷的样本共有7 个(编号6 ∼9 和编号13 ∼15), 即mu,b = 7; 设i = 1 对应好瓜,\\ni = 2 对应坏瓜, 好瓜中根蒂为蜷缩的样本共有5 个（编号1 ∼5 ), 即mu,a,1 = 5, 好瓜中根蒂为稍蜷的样\\n本共有3 个(编号6 8), 即mu,b,1 = 3, 坏瓜中根蒂为蜷缩的样本共有3 个(编号12 和编号16 ∼17 ), 即\\nmu,a,2 = 3, 坏瓜中根蒂为稍蜷的样本共有4 个', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 112, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='mu,a,2 = 3, 坏瓜中根蒂为稍蜷的样本共有4 个\\n（编号9 和编号13 ∼15), 即mu,b,2 = 4, 因此VDM 距离为\\nVDMp(a, b) =\\n\\x0c\\n\\x0c\\n\\x0c\\n\\x0c\\nmu,a,1\\nmu,a\\n−mu,b,1\\nmu,b\\n\\x0c\\n\\x0c\\n\\x0c\\n\\x0c\\np\\n+\\n\\x0c\\n\\x0c\\n\\x0c\\n\\x0c\\nmu,a,2\\nmu,a\\n−mu,b,2\\nmu,b\\n\\x0c\\n\\x0c\\n\\x0c\\n\\x0c\\np\\n=\\n\\x0c\\n\\x0c\\n\\x0c\\n\\x0c\\n5\\n8 −3\\n7\\n\\x0c\\n\\x0c\\n\\x0c\\n\\x0c\\np\\n+\\n\\x0c\\n\\x0c\\n\\x0c\\n\\x0c\\n3\\n8 −4\\n7\\n\\x0c\\n\\x0c\\n\\x0c\\n\\x0c\\np\\n9.4\\n原型聚类\\n本节介绍了三个原型聚类算法, 其中k 均值算法最为经典, 几乎成为聚类的代名词, 在Matlab、scikit-\\nlearn 等主流的科学计算包中均有kmeans 函数供调用。学习向量量化也是无监督聚类的一种方式, 在向量\\n检索的引擎，比如facebook faiss 中发挥重要的应用。\\n前两个聚类算法比较易懂, 下面主要推导第三个聚类算法：高斯混合聚类。\\n9.4.1\\n式(9.28) 的解释\\n该式就是多元高斯分布概率密度函数的定义式:\\np(x) =\\n1\\n(2π)\\nn\\n2 |Σ|\\n1\\n2 e−1', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 112, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='p(x) =\\n1\\n(2π)\\nn\\n2 |Σ|\\n1\\n2 e−1\\n2 (x−µ)⊤Σ−1(x−µ)\\n对应到我们常见的一元高斯分布概率密度函数的定义式:\\np(x) =\\n1\\n√\\n2πσ e−(x−µ)2\\n2σ2\\n其中\\n√\\n2π = (2π)\\n1\\n2 对应(2π)\\nn\\n2 , σ 对应|Σ|\\n1\\n2 , 指数项中分母中的方差σ2 对应协方差矩阵Σ, (x−µ)2\\nσ2\\n对应(x−\\nµ)⊤Σ−1(x −µ)。\\n概率密度函数p(x) 是x 的函数。其中对于某个特定的x 来说, 函数值p(x) 就是一个数, 若x 的\\n维度为2 , 则可以将函数p(x) 的图像可视化, 是三维空间的一个曲面。类似于一元高斯分布p(x) 与横\\n轴p(x) = 0 之间的面积等于1 (即\\nR\\np(x)dx = 1 )，p(x) 曲面与平面p(x) = 0 之间的体积等于1 （即\\nR\\np(x)dx = 1 。\\n注意, “西瓜书”中后面将p(x) 记为p(x | µ, Σ) 。\\n9.4.2\\n式(9.29) 的解释\\n对于该式表达的高斯混合分布概率密度函数pM(x), 与式(9.28) 中的p(x) 不同的是, 它由k 个不同', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 112, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='的多元高斯分布加权而来。具体来说, p(x) 仅由参数µ, Σ 确定, 而pM(x) 由k 个“混合系数”αi 以及k\\n组参数µi, Σi 确定。\\n在\\n“西瓜书”\\n中该式下方(P207 最后一段) 中介绍了样本的生成过程, 实际也反应了\\n“混合系数”\\nαi 的\\n含义, 即αi 为选择第i 个混合成分的概率, 或者反过来说, αi 为样本属于第i 个混合成分的概率。\\n重新描述\\n一下样本生成过程, 根据先验分布α1, α2, . . . , αk 选择其中一个高斯混合成分(即第i 个高斯混合成分被选\\n→_→\\n配套视频教程：https://www.bilibili.com/video/BV1Mh411e7VU\\n←_←', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 112, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='→_→\\n欢迎去各大电商平台选购纸质版南瓜书《机器学习公式详解第2 版》\\n←_←\\n到的概率为αi ), 假设选到了第i 个高斯混合成分, 其参数为µi, Σi; 然后根据概率密度函数p (x | µi, Σi)\\n(即将式(9.28) 中的µ, Σ 替换为µi, Σi) 进行采样生成样本x 。两个步骤的区别在于第1 步选择高斯混\\n合成分时是从k 个之中选其一(相当于概率密度函数是离散的), 而第2 步生成样本时是从x 定义域中根\\n据p (x | µi, Σi ) 选择其中一个样本, 样本x 被选中的概率即为p (x | µi, Σi) 。即第1 步对应于离散型随\\n机变量, 第2 步对应于连续型随机变量。\\n9.4.3\\n式(9.30) 的解释\\n若由上述样本生成方式得到训练集D = {x1, x2, . . . , xm}, 现在的问题是对于给定样本xj, 它是由哪\\n个高斯混合成分生成的呢? 该问题即求后验概率pM (zj | xj), 其中zj ∈{1, 2, . . . , k} 。下面对式(9.30) 进\\n行推导。', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 113, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='行推导。\\n对于任意样本, 在不考虑样本本身之前(即先验), 若瞎猜一下它由第i 个高斯混合成分生成的概率\\nP (zj = i), 那么肯定按先验概率α1, α2, . . . , αk 进行猜测, 即P (zj = i) = αi 。若考虑样本本身带来的信\\n息(即后验), 此时再猜一下它由第i 个高斯混合成分生成的概率pM (zj = i | xj), 根据贝叶斯公式, 后验概\\n率pM (zj = i | xj) 可写为\\npM (zj = i | xj) = P (zj = i) · pM (xj | zj = i)\\npM (xj)\\n分子第1 项P (zj = i) = αi; 第2 项即第i 个高斯混合成分生成样本xj 的概率p (xj | µi, Σi), 根据式\\n(9.28) 将x, µ, Σ 替换为xj, µi, Σi 即得; 分母pM (xj) 即为将xj 代入式(9.29) 即得。\\n注意, “西瓜书”中后面将pM (zj = i | xj) 记为γji, 其中1 ≤j ≤m, 1 ≤i ≤k。\\n9.4.4\\n式(9.31) 的解释', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 113, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='9.4.4\\n式(9.31) 的解释\\n若将所有γji 组成一个矩阵Γ, 其中γji 为第j 行第例的元素, 矩阵Γ 大小为m × k, 即\\nΓ =\\n\\uf8ee\\n\\uf8ef\\n\\uf8ef\\n\\uf8ef\\n\\uf8ef\\n\\uf8ef\\n\\uf8f0\\nγ11\\nγ12\\n· · ·\\nγ1k\\nγ21\\nγ22\\n· · ·\\nγ2k\\n.\\n.\\n.\\n.\\n.\\n.\\n...\\n.\\n.\\n.\\nγm1\\nγm2\\n· · ·\\nγmk\\n\\uf8f9\\n\\uf8fa\\n\\uf8fa\\n\\uf8fa\\n\\uf8fa\\n\\uf8fa\\n\\uf8fb\\nm×k\\n其中m 为训练集样本个数, k 为高斯混合模型包含的混合模型个数。可以看出, 式(9.31) 就是找出矩阵Γ\\n第j 行的所有k 个元素中最大的那个元素的位置。\\n9.4.5\\n式(9.32) 的解释\\n对于训练集D = {x1, x2, . . . , xm}, 现在要把m 个样本划分为k 个簇, 即认为训练集D 的样本是根\\n据k 个不同的多元高斯分布加权而得的高斯混合模型生成的。\\n现在的问题是, k 个不同的多元高斯分布的参数{(µi, Σi) | 1 ⩽i ⩽k} 及它们各自的权重α1, α2, . . . , αk\\n不知道, m 个样本归到底属于哪个簇也不知道, 该怎么办呢?', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 113, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='不知道, m 个样本归到底属于哪个簇也不知道, 该怎么办呢?\\n其实这跟k 均值算法类似, 开始时既不知道k 个簇的均值向量, 也不知道m 个样本归到底属于哪个\\n簇, 最后我们采用了贪心策略, 通过迭代优化来近似求解式(9.24)。\\n本节的高斯混合聚类求解方法与k 均值算法, 只是具体问题具体解法不同, 从整体上来说, 它们都应用\\n了7.6 节的期望最大化算法(EM 算法)。\\n具体来说, 现假设已知式(9.30) 的后验概率, 此时即可通过式(9.31) 知道m 个样本归到底属于哪个簇,\\n再来求解参数{(αi, µi, Σi) | 1 ⩽i ⩽k}, 怎么求解呢? 对于每个样本xj 来说, 它出现的概率是pM (xj), 既\\n然现在训练集D 中确实出现了xj, 我们当然希望待求解的参数{(αi, µi, Σi) | 1 ⩽i ⩽k} 能够使这种可能\\n性pM (xj) 最大; 又因为我们假设m 个样本是独立的, 因此它们恰好一起出现的概率就是Qm\\nj=1 pM (xj),\\n→_→\\n配套视频教程：https://www.bilibili.com/video/BV1Mh411e7VU\\n←_←', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 113, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='←_←', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 113, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='→_→\\n欢迎去各大电商平台选购纸质版南瓜书《机器学习公式详解第2 版》\\n←_←\\n即所谓的似然函数; 一般来说, 连乘容易造成下溢( m 个大于0 小于1 的数相乘, 当m 较大时, 乘积会非\\n常非常小, 以致于计算机无法表达这么小的数, 产生下溢), 所以常用对数似然替代, 即式(9.32)。\\n9.4.6\\n式(9.33) 的推导\\n根据公式(9.28) 可知：\\np (xj|µi, Σi) =\\n1\\n(2π)\\nn\\n2 |Σi|\\n1\\n2 exp\\n\\x12\\n−1\\n2 (xj −µi)T Σ−1\\ni\\n(xj −µi)\\n\\x13\\n又根据公式(9.32)，由\\n∂LL(D)\\n∂µi\\n=\\n∂LL(D)\\n∂p (xj|µi, Σi) · ∂p (xj|µi, Σi)\\n∂µi\\n= 0\\n其中：\\n∂LL(D)\\n∂p (xj|µi, \\uffffi) =\\n∂Pm\\nj=1 ln\\n\\x10Pk\\nl=1 αl · p (xj|µl, Σl)\\n\\x11\\n∂p (xj|µi, Σi)\\n=\\nm\\nX\\nj=1\\n∂ln\\n\\x10Pk\\nl=1 αl · p (xj|µl, Σl)\\n\\x11\\n∂p (xj|µi, Σi)\\n=\\nm\\nX\\nj=1\\nαi\\nPk', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 114, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='\\x11\\n∂p (xj|µi, Σi)\\n=\\nm\\nX\\nj=1\\nαi\\nPk\\nl=1 αl · p (xj|µl, Σl)\\n∂p (xj|µi, Σi)\\n∂µi\\n=\\n∂\\n1\\n(2π)\\nn\\n2 |Σi|\\n1\\n2 exp\\n\\x10\\n−1\\n2 (xj −µi)⊤Σ−1\\ni\\n(xj −µi)\\n\\x11\\n∂µi\\n=\\n1\\n(2π)\\nn\\n2 |Σi|\\n1\\n2 ·\\n∂exp\\n\\x10\\n−1\\n2 (xj −µi)⊤Σ−1\\ni\\n(xj −µi)\\n\\x11\\n∂µi\\n=\\n1\\n(2π)\\nn\\n2 |Σi|\\n1\\n2 · exp\\n\\x12\\n−1\\n2 (xj −µi)⊤Σ−1\\ni\\n(xj −µi)\\n\\x13\\n· −1\\n2\\n∂(xj −µi)⊤Σ−1\\ni\\n(xj −µi)\\n∂µi\\n=\\n1\\n(2π)\\nn\\n2 |Σi|\\n1\\n2 · exp\\n\\x12\\n−1\\n2 (xj −µi)⊤Σ−1\\ni\\n(xj −µi)\\n\\x13\\n· Σ−1\\ni\\n(xj −µi)\\n= p (xj|µi, Σi) · Σ−1\\ni\\n(xj −µi)\\n其中，由矩阵求导的法则∂aT Xa\\n∂a\\n= 2Xa 可得：\\n−1\\n2\\n∂(xj −µi)⊤Σ−1\\ni\\n(xj −µi)\\n∂µi\\n= −1\\n2 · 2Σ−1\\ni', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 114, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='−1\\n2\\n∂(xj −µi)⊤Σ−1\\ni\\n(xj −µi)\\n∂µi\\n= −1\\n2 · 2Σ−1\\ni\\n(µi −xj)\\n= Σ−1\\ni\\n(xj −µi)\\n因此有：\\n∂LL(D)\\n∂µi\\n=\\nm\\nX\\nj=1\\nαi\\nPk\\nl=1 αl · p (xj|µl, \\uffffl)\\n· p (xj|µi, Σi) · Σ−1\\ni\\n(xj −µi) = 0\\n9.4.7\\n式(9.34) 的推导\\n由式(9.30)\\nγji = pM (zj = i|Xj) =\\nαi · p (Xj|µi, Σi)\\nPk\\nl=1 αl · p (Xj|µl, Σl)\\n带入式(9.33)\\nm\\nX\\nj=1\\nγji (Xj −µi) = 0\\n→_→\\n配套视频教程：https://www.bilibili.com/video/BV1Mh411e7VU\\n←_←', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 114, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='→_→\\n欢迎去各大电商平台选购纸质版南瓜书《机器学习公式详解第2 版》\\n←_←\\n移项, 得\\nm\\nX\\nj=1\\nγjixj =\\nm\\nX\\nj=1\\nγjiµi = µi ·\\nm\\nX\\nj=1\\nγji\\n第二个等号是因为µi 对于求和变量j 来说是常量, 因此可以提到求和号外面; 因此\\nµi =\\nPm\\nj=1 γjixj\\nPm\\nj=1 γji\\n9.4.8\\n式(9.35) 的推导\\n根据公式(9.28) 可知：\\np(xj|µi, Σi) =\\n1\\n(2π)\\nn\\n2 |Σi|\\n1\\n2 exp\\n\\x12\\n−1\\n2(xj −µi)T Σ−1\\ni (xj −µi)\\n\\x13\\n又根据公式(9.32)，由\\n∂LL(D)\\n∂Σi\\n= 0\\n可得\\n∂LL(D)\\n∂Σi\\n=\\n∂\\n∂Σi\\n\" m\\nX\\nj=1\\nln\\n \\nk\\nX\\ni=1\\nαi · p(xj|µi, Σi)\\n!#\\n=\\nm\\nX\\nj=1\\n∂\\n∂Σi\\n\"\\nln\\n \\nk\\nX\\ni=1\\nαi · p(xj|µi, Σi)\\n!#\\n=\\nm\\nX\\nj=1\\nαi ·\\n∂\\n∂Σi\\n(p(xj|µi, Σi))\\nPk\\nl=1 αl · p(xj|µl, Σl)\\n其中\\n∂\\n∂Σi', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 115, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='(p(xj|µi, Σi))\\nPk\\nl=1 αl · p(xj|µl, Σl)\\n其中\\n∂\\n∂Σi\\n(p(xj|µi, Σi)) =\\n∂\\n∂Σi\\n\\uf8ee\\n\\uf8f0\\n1\\n(2π)\\nn\\n2 |Σi|\\n1\\n2 exp\\n\\x12\\n−1\\n2(xj −µi)T Σ−1\\ni (xj −µi)\\n\\x13\\uf8f9\\n\\uf8fb\\n=\\n∂\\n∂Σi\\n\\uf8f1\\n\\uf8f2\\n\\uf8f3exp\\n\\uf8ee\\n\\uf8f0ln\\n\\uf8eb\\n\\uf8ed\\n1\\n(2π)\\nn\\n2 |Σi|\\n1\\n2 exp\\n\\x12\\n−1\\n2(xj −µi)T Σ−1\\ni (xj −µi)\\n\\x13\\uf8f6\\n\\uf8f8\\n\\uf8f9\\n\\uf8fb\\n\\uf8fc\\n\\uf8fd\\n\\uf8fe\\n= p(xj|µi, Σi) ·\\n∂\\n∂Σi\\n\\uf8ee\\n\\uf8f0ln\\n\\uf8eb\\n\\uf8ed\\n1\\n(2π)\\nn\\n2 |Σi|\\n1\\n2 exp\\n\\x12\\n−1\\n2(xj −µi)T Σ−1\\ni (xj −µi)\\n\\x13\\uf8f6\\n\\uf8f8\\n\\uf8f9\\n\\uf8fb\\n= p(xj|µi, Σi) ·\\n∂\\n∂Σi\\n\\uf8ee\\n\\uf8f0ln\\n1\\n(2π)\\nn\\n2 −\\n1\\n2 ln |Σi| −1\\n2(xj −µi)T Σ−1\\ni (xj −µi)\\n\\uf8f9\\n\\uf8fb\\n= p(xj|µi, Σi) ·\\n\\uf8ee\\n\\uf8f0−\\n1\\n2\\n∂(ln |Σi|)\\n∂Σi\\n−\\n1\\n2\\n∂\\n\\x02\\n(xj −µi)T Σ−1\\ni (xj −µi)\\n\\x03\\n∂Σi', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 115, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='∂Σi\\n−\\n1\\n2\\n∂\\n\\x02\\n(xj −µi)T Σ−1\\ni (xj −µi)\\n\\x03\\n∂Σi\\n\\uf8f9\\n\\uf8fb\\n由矩阵微分公式\\n∂|X|\\n∂X = |X| · (X−1)T ,\\n∂aT X−1B\\n∂X\\n= −X−T abT X−T 可得\\n∂\\n∂Σi\\n(p(xj|µi, Σi)) = p(xj|µi, Σi) ·\\n\\uf8ee\\n\\uf8f0−\\n1\\n2Σ−1\\ni\\n+\\n1\\n2Σ−1\\ni (xj −µi)(xj −µi)T Σ−1\\ni\\n\\uf8f9\\n\\uf8fb\\n→_→\\n配套视频教程：https://www.bilibili.com/video/BV1Mh411e7VU\\n←_←', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 115, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='→_→\\n欢迎去各大电商平台选购纸质版南瓜书《机器学习公式详解第2 版》\\n←_←\\n将此式代回\\n∂LL(D)\\n∂Σi\\n中可得\\n∂LL(D)\\n∂Σi\\n=\\nm\\nX\\nj=1\\nαi · p(xj|µi, Σi)\\nPk\\nl=1 αl · p(xj|µl, Σl)\\n·\\n\\uf8ee\\n\\uf8f0−\\n1\\n2Σ−1\\ni\\n+\\n1\\n2Σ−1\\ni (xj −µi)(xj −µi)T Σ−1\\ni\\n\\uf8f9\\n\\uf8fb\\n又由公式(9.30) 可知\\nαi · p(xj|µi, Σi)\\nPk\\nl=1 αl · p(xj|µl, Σl)\\n= γji，所以上式可进一步化简为\\n∂LL(D)\\n∂Σi\\n=\\nm\\nX\\nj=1\\nγji ·\\n\\uf8ee\\n\\uf8f0−\\n1\\n2Σ−1\\ni\\n+\\n1\\n2Σ−1\\ni (xj −µi)(xj −µi)T Σ−1\\ni\\n\\uf8f9\\n\\uf8fb\\n令上式等于0 可得\\n∂LL(D)\\n∂Σi\\n=\\nm\\nX\\nj=1\\nγji ·\\n\\uf8ee\\n\\uf8f0−\\n1\\n2Σ−1\\ni\\n+\\n1\\n2Σ−1\\ni (xj −µi)(xj −µi)T Σ−1\\ni\\n\\uf8f9\\n\\uf8fb= 0\\n移项推导有：\\nm\\nX\\nj=1\\nγji ·\\n\\x02\\n−I + (xj −µi)(xj −µi)T Σ−1\\ni\\n\\x03\\n= 0\\nm\\nX\\nj=1', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 116, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='\\x02\\n−I + (xj −µi)(xj −µi)T Σ−1\\ni\\n\\x03\\n= 0\\nm\\nX\\nj=1\\nγji(xj −µi)(xj −µi)T Σ−1\\ni\\n=\\nm\\nX\\nj=1\\nγjiI\\nm\\nX\\nj=1\\nγji(xj −µi)(xj −µi)T =\\nm\\nX\\nj=1\\nγjiΣi\\nΣ−1\\ni\\n·\\nm\\nX\\nj=1\\nγji(xj −µi)(xj −µi)T =\\nm\\nX\\nj=1\\nγji\\nΣi =\\nPm\\nj=1 γji(xj −µi)(xj −µi)T\\nPm\\nj=1 γji\\n此即为公式(9.35)。\\n9.4.9\\n式(9.36) 的解释\\n该式即LL(D) 添加了等式约束Pk\\ni=1 αi = 1 的拉格朗日形式。\\n9.4.10\\n式(9.37) 的推导\\n重写式(9.32) 如下:\\nLL(D) =\\nm\\nX\\nj=1\\nln\\n k\\nX\\nl=1\\nαl · p (xj | µl, Σl)\\n!\\n这里将第2 个求和号的求和变量由式(9.32) 的i 改为了l, 这是为了避免对αi 求导时与变量i 相混淆。将\\n式(9.36) 中的两项分别对αi 求导, 得\\n∂LL(D)\\n∂αi\\n=\\n∂Pm\\nj=1 ln\\n\\x10Pk', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 116, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='∂LL(D)\\n∂αi\\n=\\n∂Pm\\nj=1 ln\\n\\x10Pk\\nl=1 αl · p (xj | µl, Σl)\\n\\x11\\n∂αi\\n=\\nm\\nX\\nj=1\\n1\\nPk\\nl=1 αl · p (xj | µl, Σl)\\n· ∂Pk\\nl=1 αl · p (xj | µl, Σl)\\n∂αi\\n=\\nm\\nX\\nj=1\\n1\\nPk\\nl=1 αl · p (xj | µl, Σl)\\n· p (xj | µi, Σi)\\n→_→\\n配套视频教程：https://www.bilibili.com/video/BV1Mh411e7VU\\n←_←', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 116, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='→_→\\n欢迎去各大电商平台选购纸质版南瓜书《机器学习公式详解第2 版》\\n←_←\\n∂\\n\\x10Pk\\nl=1 αl −1\\n\\x11\\n∂αi\\n= ∂(α1 + α2 + . . . + αi + . . . + αk −1)\\n∂αi\\n= 1\\n综合两项求导结果, 并令导数等于零即得式(9.37)。\\n9.4.11\\n式(9.38) 的推导\\n注意, 在“西瓜书”第14 次印刷中式(9.38) 上方的一行话进行了勘误: “两边同乘以αi, 对所有混合\\n成分求和可知λ = −m ”, 将原来的“样本”修改为“混合成分”\\n。\\n对公式(9.37) 两边同时乘以αi 可得\\nm\\nX\\nj=1\\nαi · p(xj|µi, Σi)\\nPk\\nl=1 αl · p(xj|µl, Σl)\\n+ λαi = 0\\nm\\nX\\nj=1\\nαi · p(xj|µi, Σi)\\nPk\\nl=1 αl · p(xj|µl, Σl)\\n= −λαi\\n两边对所有混合成分求和可得\\nk\\nX\\ni=1\\nm\\nX\\nj=1\\nαi · p(xj|µi, Σi)\\nPk\\nl=1 αl · p(xj|µl, Σl)\\n= −λ\\nk\\nX\\ni=1\\nαi\\nm\\nX\\nj=1\\nk\\nX\\ni=1', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 117, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='= −λ\\nk\\nX\\ni=1\\nαi\\nm\\nX\\nj=1\\nk\\nX\\ni=1\\nαi · p(xj|µi, Σi)\\nPk\\nl=1 αl · p(xj|µl, Σl)\\n= −λ\\nk\\nX\\ni=1\\nαi\\n因为\\nk\\nX\\ni=1\\nαi · p(xj|µi, \\uffffi)\\nPk\\nl=1 αl · p(xj|µl, \\uffffl)\\n=\\nPk\\ni=1 αi · p(xj|µi, \\uffffi)\\nPk\\nl=1 αl · p(xj|µl, \\uffffl)\\n= 1\\n且Pk\\ni=1 αi = 1，所以有m = −λ，因此\\nm\\nX\\nj=1\\nαi · p(xj|µi, Σi)\\nPk\\nl=1 αl · p(xj|µl, Σl)\\n= −λαi = mαi\\n因此\\nαi =\\n1\\nm\\nm\\nX\\nj=1\\nαi · p(xj|µi, Σi)\\nPk\\nl=1 αl · p(xj|µl, Σl)\\n又由公式(9.30) 可知\\nαi · p(xj|µi, Σi)\\nPk\\nl=1 αl · p(xj|µl, Σl)\\n= γji，所以上式可进一步化简为\\nαi =\\n1\\nm\\nm\\nX\\nj=1\\nγji\\n此即为公式(9.38)。\\n9.4.12\\n图9.6 的解释', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 117, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='αi =\\n1\\nm\\nm\\nX\\nj=1\\nγji\\n此即为公式(9.38)。\\n9.4.12\\n图9.6 的解释\\n第1 行初始化参数, 本页接下来的例子是按如下策略初始化的: 混合系数αi = 1\\nk; 任选训练集中的k\\n个样本分别初始化k 个均值向量µi(1 ⩽i ⩽k); 使用对角元素为0.1 的对角阵初始化k 个协方差矩阵\\nΣi(1 ⩽i ⩽k) 。\\n第3~5 行根据式(9.30) 计算共m × k 个γji 。\\n第6~10 行分别根据式(9.34)、式(9.35)、式(9.38) 使用刚刚计算得到的γji 更新均值向量、协方差\\n矩阵、混合系数; 注意第8 行计算协方差矩阵时使用的是第7 行计算得到的均值向量, 这并没错, 因为协方\\n→_→\\n配套视频教程：https://www.bilibili.com/video/BV1Mh411e7VU\\n←_←', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 117, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='→_→\\n欢迎去各大电商平台选购纸质版南瓜书《机器学习公式详解第2 版》\\n←_←\\n差矩阵Σ′\\ni 与均值向量µ′\\ni 是对应的, 而非µi; 第7 行的µ′\\ni 在第8 行使用之后会在下一轮迭代中第4 行计\\n算γji 再次使用。\\n整体来说, 第2 ~12 行就是一个EM 算法的具体使用例子, 学习完7.6 节EM 算法可能根本无法理解\\n其思想。此例中有两组变量, 分别是γji 和(αi, µi, Σi), 它们之间相互影响, 但都是末知的, 因此EM 算法\\n就有了用武之地: 初始化其中一组变量(αi, µi, Σi), 然后计算γji; 再根据γji 根据最大似然推导出的公式\\n更新(αi, µi, Σi), 反复迭代, 直到满足停止条件。\\n9.5\\n密度聚类\\n本节介绍的DBSCAN 算法并不难懂，只是本节在最后举例时并没有说清楚密度聚类算法与前面原型\\n聚类算法的区别，当然这也可能是作者有意为之，因为在“西瓜书”本章习题9.7 题就提到了“凸聚类”\\n的概念。具体来说，前面介绍的聚类算法只能产生“凸聚类”\\n，而本节介绍的DBSCAN 则能产生“非凸聚\\n类”', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 118, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='，而本节介绍的DBSCAN 则能产生“非凸聚\\n类”\\n，其本质原因，个人感觉在于聚类时使用的距离度量，均值算法使用欧氏距离，而DBSCAN 使用类似\\n于测地线距离（只是类似，并不相同，测地线距离参见“西瓜书”10.5 节）\\n，因此可以产生如图9-1所示的\\n聚类结果（中间为典型的非凸聚类）\\n。\\n图9-1 DBSCAN 聚类结果\\n注意，虽然左图为“凸聚类”\\n（四个簇都有一个凸包）\\n，但均值算法却无法产生此结果，因为最大的簇\\n太大了，其外围样本与另三个小簇的中心之间的距离更近，因此中间最大的簇肯定会被均值算法划分到不\\n同的簇之中，这显然不是我们希望的结果。\\n密度聚类算法可以产生任意形状的簇，不需要事先指定聚类个数k，并且对噪声鲁棒。\\n9.5.1\\n密度直达、密度可达与密度相连\\nxj 由xi 密度直达, 该概念最易理解, 但要特别注意: 密度直达除了要求xj 位于xi 的ϵ−领域的条件\\n之外, 还额外要求xi 是核心对象; ϵ-领域满足对称性, 但xj 不一定为核心对象, 因此密度直达关系通常不\\n满足对称性。', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 118, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='满足对称性。\\nxj 由xi 密度可达，该概念基于密度直达，因此样本序列p1, p2, . . . , pn 中除了pn = xj 之外, 其余样\\n本均为核心对象(当然包括p1 = xi ), 所以同理, 一般不满足对称性。\\n以上两个概念中, 若xj 为核心对象, 已知xj 由xi 密度直达/可达, 则xi 由xj 密度直达/ 可达, 即满\\n足对称性(也就是说, 核心对象之间的密度直达/可达满足对称性)。\\nxi 与xj 密度相连, 不要求xi 与xj 为核心对象, 所以满足对称性。\\n→_→\\n配套视频教程：https://www.bilibili.com/video/BV1Mh411e7VU\\n←_←', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 118, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='→_→\\n欢迎去各大电商平台选购纸质版南瓜书《机器学习公式详解第2 版》\\n←_←\\n9.5.2\\n图9.9 的解释\\n在第1 ∼7 行中, 算法先根据给定的邻域参数(ϵ, MinPts) 找出所有核心对象, 并存于集合Ω之中; 第\\n4 行的if 判断语句即在判别xj 是否为核心对象。\\n在第10 ∼24 行中, 以任一核心对象为出发点(由第12 行实现), 找出其密度可达的样本生成聚类簇\\n(由第14 ∼21 行实现), 直到所有核心对象被访问过为止（由第10 行和第23 行配合实现)。具体来说:\\n其中第14 ∼21 行while 循环中的if 判断语句（第16 行）在第一次循环时一定为真（因为Q 在第\\n12 行初始化为某核心对象), 此时会往队列Q 中加入q 密度直达的样本(已知q 为核心对象, q 的ϵ-领域\\n中的样本即为q 密度直达的), 队列遵循先进先出规则, 接下来的循环将依次判别q 的ϵ-领域中的样本是\\n否为核心对象(第16 行), 若为核心对象, 则将密度直达的样本( ϵ-领域中的样本) 加入Q。根据密度可达', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 119, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='的概念, while 循环中的if 判断语句（第16 行）找出的核心对象之间一定是相互密度可达的, 非核心对象\\n一定是密度相连的。\\n第14 ∼21 行while 循环每跳出一次, 即生成一个聚类簇。每次生成聚类\\x00之前, 会记录当前末访问过\\n样本集合(第11 行Γold = Γ ), 然后当前要生成的聚类簇每决定录取一个样本后会将该样本从厂去除(第\\n13 行和第19 行), 因此第14~21 行while 循环每跳出一次后, Γold 与Γ 差别即为聚类簇的样本成员(第\\n22 行), 并将该聚类簇中的核心对象从第1 ∼7 行生成的核心对象集合Ω中去除。\\n符号“\\\\”为集合求差, 例如集合A = {a, b, c, d, e, f}, B = {a, d, f, g, h}, 则A\\\\B 为A\\\\B = {b, c, e},\\n即将A, B 所有相同元素从A 中去除。\\n9.6\\n层次聚类\\n本节主要介绍了层次聚类的代表算法AGNES。\\n式(9.41) (9.43) 介绍了三种距离计算方式, 这与“西瓜书”9.3 节中介绍的距离不同之处在于, 此三种', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 119, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='距离计算面向集合之间, 而9.3 节的距离则面向两点之间。正如“西瓜书”第215 页左上边注所示, 集合间\\n的距离计算常采用豪斯多夫距离(Hausdorff distance)。\\n算法AGNES 很简单, 就是不断重复执行合并距离最近的两个聚类簇。\\n“西瓜书”图9.11 为具体实现\\n方法, 核心就是在合并两个聚类簇后更新距离矩阵（第11 ∼23 行), 之所以看起来复杂, 是因为该实现只\\n更新原先距离矩阵中发生变化的行和列, 因此需要为此做一些调整。\\n在第1 ∼9 行, 算法先对仅含一个样本的初始聚类簇和相应的距离矩阵进行初始化。注意, 距离矩阵\\n中, 第i 行为聚类簇Ci 到各聚类簇的距离, 第i 列为各聚类簇到聚类簇Ci 的距离, 由第7 行可知, 距离矩\\n阵为对称矩阵, 即使用的集合间的距离计算方法满足对称性。\\n第18 ∼21 行更新距离矩阵M 的第i∗行与第i∗列, 因为此时的聚类簇Ci∗已经合并了Cj∗, 因此与\\n其余聚类簇之间的距离都发生了变化, 需要更新。\\n→_→\\n配套视频教程：https://www.bilibili.com/video/BV1Mh411e7VU', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 119, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='←_←', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 119, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='→_→\\n欢迎去各大电商平台选购纸质版南瓜书《机器学习公式详解第2 版》\\n←_←\\n第10 章\\n降维与度量学习\\n10.1\\n预备知识\\n本章内容需要较多的线性代数和矩阵分析的基础，因此将相关的预备知识整体整理如下。\\n10.1.1\\n符号约定\\n向量元素之间分号“;”表示列元素分隔符, 如α = (a1; a2; . . . ; ai; . . . ; am) 表示m × 1 的列向量; 而逗\\n号“,”表示行元素分隔符, 如α = (a1, a2, . . . , ai, . . . , am) 表示1 × m 的行向量。\\n10.1.2\\n矩阵与单位阵、向量的乘法\\n(1) 矩阵左乘对角阵相当于矩阵每行乘以对应对角阵的对角线元素, 如：\\n\\uf8ee\\n\\uf8ef\\n\\uf8ef\\n\\uf8f0\\nλ1\\nλ2\\nλ3\\n\\uf8f9\\n\\uf8fa\\n\\uf8fa\\n\\uf8fb\\n\\uf8ee\\n\\uf8ef\\n\\uf8ef\\n\\uf8f0\\nx11\\nx12\\nx13\\nx21\\nx22\\nx23\\nx31\\nx32\\nx33\\n\\uf8f9\\n\\uf8fa\\n\\uf8fa\\n\\uf8fb=\\n\\uf8ee\\n\\uf8ef\\n\\uf8ef\\n\\uf8f0\\nλ1x11\\nλ1x12\\nλ1x13\\nλ2x21\\nλ2x22\\nλ2x23\\nλ3x31\\nλ3x32\\nλ3x33\\n\\uf8f9\\n\\uf8fa\\n\\uf8fa\\n\\uf8fb\\n(2) 矩阵右乘对角阵相当于矩阵每列乘以对应对角阵的对角线元素, 如：\\n\\uf8ee', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 120, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='\\uf8f9\\n\\uf8fa\\n\\uf8fa\\n\\uf8fb\\n(2) 矩阵右乘对角阵相当于矩阵每列乘以对应对角阵的对角线元素, 如：\\n\\uf8ee\\n\\uf8ef\\n\\uf8ef\\n\\uf8f0\\nx11\\nx12\\nx13\\nx21\\nx22\\nx23\\nx31\\nx32\\nx33\\n\\uf8f9\\n\\uf8fa\\n\\uf8fa\\n\\uf8fb\\n\\uf8ee\\n\\uf8ef\\n\\uf8ef\\n\\uf8f0\\nλ1\\nλ2\\nλ3\\n\\uf8f9\\n\\uf8fa\\n\\uf8fa\\n\\uf8fb=\\n\\uf8ee\\n\\uf8ef\\n\\uf8ef\\n\\uf8f0\\nλ1x11\\nλ2x12\\nλ3x13\\nλ1x21\\nλ2x22\\nλ3x23\\nλ1x31\\nλ2x32\\nλ3x33\\n\\uf8f9\\n\\uf8fa\\n\\uf8fa\\n\\uf8fb\\n(3) 矩阵左乘行向量相当于矩阵每行乘以对应行向量的元素之和, 如：\\nh\\nλ1\\nλ2\\nλ3\\ni\\n\\uf8ee\\n\\uf8ef\\n\\uf8ef\\n\\uf8f0\\nx11\\nx12\\nx13\\nx21\\nx22\\nx23\\nx31\\nx32\\nx33\\n\\uf8f9\\n\\uf8fa\\n\\uf8fa\\n\\uf8fb\\n= λ1\\nh\\nx11\\nx12\\nx13\\ni\\n+ λ2\\nh\\nx21\\nx22\\nx23\\ni\\n+ λ3\\nh\\nx31\\nx32\\nx33\\ni\\n=\\n\\x10\\nλ1x11 + λ2x21 + λ3x31, λ1x12 + λ2x22 + λ3x32, λ1x13 + λ2x23 + λ3x33\\n\\x11\\n(4) 矩阵右乘列向量相当于矩阵每列乘以对应列向量的元素之和, 如：\\n\\uf8ee\\n\\uf8ef\\n\\uf8ef\\n\\uf8f0\\nx11\\nx12\\nx13\\nx21\\nx22\\nx23\\nx31', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 120, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='\\uf8ee\\n\\uf8ef\\n\\uf8ef\\n\\uf8f0\\nx11\\nx12\\nx13\\nx21\\nx22\\nx23\\nx31\\nx32\\nx33\\n\\uf8f9\\n\\uf8fa\\n\\uf8fa\\n\\uf8fb\\n\\uf8ee\\n\\uf8ef\\n\\uf8ef\\n\\uf8f0\\nλ1\\nλ2\\nλ3\\n\\uf8f9\\n\\uf8fa\\n\\uf8fa\\n\\uf8fb\\n= λ1\\n\\uf8ee\\n\\uf8ef\\n\\uf8ef\\n\\uf8f0\\nx11\\nx21\\nx31\\n\\uf8f9\\n\\uf8fa\\n\\uf8fa\\n\\uf8fb+ λ2\\n\\uf8ee\\n\\uf8ef\\n\\uf8ef\\n\\uf8f0\\nx12\\nx22\\nx32\\n\\uf8f9\\n\\uf8fa\\n\\uf8fa\\n\\uf8fb+ λ3\\n\\uf8ee\\n\\uf8ef\\n\\uf8ef\\n\\uf8f0\\nx13\\nx23\\nx33\\n\\uf8f9\\n\\uf8fa\\n\\uf8fa\\n\\uf8fb=\\n3\\nX\\ni=1\\n\\uf8eb\\n\\uf8ec\\n\\uf8ec\\n\\uf8edλi\\n\\uf8ee\\n\\uf8ef\\n\\uf8ef\\n\\uf8f0\\nx1i\\nx2i\\nx3i\\n\\uf8f9\\n\\uf8fa\\n\\uf8fa\\n\\uf8fb\\n\\uf8f6\\n\\uf8f7\\n\\uf8f7\\n\\uf8f8\\n= (λ1x11 + λ2x12 + λ3x13; λ1x21 + λ2x22 + λ3x23; λ1x31 + λ2x32 + λ3x33)\\n综上, 左乘是对矩阵的行操作, 而右乘则是对矩阵的列操作, 第(2) 个和第(4) 个结论后面推导过程中\\n灵活应用较多。\\n10.2\\n矩阵的F 范数与迹\\n(1) 对于矩阵A ∈Rm×n, 其Frobenius 范数(简称F 范数) ∥A∥F 定义为\\n∥A∥F =\\n m\\nX\\ni=1\\nn\\nX\\nj=1\\n|aij|2\\n! 1\\n2\\n→_→', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 120, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='∥A∥F =\\n m\\nX\\ni=1\\nn\\nX\\nj=1\\n|aij|2\\n! 1\\n2\\n→_→\\n配套视频教程：https://www.bilibili.com/video/BV1Mh411e7VU\\n←_←', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 120, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='→_→\\n欢迎去各大电商平台选购纸质版南瓜书《机器学习公式详解第2 版》\\n←_←\\n其中aij 为矩阵A 第i 行第j 列的元素, 即\\nA =\\n\\uf8ee\\n\\uf8ef\\n\\uf8ef\\n\\uf8ef\\n\\uf8ef\\n\\uf8ef\\n\\uf8ef\\n\\uf8ef\\n\\uf8ef\\n\\uf8ef\\n\\uf8ef\\n\\uf8f0\\na11\\na12\\n· · ·\\na1j\\n· · ·\\na1n\\na21\\na22\\n· · ·\\na2j\\n· · ·\\na2n\\n.\\n.\\n.\\n.\\n.\\n.\\n...\\n.\\n.\\n.\\n...\\n.\\n.\\n.\\nai1\\nai2\\n· · ·\\naij\\n· · ·\\nain\\n.\\n.\\n.\\n.\\n.\\n.\\n...\\n.\\n.\\n.\\n...\\n.\\n.\\n.\\nam1\\nam2\\n· · ·\\namj\\n· · ·\\namn\\n\\uf8f9\\n\\uf8fa\\n\\uf8fa\\n\\uf8fa\\n\\uf8fa\\n\\uf8fa\\n\\uf8fa\\n\\uf8fa\\n\\uf8fa\\n\\uf8fa\\n\\uf8fa\\n\\uf8fb\\n(2) 若A = (α1, α2, . . . , αj, . . . , αn), 其中αj = (a1j; a2j; . . . ; aij; . . . ; amj) 为其列向量, A ∈Rm×n, αj ∈\\nRm×1, 则∥A∥2\\nF = Pn\\nj=1 ∥αj∥2\\n2;', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 121, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='Rm×1, 则∥A∥2\\nF = Pn\\nj=1 ∥αj∥2\\n2;\\n同理, 若A = (β1; β2; . . . ; βi; . . . ; βm), 其中βi = (ai1, ai2, . . . , aij, . . . , ain) 为其行向量, A ∈Rm×n, βi ∈\\nR1×n, 则∥A∥2\\nF = Pm\\ni=1 ∥βi∥2\\n2 。\\n证明：该结论是显而易见的, 因为∥αj∥2\\n2 = Pm\\ni=1 |aij|2, 而∥A∥2\\nF = Pm\\ni=1\\nPn\\nj=1 |aij|2 。\\n(3) 若λj\\n\\x00A⊤A\\n\\x01\\n表示n 阶方阵A⊤A 的第j 个特征值, tr\\n\\x00A⊤A\\n\\x01\\n是A⊤A 的迹（对角线元素之和);\\nλi\\n\\x10\\nAA⊤\\x11\\n表示m 阶方阵AA⊤的第i 个特征值, tr\\n\\x10\\nAA⊤\\x11\\n是AA⊤的迹, 则\\n∥A∥2\\nF = tr\\n\\x00A⊤A\\n\\x01\\n=\\nn\\nX\\nj=1\\nλj\\n\\x00A⊤A\\n\\x01\\n= tr\\n\\x00AA⊤\\x01\\n=\\nm\\nX\\ni=1\\nλi\\n\\x00AA⊤\\x01\\n证明：先证∥A∥2\\nF = tr\\n\\x00A⊤A\\n\\x01', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 121, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='\\x00AA⊤\\x01\\n=\\nm\\nX\\ni=1\\nλi\\n\\x00AA⊤\\x01\\n证明：先证∥A∥2\\nF = tr\\n\\x00A⊤A\\n\\x01\\n，令B = A⊤A ∈Rn×n, bij 表示B 第i 行第j 列元素, tr(B) = Pn\\nj=1 bjȷ\\nB = A⊤A =\\n\\uf8ee\\n\\uf8ef\\n\\uf8ef\\n\\uf8ef\\n\\uf8ef\\n\\uf8ef\\n\\uf8ef\\n\\uf8ef\\n\\uf8ef\\n\\uf8ef\\n\\uf8ef\\n\\uf8f0\\na11\\na21\\n· · ·\\nai1\\n· · ·\\nam1\\na12\\na22\\n· · ·\\nai2\\n· · ·\\nam2\\n.\\n.\\n.\\n.\\n.\\n.\\n...\\n.\\n.\\n.\\n...\\n.\\n.\\n.\\na1j\\na2j\\n· · ·\\naij\\n· · ·\\namj\\n.\\n.\\n.\\n.\\n.\\n.\\n...\\n.\\n.\\n.\\n...\\n.\\n.\\n.\\na1n\\na2n\\n· · ·\\nain\\n· · ·\\namn\\n\\uf8f9\\n\\uf8fa\\n\\uf8fa\\n\\uf8fa\\n\\uf8fa\\n\\uf8fa\\n\\uf8fa\\n\\uf8fa\\n\\uf8fa\\n\\uf8fa\\n\\uf8fa\\n\\uf8fb\\n\\uf8ee\\n\\uf8ef\\n\\uf8ef\\n\\uf8ef\\n\\uf8ef\\n\\uf8ef\\n\\uf8ef\\n\\uf8ef\\n\\uf8ef\\n\\uf8ef\\n\\uf8ef\\n\\uf8f0\\na11\\na12\\n· · ·\\na1j\\n· · ·\\na1n\\na21\\na22\\n· · ·\\na2j\\n· · ·\\na2n\\n.\\n.\\n.\\n.\\n.\\n.\\n...\\n.\\n.\\n.\\n...\\n.\\n.\\n.\\nai1\\nai2\\n· · ·\\naij\\n· · ·\\nain\\n.\\n.\\n.\\n.\\n.\\n.\\n...\\n.\\n.', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 121, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='.\\nai1\\nai2\\n· · ·\\naij\\n· · ·\\nain\\n.\\n.\\n.\\n.\\n.\\n.\\n...\\n.\\n.\\n.\\n...\\n.\\n.\\n.\\nam1\\nam2\\n· · ·\\namj\\n· · ·\\namn\\n\\uf8f9\\n\\uf8fa\\n\\uf8fa\\n\\uf8fa\\n\\uf8fa\\n\\uf8fa\\n\\uf8fa\\n\\uf8fa\\n\\uf8fa\\n\\uf8fa\\n\\uf8fa\\n\\uf8fb\\n由矩阵运算规则, bjj 等于A⊤的第j 行与A 的第j 列的内积, 因此\\ntr(B) =\\nn\\nX\\nj=1\\nbjj =\\nn\\nX\\nj=1\\n m\\nX\\ni=1\\n|aij|2\\n!\\n=\\nm\\nX\\ni=1\\nn\\nX\\nj=1\\n|aij|2 = ∥A∥2\\nF\\n以上第三个等号交换了求和号次序（类似于交换积分号次序), 显然这不影响求和结果。\\n同理, 可证∥A∥2\\nF = tr\\n\\x00AA⊤\\x01\\n：\\nC = AA⊤=\\n\\uf8ee\\n\\uf8ef\\n\\uf8ef\\n\\uf8ef\\n\\uf8ef\\n\\uf8ef\\n\\uf8ef\\n\\uf8ef\\n\\uf8ef\\n\\uf8ef\\n\\uf8ef\\n\\uf8f0\\na11\\na12\\n· · ·\\na1j\\n· · ·\\na1n\\na21\\na22\\n· · ·\\na2j\\n· · ·\\na2n\\n.\\n.\\n.\\n.\\n.\\n.\\n...\\n.\\n.\\n.\\n...\\n.\\n.\\n.\\nai1\\nai2\\n· · ·\\naij\\n· · ·\\nain\\n.\\n.\\n.\\n.\\n.\\n.\\n...\\n.\\n.\\n.\\n...\\n.\\n.\\n.\\nam1\\nam2\\n· · ·\\namj', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 121, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='.\\n.\\n.\\n.\\n.\\n.\\n...\\n.\\n.\\n.\\n...\\n.\\n.\\n.\\nam1\\nam2\\n· · ·\\namj\\n· · ·\\namn\\n\\uf8f9\\n\\uf8fa\\n\\uf8fa\\n\\uf8fa\\n\\uf8fa\\n\\uf8fa\\n\\uf8fa\\n\\uf8fa\\n\\uf8fa\\n\\uf8fa\\n\\uf8fa\\n\\uf8fb\\n\\uf8ee\\n\\uf8ef\\n\\uf8ef\\n\\uf8ef\\n\\uf8ef\\n\\uf8ef\\n\\uf8ef\\n\\uf8ef\\n\\uf8ef\\n\\uf8ef\\n\\uf8ef\\n\\uf8f0\\na11\\na21\\n· · ·\\nai1\\n· · ·\\nam1\\na12\\na22\\n· · ·\\nai2\\n· · ·\\nam2\\n.\\n.\\n.\\n.\\n.\\n.\\n...\\n.\\n.\\n.\\n...\\n.\\n.\\n.\\na1j\\na2j\\n· · ·\\naij\\n· · ·\\namj\\n.\\n.\\n.\\n.\\n.\\n.\\n...\\n.\\n.\\n.\\n...\\n.\\n.\\n.\\na1n\\na2n\\n· · ·\\nain\\n· · ·\\namn\\n\\uf8f9\\n\\uf8fa\\n\\uf8fa\\n\\uf8fa\\n\\uf8fa\\n\\uf8fa\\n\\uf8fa\\n\\uf8fa\\n\\uf8fa\\n\\uf8fa\\n\\uf8fa\\n\\uf8fb\\n→_→\\n配套视频教程：https://www.bilibili.com/video/BV1Mh411e7VU\\n←_←', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 121, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='→_→\\n欢迎去各大电商平台选购纸质版南瓜书《机器学习公式详解第2 版》\\n←_←\\n由矩阵运算规则, cii 等于A 的第i 行与A⊤的第i 列的内积（红色元素), 因此\\ntr(C) =\\nm\\nX\\ni=1\\ncii =\\nm\\nX\\ni=1\\n n\\nX\\nj=1\\n|aij|2\\n!\\n=\\nm\\nX\\ni=1\\nn\\nX\\nj=1\\n|aij|2 = ∥A∥2\\nF\\n有关方阵的特征值之和等于对角线元素之和, 可以参见线性代数教材。\\n10.3\\nk 近邻学习\\n10.3.1\\n式(10.1) 的解释\\nP(err) = 1 −\\nX\\nc∈Y\\nP(c|x)P(c|z)\\n首先, P(c | x) 表示样本x 为类别c 的后验概率, P(c | z) 表示样本z 为类别c 的后验概率; 其次,\\nP(c | x)P(c | z) 表示样本x 和样本z 同时为类别c 的概率;\\n再次, P\\nc∈Y P(c | x)P(c | z) 表示样本x 和样本z 类别相同的概率; 这一点可以进一步解释，设\\nY = {c1, c2, · · · , cN}, 则该求和式子变为：', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 122, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='Y = {c1, c2, · · · , cN}, 则该求和式子变为：\\nP (c1 | x) P (c1 | z) + P (c2 | x) P (c2 | z) + · · · + P (cN | x) P (cN | z)\\n即样本x 和样本z 同时为c1 的概率, 加上同时为c2 的概率, · · · · · · , 加上同时为cN 的概率, 即样本\\nx 和样本z 类别相同的概率;\\n最后, P(err) 表示样本x 和样本z 类别不相同的概率, 即1 减去二者类别相同的概率。\\n10.3.2\\n式(10.2) 的推导\\n式(10.2) 推导关键在于理解第二行的“约等(≃) ”关系和第三行的“小于等于(⩽) ”关系。\\n第二行的“约等(≃) ”关系的依据在于该式前面一段话：\\n“假设样本独立同分布, 且对任意x 和任\\n意小正数δ, 在x 附近δ 距离范围内总能找到一个训练样本”, 这意味着对于任意测试样本在训练集中\\n都可以找出一个与其非常像(任意小正数δ ) 的近邻, 这里还有一个假设书中末提及：P(c | x) 必须是', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 122, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='连续函数(对于连续函数f(x) 和任意小正数δ, f(x) ≃f(x + δ)), 即对于两个非常像的样本z 与x 有\\nP(c | x) ≃P(c | z), 即\\nX\\nc∈Y\\nP(c | x)P(c | z) ≃\\nX\\nc∈Y\\nP 2(c | x)\\n第三行的“小于等于(⩽) ”关系更简单：由于c∗∈Y, 所以P 2 (c∗| x) ⩽P\\nc∈Y P 2(c | x), 也就是“小\\n于等于(⩽) ”左边只是右边的一部分, 所以肯定是小于等于的关系;\\n第四行就是数学公式a2 −b2 = (a + b)(a −b)；\\n第五行是由于1 + P (c∗| x) ⩽2, 这是由于概率值P (c∗| x) ⩽1\\n经过以上推导, 本节最后给出一个惊人的结论：最近邻分类器虽简单, 但它的泛化错误率不超过贝叶\\n斯最优分类器的错误率的两倍!\\n然而这是一个没啥实际用途的结论, 因为这个结论必须满足两个假设条件, 且不说P(c | x) 是连续函\\n数（第一个假设）是否满足, 单就“对任意x 和任意小正数δ, 在x 附近δ 距离范围内总能找到一个训练', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 122, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='样本”(第二个假设) 是不可能满足的, 这也就有了10.2 节开头一段的讨论, 抛开“任意小正数δ ”不谈, 具\\n体到δ = 0.001 都是不现实的。\\n→_→\\n配套视频教程：https://www.bilibili.com/video/BV1Mh411e7VU\\n←_←', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 122, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='→_→\\n欢迎去各大电商平台选购纸质版南瓜书《机器学习公式详解第2 版》\\n←_←\\n10.4\\n低维嵌入\\n10.4.1\\n图10.2 的解释\\n只要注意一点就行：在图(a) 三维空间中，红色线是弯曲的，但去掉高度这一维（竖着的坐标轴）后，\\n红色线变成直线，而直线更容易学习。\\n10.4.2\\n式(10.3) 的推导\\n已知Z = {z1, z2, . . . , zi, . . . , zm} ∈Rd′×m, 其中zi = (zi1; zi2; . . . ; zid′) ∈Rd′×1; 降维后的内积矩阵\\nB = Z⊤Z ∈Rm×m, 其中第i 行第j 列元素bij, 特别的\\nbii = z⊤\\ni zi = ∥zi∥2 , bjj = z⊤\\nj zj = ∥zj∥2 , bij = z⊤\\ni zj\\nMDS 算法的目标是∥zi −zj∥= distij = ∥xi −xj∥, 即保持样本的欧氏距离在d′ 维空间和原始d 维空间\\n相同(d′ ⩽d) 。\\ndist2\\nij = ∥zi −zj∥2 = (zi1 −zj1)2 + (zi2 −zj2)2 + . . . + (zid′ −zjd′)2\\n=', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 123, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='=\\n\\x00z2\\ni1 −2zi1zj1 + z2\\nj1\\n\\x01\\n+\\n\\x00z2\\ni2 −2zi2zj2 + z2\\nj2\\n\\x01\\n+ . . . +\\n\\x00z2\\nid′ −2zid′zjd′ + z2\\njd′\\n\\x01\\n=\\n\\x00z2\\ni1 + z2\\ni2 + . . . + z2\\nid′\\n\\x01\\n+\\n\\x00z2\\nj1 + z2\\nj2 + . . . + z2\\njd′\\n\\x01\\n−2 (zi1zj1 + zi2zj2 + . . . + zid′zjd′)\\n= ∥zi∥2 + ∥zj∥2 −2z⊤\\ni zj\\n= bii + bjj −2bij\\n本章矩阵运算非常多, 刚刚是从矩阵元素层面的推导; 实际可发现上式运算结果基本与标量运算规则\\n相同, 因此后面会尽可能不再从元素层面推导。具体来说：\\ndist2\\nij = ∥zi −zj∥2 = (zi −zj)⊤(zi −zj)\\n= z⊤\\ni zi −z⊤\\ni zj −z⊤\\nj zi + z⊤\\nj zj\\n= z⊤\\ni zi + z⊤\\nj zj −2z⊤\\ni zj\\n= ∥zi∥2 + ∥zj∥2 −2z⊤\\ni zj\\n= bii + bjj −2bij\\n上式第三个等号化简是由于内积z⊤', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 123, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='i zj\\n= bii + bjj −2bij\\n上式第三个等号化简是由于内积z⊤\\ni zj 和z⊤\\nj zi 均为标量, 因此转置等于本身。\\n10.4.3\\n式(10.4) 的推导\\n首先解释两个条件：\\n(1) 令降维后的样本Z 被中心化, 即Pm\\ni=1 zi = 0 注意Z ∈Rd′×m, d′ 是样本维度(属性个数), m 是样\\n本个数, 易知Z 的每一行有m 个元素(每行表示样本集的一维属性), Z 的每一列有d′ 个元素(每列表示\\n一个样本)。\\n式Pm\\ni=1 zi = 0 中的zi 明显表示的是第i 列, m 列相加得到一个零向量0d′×1, 意思是样本集合中所\\n有样本的每一维属性之和均等于0 , 因此被中心化的意思是将样本集合Z 的每一行（属性）减去该行的均\\n值。\\n(2) 显然, 矩阵B 的行与列之各均为零, 即Pm\\ni=1 bij = Pm\\nj=1 bij = 0 。\\n注意bij = z⊤\\ni zj (也可以写为bij = z⊤\\nj zi, 其实就是对应元素相乘, 再求和)\\nm\\nX\\ni=1\\nbij =\\nm\\nX\\ni=1\\nz⊤\\nj zi = z⊤\\nj\\nm\\nX\\ni=1', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 123, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='m\\nX\\ni=1\\nbij =\\nm\\nX\\ni=1\\nz⊤\\nj zi = z⊤\\nj\\nm\\nX\\ni=1\\nzi = z⊤\\nj · 0d′×1 = 0\\nm\\nX\\nj=1\\nbij =\\nm\\nX\\nj=1\\nz⊤\\ni zj = z⊤\\ni\\nm\\nX\\nj=1\\nzj = z⊤\\ni · 0d′×1 = 0\\n→_→\\n配套视频教程：https://www.bilibili.com/video/BV1Mh411e7VU\\n←_←', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 123, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='→_→\\n欢迎去各大电商平台选购纸质版南瓜书《机器学习公式详解第2 版》\\n←_←\\n接下来我们推导式(10.4), 将式(10.3) 的dist2\\nij 表达式代入：\\nm\\nX\\ni=1\\ndist2\\nij =\\nm\\nX\\ni=1\\n\\x10\\n∥zi∥2 + ∥zj∥2 −2z⊤\\ni zj\\n\\x11\\n=\\nm\\nX\\ni=1\\n∥zi∥2 +\\nm\\nX\\ni=1\\n∥zj∥2 −2\\nm\\nX\\ni=1\\nz⊤\\ni zj\\n根据定义：\\nm\\nX\\ni=1\\n∥zi∥2 =\\nm\\nX\\ni=1\\nz⊤\\ni zi =\\nm\\nX\\ni=1\\nbii = tr(B)\\nm\\nX\\ni=1\\n∥zj∥2 = ∥zj∥2\\nm\\nX\\ni=1\\n1 = m ∥zj∥2 = mz⊤\\nj zj = mbjj\\n根据前面结果：\\nm\\nX\\ni=1\\nz⊤\\ni zj =\\n m\\nX\\ni=1\\nz⊤\\ni\\n!\\nzj = 01×d′ · zj = 0\\n代入上式即得：\\nm\\nX\\ni=1\\ndist2\\nij =\\nm\\nX\\ni=1\\n∥zi∥2 +\\nm\\nX\\ni=1\\n∥zj∥2 −2\\nm\\nX\\ni=1\\nz⊤\\ni zj\\n= tr(B) + mbjj\\n10.4.4\\n式(10.5) 的推导\\n与式(10.4) 类似：\\nm\\nX\\nj=1', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 124, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='10.4.4\\n式(10.5) 的推导\\n与式(10.4) 类似：\\nm\\nX\\nj=1\\ndist2\\nij =\\nm\\nX\\nj=1\\n\\x10\\n∥zi∥2 + ∥zj∥2 −2z⊤\\ni zj\\n\\x11\\n=\\nm\\nX\\nj=1\\n∥zi∥2 +\\nm\\nX\\nj=1\\n∥zj∥2 −2\\nm\\nX\\nj=1\\nz⊤\\ni zj\\n= mbii + tr(B)\\n10.4.5\\n式(10.6) 的推导\\nm\\nX\\ni=1\\nm\\nX\\nj=1\\ndist2\\nij =\\nm\\nX\\ni=1\\nm\\nX\\nj=1\\n\\x10\\n∥zi∥2 + ∥zj∥2 −2z⊤\\ni zj\\n\\x11\\n=\\nm\\nX\\ni=1\\nm\\nX\\nj=1\\n∥zi∥2 +\\nm\\nX\\ni=1\\nm\\nX\\nj=1\\n∥zj∥2 −2\\nm\\nX\\ni=1\\nm\\nX\\nj=1\\nz⊤\\ni zj\\n其中各子项的推导如下：\\nm\\nX\\ni=1\\nm\\nX\\nj=1\\n∥zi∥2 = m\\nm\\nX\\ni=1\\n∥zi∥2 = m tr(B)\\nm\\nX\\ni=1\\nm\\nX\\nj=1\\n∥zj∥2 = m\\nm\\nX\\nj=1\\n∥zj∥2 = m tr(B)\\nm\\nX\\ni=1\\nm\\nX\\nj=1\\nz⊤\\ni zj = 0\\n最后一个式子是来自于书中的假设，假设降维后的样本Z 被中心化。\\n→_→', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 124, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='z⊤\\ni zj = 0\\n最后一个式子是来自于书中的假设，假设降维后的样本Z 被中心化。\\n→_→\\n配套视频教程：https://www.bilibili.com/video/BV1Mh411e7VU\\n←_←', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 124, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='→_→\\n欢迎去各大电商平台选购纸质版南瓜书《机器学习公式详解第2 版》\\n←_←\\n10.4.6\\n式(10.10) 的推导\\n由式(10.3) 可得\\nbij = −1\\n2(dist2\\nij −bii −bjj)\\n由式(10.6) 和(10.9) 可得\\ntr(B) =\\n1\\n2m\\nm\\nX\\ni=1\\nm\\nX\\nj=1\\ndist2\\nij\\n= m\\n2 dist2\\n·\\n由式(10.4) 和(10.8) 可得\\nbjj = 1\\nm\\nm\\nX\\ni=1\\ndist2\\nij −1\\nmtr(B)\\n= dist2\\n·j −1\\n2dist2\\n·\\n由式(10.5) 和式(10.7) 可得\\nbii = 1\\nm\\nm\\nX\\nj=1\\ndist2\\nij −1\\nmtr(B)\\n= dist2\\ni· −1\\n2dist2\\n·\\n综合可得\\nbij = −1\\n2(dist2\\nij −bii −bjj)\\n= −1\\n2(dist2\\nij −dist2\\ni· + 1\\n2dist2\\n·· −dist2\\n·j + 1\\n2dist2\\n··)\\n= −1\\n2(dist2\\nij −dist2\\ni· −dist2\\n·j + dist2\\n··)', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 125, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='= −1\\n2(dist2\\nij −dist2\\ni· −dist2\\n·j + dist2\\n··)\\n在式(10.10) 后紧跟着一句话：\\n“由此即可通过降维前后保持不变的距离矩阵D 求取内积矩阵B”, 我\\n们来解释一下这句话。\\n首先解释式(10.10) 等号右侧的变量含义：\\ndistij = ∥zi −zj∥表示降维后zi 与zj 的欧氏距离, 注\\n意这同时也应该是原始空间xi 与xj 的距离, 因为降维的目标（也是约束条件）是“任意两个样本在d′ 维\\n空间中的欧氏距离等于原始空间中的距离”; 其次, 式(10.10) 等号左侧bij 是降维后内积矩阵B 的元素,\\n即B 的元素bij 可以由距离矩阵D 来表达求取。\\n10.4.7\\n式(10.11) 的解释\\n由题设知，d∗为V 的非零特征值，因此B = VΛV⊤可以写成B = V∗Λ∗V⊤\\n∗，其中Λ∗∈Rd×d 为d\\n个非零特征值构成的特征值对角矩阵，而V∗∈Rm×d 为Λ∗∈Rd×d 对应的特征值向量矩阵，因此有\\nB =\\n\\x10\\nV∗Λ1/2\\n∗\\n\\x11 \\x10\\nΛ1/2\\n∗V⊤\\n∗\\n\\x11\\n故而Z = Λ1/2\\n∗V⊤\\n∗∈Rd×m\\n10.4.8', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 125, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='∗\\n\\x11 \\x10\\nΛ1/2\\n∗V⊤\\n∗\\n\\x11\\n故而Z = Λ1/2\\n∗V⊤\\n∗∈Rd×m\\n10.4.8\\n图10.3 关于MDS 算法的解释\\n首先要清楚此处降维算法要完成的任务：获得d 维空间的样本集合X ∈Rd×m 在d′ 维空间的表示\\nZ ∈Rd′×m, 并且保证距离矩阵D ∈Rm×m 相同, 其中d′ < d, m 为样本个数, 距离矩阵即样本之间的欧氏\\n距离。那么怎么由X ∈Rd×m 得到Z ∈Rd′×m 呢?\\n经过推导发现(式(10.3) 式(10.10)), 在保证距离矩阵D ∈Rm×m 相同的前提下, d′ 维空间的样本集\\n合Z ∈Rd′×m 的内积矩阵B = Z⊤Z ∈Rm×m 可以由距离矩阵D ∈Rm×m 得到(参见式(10.10)), 此时只\\n→_→\\n配套视频教程：https://www.bilibili.com/video/BV1Mh411e7VU\\n←_←', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 125, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='→_→\\n欢迎去各大电商平台选购纸质版南瓜书《机器学习公式详解第2 版》\\n←_←\\n要对B 进行矩阵分解即可得到Z; 具体来说, 对B 进行特征值分解可得B = VΛV⊤, 其中V ∈Rm×m 为\\n特征值向量矩阵, \\uffff∈Rm×m 为特征值构成的对角矩阵, 接下来分类讨论：\\n(1) 当d > m 时, 即样本属性比样本个数还要多此时, 样本集合X ∈Rd×m 的d 维属性一定是线性相\\n关的(即有品几余), 因为矩阵X 的秩不会大于m (此处假设矩阵X 的秩恰好等于m ), 因此Λ ∈Rm×m\\n主对角线有m 个非零值, 进而B =\\n\\x10\\nVΛ1/2\\x11 \\x10\\nΛ1/2V⊤\\x11\\n, 得到的Z = Λ1/2V⊤∈Rd′×m 实际将d 维属性降\\n成了d′ = m 维属性。\\n(2) 当d < m 时, 即样本个数比样本属性多这是现实中最常见的一种情况。此时Λ ∈Rm×m 至多\\n有d 个非零值（此处假设恰有d 个非零值), 因此B = VΛV⊤可以写成B = V∗Λ∗V⊤\\n∗, 其中Λ∗∈Rd×d\\n为d 个非零值特征值构成的特征值对角矩阵, V∗∈Rm×d 为Λ∗∈Rd×d 相应的特征值向量矩阵, 进而\\nB =', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 126, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='B =\\n\\x10\\nV∗Λ1/2\\n∗\\n\\x11 \\x10\\nΛ1/2\\n∗V⊤\\n∗\\n\\x11\\n, 求得Z = Λ1/2\\n∗V⊤\\n∗∈Rd×m, 此时属性没有冗杂, 因此按降维的规则（降维后距\\n离矩阵不变）并不能实现有效降维。\\n由以上分析可以看出, 降维后的维度d′ 实际为B 特征值分解后非零特征值的个数。\\n10.5\\n主成分分析\\n注意，作者在数次印刷中对本节符号进行修订，详见勘误修订，直接搜索页码即可，此处仅按个人推\\n导需求定义符号，可能与不同印次书中符号不一致。\\n10.5.1\\n式(10.14) 的推导\\n在一个坐标系中, 任意向量等于其在各个坐标轴的坐标值乘以相应坐标轴单位向量之和。例如, 在\\n二维直角坐标系中, x 轴和y 轴的单位向量分别为v1 = (1; 0) 和v2 = (0; 1), 向量r = (2; 3) 可以\\n表示为r = 2v1 + 3v2; 其实v1 = (1; 0) 和v2 = (0; 1) 只是二维平面的一组标准正交基, 但二维平面\\n实际有无数标准正交基, 如v′\\n1 =\\n\\x10\\n1\\n√\\n2;\\n1\\n√\\n2\\n\\x11\\n和v′\\n2 =\\n\\x10\\n−1\\n√\\n2;\\n1\\n√\\n2\\n\\x11\\n, 此时向量r =\\n5\\n√\\n2v′', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 126, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='√\\n2\\n\\x11\\n和v′\\n2 =\\n\\x10\\n−1\\n√\\n2;\\n1\\n√\\n2\\n\\x11\\n, 此时向量r =\\n5\\n√\\n2v′\\n1 +\\n1\\n√\\n2v′\\n2, 其中\\n5\\n√\\n2 = (v′\\n1)⊤r,\\n1\\n√\\n2 = (v′\\n2)⊤r, 即新坐标系里的坐标。\\n下面开始推导，对于d 维空间Rd×1 来说, 传统的坐标系为{v1, v2, . . . , vk, . . . , vd}, 其中vk 为除第k\\n个元素为1 其余元素均0 的d 维列向量; 此时对于样本点xi = (xi1; xi2; . . . ; xid) ∈Rd×1 来说亦可表示为\\nxi = xi1v1 + xi2v2 + . . . + xidvd 。\\n现假定投影变换后得到的新坐标系为{w1, w2, . . . , wk, . . . , wd} （即一组新的标准正交基), 则xi 在\\n新坐标系中的坐标为\\n\\x00w⊤\\n1 xi; w⊤\\n2 xi; . . . ; w⊤\\nd xi\\n\\x01\\n。若丢弃新坐标系中的部分坐标, 即将维度降低到d′ < d\\n(不失一般性, 假设丢掉的是后d −d′ 维坐标), 并令', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 126, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='(不失一般性, 假设丢掉的是后d −d′ 维坐标), 并令\\nW = (w1, w2, . . . , wd′) ∈Rd×d′\\n则xi 在低维坐标系中的投影为\\nzi = (zi1; zi2; . . . ; zid′) =\\n\\x00w⊤\\n1 xi; w⊤\\n2 xi; . . . ; w⊤\\nd′xi\\n\\x01\\n= W⊤xi\\n若基于zi 来重构xi, 则会得到ˆ\\nxi = Pd′\\nj=1 zijwj = Wzi (“西瓜书”P230 第11 行)。\\n有了以上符号基础, 接下来将式(10.14) 化简成式(10.15) 目标函数形式(可逐一核对各项维数以验证\\n推导是否有误)：\\n→_→\\n配套视频教程：https://www.bilibili.com/video/BV1Mh411e7VU\\n←_←', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 126, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='→_→\\n欢迎去各大电商平台选购纸质版南瓜书《机器学习公式详解第2 版》\\n←_←\\nm\\nX\\ni=1\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\nd′\\nX\\nj=1\\nzijwj −xi\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n2\\n2\\n=\\nm\\nX\\ni=1\\n∥Wzi −xi∥2\\n2\\n1\\n=\\nm\\nX\\ni=1\\n\\r\\n\\rWW⊤xi −xi\\n\\r\\n\\r2\\n2\\n2\\n=\\nm\\nX\\ni=1\\n\\x00WW⊤xi −xi\\n\\x01⊤\\x00WW⊤xi −xi\\n\\x01\\n3\\n=\\nm\\nX\\ni=1\\n\\x00x⊤\\ni WW⊤WW⊤xi −2x⊤\\ni WW⊤xi + x⊤\\ni xi\\n\\x01\\n4\\n=\\nm\\nX\\ni=1\\n\\x00x⊤\\ni WW⊤xi −2x⊤\\ni WW⊤xi + x⊤\\ni xi\\n\\x01\\n5\\n=\\nm\\nX\\ni=1\\n\\x00−x⊤\\ni W⊤xi + x⊤\\ni xi\\n\\x01\\n6\\n=\\nm\\nX\\ni=1\\n\\x10\\n−\\n\\x00W⊤xi\\n\\x01⊤\\x00W⊤xi\\n\\x01\\n+ x⊤\\ni xi\\n\\x11\\n7\\n=\\nm\\nX\\ni=1\\n\\x10\\n−\\n\\r\\n\\rW⊤xi\\n\\r\\n\\r2\\n2 + x⊤\\ni xi\\n\\x11\\n8\\n∝−\\nm\\nX\\ni=1\\n\\r\\n\\rW⊤xi\\n\\r\\n\\r2\\n2\\n9\\n3 →4 是由于\\n\\x00WW⊤\\x01⊤=\\n\\x00W⊤\\x01⊤(W)⊤= WW⊤, 因此\\n\\x00WW⊤xi\\n\\x01⊤= x⊤\\ni\\n\\x00WW⊤\\x01⊤= x⊤', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 127, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='\\x00W⊤\\x01⊤(W)⊤= WW⊤, 因此\\n\\x00WW⊤xi\\n\\x01⊤= x⊤\\ni\\n\\x00WW⊤\\x01⊤= x⊤\\ni WW⊤\\n代入即得4 ;\\n4 →5 是由于w⊤\\ni wj = 0, (i ̸= j), ∥wi∥= 1, 因此W⊤W = I ∈Rd′×d′, 代入即得5 。\\n由于最终目标是\\n寻找W 使目标函数(10.14) 最小, 而x⊤\\ni xi 与W 无关, 因此在优化时可以去掉。\\n令X = (x1, x2, . . . , xm) ∈\\nRd×m, 即每列为一个样本, 则式(10.14) 可继续化简为(参见10.2节)\\n−\\nm\\nX\\ni=1\\n\\r\\n\\rW⊤xi\\n\\r\\n\\r2\\n2 = −\\n\\r\\n\\rW⊤X\\n\\r\\n\\r2\\nF\\n= −tr\\n\\x10\\x00W⊤X\\n\\x01 \\x00W⊤X\\n\\x01⊤\\x11\\n= −tr\\n\\x00W⊤XX⊤W\\n\\x01\\n这里W⊤xi = zi, 这里仅为得到式(10.15) 的形式才最终保留W 和xi 的; 若令Z = (z1, z2, . . . , zm) ∈\\nRd′×m 为低维坐标系中的样本集合, 则Z = W⊤X, 即zi 为矩阵Z 的第i 列; 而Pm\\ni=1\\n\\r\\n\\rW⊤xi\\n\\r\\n\\r2\\n2 =\\nPm\\ni=1 ∥zi∥2', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 127, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='i=1\\n\\r\\n\\rW⊤xi\\n\\r\\n\\r2\\n2 =\\nPm\\ni=1 ∥zi∥2\\n2 表示Z 所有列向量2 范数的平方, 也就是Z 所有元素的平方和, 即为∥Z∥2\\nF, 此即第一个等号\\n的由来; 而根据10.2节中第(3) 个结论, 即对于矩阵Z 有∥Z∥2\\nF = tr\\n\\x00Z⊤Z\\n\\x01\\n= tr\\n\\x00ZZ⊤\\x01\\n, 其中tr(·) 表示求矩\\n阵的迹, 即对角线元素之和, 此即第二个等号的由来; 第三个等号将转置化简即得。\\n到此即得式(10.15) 的目标函数, 约束条件W⊤W = I 已在推导中说明。\\n式(10.15) 的目标函数式(10.14) 结果略有差异, 接下来推导Pm\\ni=1 xix⊤\\ni = XX⊤以弥补这个差异（这\\n个结论可以记下来)。\\n→_→\\n配套视频教程：https://www.bilibili.com/video/BV1Mh411e7VU\\n←_←', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 127, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='→_→\\n欢迎去各大电商平台选购纸质版南瓜书《机器学习公式详解第2 版》\\n←_←\\n先化简Pm\\ni=1 xix⊤\\ni , 首先\\nxix⊤\\ni =\\n\\uf8ee\\n\\uf8ef\\n\\uf8ef\\n\\uf8ef\\n\\uf8ef\\n\\uf8ef\\n\\uf8f0\\nxi1\\nxi2\\n.\\n.\\n.\\nxid\\n\\uf8f9\\n\\uf8fa\\n\\uf8fa\\n\\uf8fa\\n\\uf8fa\\n\\uf8fa\\n\\uf8fb\\nh\\nxi1\\nxi2\\n· · ·\\nxid\\ni\\n=\\n\\uf8ee\\n\\uf8ef\\n\\uf8ef\\n\\uf8ef\\n\\uf8ef\\n\\uf8ef\\n\\uf8f0\\nxi1xi1\\nxi1xi2\\n· · ·\\nxi1xid\\nxi2xi1\\nxi2xi2\\n· · ·\\nxi2xid\\n.\\n.\\n.\\n.\\n.\\n.\\n...\\n.\\n.\\n.\\nxidxi1\\nxidxi2\\n· · ·\\nxidxid\\n\\uf8f9\\n\\uf8fa\\n\\uf8fa\\n\\uf8fa\\n\\uf8fa\\n\\uf8fa\\n\\uf8fb\\nd×d\\n整体代入求和号Pm\\ni=1 xix⊤\\ni , 得\\nm\\nX\\ni=1\\nxix⊤\\ni =\\nm\\nX\\ni=1\\n\\uf8ee\\n\\uf8ef\\n\\uf8ef\\n\\uf8ef\\n\\uf8ef\\n\\uf8ef\\n\\uf8f0\\nxi1xi1\\nxi1xi2\\n· · ·\\nxi1xid\\nxi2xi1\\nxi2xi2\\n· · ·\\nxi2xid\\n.\\n.\\n.\\n.\\n.\\n.\\n...\\n.\\n.\\n.\\nxidxi1\\nxidxi2\\n· · ·\\nxidxid\\n\\uf8f9\\n\\uf8fa\\n\\uf8fa\\n\\uf8fa\\n\\uf8fa\\n\\uf8fa\\n\\uf8fb\\nd×d\\n=\\n\\uf8ee\\n\\uf8ef\\n\\uf8ef\\n\\uf8ef\\n\\uf8ef\\n\\uf8ef\\n\\uf8f0\\nPm\\ni=1 xi1xi1\\nPm', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 128, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='\\uf8fa\\n\\uf8fa\\n\\uf8fa\\n\\uf8fa\\n\\uf8fa\\n\\uf8fb\\nd×d\\n=\\n\\uf8ee\\n\\uf8ef\\n\\uf8ef\\n\\uf8ef\\n\\uf8ef\\n\\uf8ef\\n\\uf8f0\\nPm\\ni=1 xi1xi1\\nPm\\ni=1 xi1xi2\\n· · ·\\nPm\\ni=1 xi1xid\\nPm\\ni=1 xi2xi1\\nPm\\ni=1 xi2xi2\\n· · ·\\nPm\\ni=1 xi2xid\\n.\\n.\\n.\\n.\\n.\\n.\\n...\\n.\\n.\\n.\\nPm\\ni=1 xidxi1\\nPm\\ni=1 xidxi2\\n· · ·\\nPm\\ni=1 xidxid\\n\\uf8f9\\n\\uf8fa\\n\\uf8fa\\n\\uf8fa\\n\\uf8fa\\n\\uf8fa\\n\\uf8fb\\nd×d\\n再化简XX⊤∈Rd×d\\nXX⊤=\\nh\\nx1\\nx2\\n· · ·\\nxd\\ni\\n\\uf8ee\\n\\uf8ef\\n\\uf8ef\\n\\uf8ef\\n\\uf8ef\\n\\uf8ef\\n\\uf8f0\\nx⊤\\n1\\nx⊤\\n2\\n.\\n.\\n.\\nx⊤\\nd\\n\\uf8f9\\n\\uf8fa\\n\\uf8fa\\n\\uf8fa\\n\\uf8fa\\n\\uf8fa\\n\\uf8fb\\n将列向量xi = (xi1; xi2; . . . ; xid) ∈Rd×1 代入\\nXX⊤=\\n\\uf8ee\\n\\uf8ef\\n\\uf8ef\\n\\uf8ef\\n\\uf8ef\\n\\uf8ef\\n\\uf8f0\\nx11\\nx21\\n· · ·\\nxm1\\nx12\\nx22\\n· · ·\\nxm2\\n.\\n.\\n.\\n.\\n.\\n.\\n...\\n.\\n.\\n.\\nx1d\\nx2d\\n· · ·\\nxmd\\n\\uf8f9\\n\\uf8fa\\n\\uf8fa\\n\\uf8fa\\n\\uf8fa\\n\\uf8fa\\n\\uf8fb\\nd×m\\n•\\n\\uf8ee\\n\\uf8ef\\n\\uf8ef\\n\\uf8ef\\n\\uf8ef\\n\\uf8ef\\n\\uf8f0\\nx11\\nx12\\n· · ·\\nx1d\\nx21\\nx22', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 128, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='\\uf8fa\\n\\uf8fb\\nd×m\\n•\\n\\uf8ee\\n\\uf8ef\\n\\uf8ef\\n\\uf8ef\\n\\uf8ef\\n\\uf8ef\\n\\uf8f0\\nx11\\nx12\\n· · ·\\nx1d\\nx21\\nx22\\n· · ·\\nx2d\\n.\\n.\\n.\\n.\\n.\\n.\\n...\\n.\\n.\\n.\\nxm1\\nxm2\\n· · ·\\nxmd\\n\\uf8f9\\n\\uf8fa\\n\\uf8fa\\n\\uf8fa\\n\\uf8fa\\n\\uf8fa\\n\\uf8fb\\nm×d\\n=\\n\\uf8ee\\n\\uf8ef\\n\\uf8ef\\n\\uf8ef\\n\\uf8ef\\n\\uf8ef\\n\\uf8f0\\nPm\\ni=1 xi1xi1\\nPm\\ni=1 xi1xi2\\n· · ·\\nPm\\ni=1 xi1xid\\nPm\\ni=1 xi2xi1\\nPm\\ni=1 xi2xi2\\n· · ·\\nPm\\ni=1 xi2xid\\n.\\n.\\n.\\n.\\n.\\n.\\n...\\n.\\n.\\n.\\nPm\\ni=1 xidxi1\\nPm\\ni=1 xidxi2\\n· · ·\\nPm\\ni=1 xidxid\\n\\uf8f9\\n\\uf8fa\\n\\uf8fa\\n\\uf8fa\\n\\uf8fa\\n\\uf8fa\\n\\uf8fb\\nd×d\\n综合Pm\\ni=1 xix⊤\\ni 和XX⊤的化简结果, 即Pm\\ni=1 xix⊤\\ni = XX⊤(协方差矩阵)。根据刚刚推导得到的结\\n论, 式(10.14) 最后的结果即可化为式(10.15) 的目标函数\\ntr\\n \\nW⊤\\n m\\nX\\ni=1\\nxix⊤\\ni\\n!\\nW\\n!\\n= tr\\n\\x00W⊤XX⊤W\\n\\x01', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 128, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='tr\\n \\nW⊤\\n m\\nX\\ni=1\\nxix⊤\\ni\\n!\\nW\\n!\\n= tr\\n\\x00W⊤XX⊤W\\n\\x01\\n式(10.15) 描述的优化问题的求解详见式(10.17) 最后的解释。\\n10.5.2\\n式(10.16) 的解释\\n先说什么是方差，对于包含n 个样本的一组数据X = {x1, x2, . . . , xn} 来说, 均值M 为\\nM = x1 + x2 + . . . + xn\\nn\\n=\\nn\\nX\\ni=1\\nxi\\n→_→\\n配套视频教程：https://www.bilibili.com/video/BV1Mh411e7VU\\n←_←', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 128, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='→_→\\n欢迎去各大电商平台选购纸质版南瓜书《机器学习公式详解第2 版》\\n←_←\\n则方差σ2\\nX 公式为\\nσ2 = (x1 −M)2 + (x2 −M)2 + . . . + (xn −M)2\\nn\\n= 1\\nn\\nn\\nX\\ni=1\\n(xi −M)2\\n方差衡量了该组数据偏离均值的程度，样本越分散, 其方差越大。\\n再说什么是协方差，若还有包含n 个样本的另一组数据X′ = {x′\\n1, x′\\n2, . . . , x′\\nn}, 均值为M ′, 则下式\\nσ2\\nXX′ = (x1 −M) (x′\\n1 −M ′) + (x2 −M) (x′\\n2 −M ′) + . . . + (xn −M) (x′\\nn −M ′)\\nn\\n= 1\\nn\\nn\\nX\\ni=1\\n(xi −M) (x′\\ni −M ′)\\n称为两组数据的协方差。σ2\\nXX′ 能说明第一组数据x1, x2, . . . , xn 和第二组数据x′\\n1, x′\\n2, . . . , x′\\nn 的变\\n化情况。具体来说, 如果两组数据总是同时大于或小于自己的均值, 则(xi −M) (x′\\ni −M ′) > 0, 此时\\nσ2', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 129, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='i −M ′) > 0, 此时\\nσ2\\nXX′ > 0; 如果两组数据总是一个大于(或小于) 自己的均值而别一个小于(或大于) 自己的均值, 则\\n(xi −M) (x′\\ni −M ′) < 0, 此时σ2\\nXX′ < 0; 如果两组数据与自己的均值的大小关系无规律, 则(xi −M) (x′\\ni −M ′)\\n的正负号随机变化, 其平均数σ2\\nXX, 则会趋近于0 。引用百度百科协方差词条原话：\\n“从直观上来看, 协方\\n差表示的是两个变量总体误差的期望。如果两个变量的变化趋势一致, 也就是说如果其中一个大于自身的\\n期望值时另外一个也大于自身的期望值, 那么两个变量之间的协方差就是正值; 如果两个变量的变化趋势\\n相反, 即其中一个变量大于自身的期望值时另外一个却小于自身的期望值, 那么两个变量之间的协方差就\\n是负值。如果两个变量是统计独立的, 那么二者之间的协方差就是0 , 但是, 反过来并不成立。协方差为0\\n的两个随机变量称为是不相关的。\\n”\\n最后说什么是协方差矩阵，结合本书中的符号：\\nX = (x1, x2, . . . , xm) =\\n\\uf8ee\\n\\uf8ef\\n\\uf8ef\\n\\uf8ef\\n\\uf8ef\\n\\uf8ef\\n\\uf8f0\\nx11\\nx21', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 129, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='X = (x1, x2, . . . , xm) =\\n\\uf8ee\\n\\uf8ef\\n\\uf8ef\\n\\uf8ef\\n\\uf8ef\\n\\uf8ef\\n\\uf8f0\\nx11\\nx21\\n· · ·\\nxm1\\nx12\\nx22\\n· · ·\\nxm2\\n.\\n.\\n.\\n.\\n.\\n.\\n...\\n.\\n.\\n.\\nx1d\\nx2d\\n· · ·\\nxmd\\n\\uf8f9\\n\\uf8fa\\n\\uf8fa\\n\\uf8fa\\n\\uf8fa\\n\\uf8fa\\n\\uf8fb\\nd×m\\n矩阵X 每一行表示一维特征, 每一列表示该数据集的一个样本; 而本节开始已假定数据样本进行了中心化,\\n即Pm\\ni=1 xi = 0 ∈Rd×1 (中心化过程可通过X\\n\\x00I −1\\nm11⊤) 实现, 其中I ∈Rm×m 为单位阵, 1 ∈Rm×1 为\\n全1 列向量, 参见习题10.3), 即上式矩阵的每一行平均值等于零(其实就是分别对所有xi 的每一维坐标\\n进行中心化, 而不是分别对单个样本xi 中心化）对于包含d 个特征的特征空间（或称d 维特征空间）来\\n说, 每一维特征可以看成是一个随机变量, 而X 中包含m 个样本, 也就是说每个随机变量有m 个数据, 根\\n据前面XX⊤的矩阵表达形式：\\n1\\nmXX⊤= 1\\nm\\n\\uf8ee\\n\\uf8ef\\n\\uf8ef\\n\\uf8ef\\n\\uf8ef\\n\\uf8ef\\n\\uf8f0\\nPm\\ni=1 xi1xi1\\nPm\\ni=1 xi1xi2\\n· · ·\\nPm', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 129, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='\\uf8ef\\n\\uf8ef\\n\\uf8ef\\n\\uf8ef\\n\\uf8ef\\n\\uf8f0\\nPm\\ni=1 xi1xi1\\nPm\\ni=1 xi1xi2\\n· · ·\\nPm\\ni=1 xi1xid\\nPm\\ni=1 xi2xi1\\nPm\\ni=1 xi2xi2\\n· · ·\\nPm\\ni=1 xi2xid\\n.\\n.\\n.\\n.\\n.\\n.\\n...\\n.\\n.\\n.\\nPm\\ni=1 xidxi1\\nPm\\ni=1 xidxi2\\n· · ·\\nPm\\ni=1 xidxid\\n\\uf8f9\\n\\uf8fa\\n\\uf8fa\\n\\uf8fa\\n\\uf8fa\\n\\uf8fa\\n\\uf8fb\\nd×d\\n根据前面的结果知道\\n1\\nmXX⊤的第i 行第j 列的元素表示X 中第i 行和X⊤第j 列（即X 中第j 行）\\n的方差(i = j) 或协方差(i ̸= j) 。注意：协方差矩阵对角线元素为各行的方差。\\n接下来正式解释式(10.16)：对于X = (x1, x2, . . . , xm) ∈Rd×m, 将其投影为Z = (z1, z2, . . . , zm) ∈\\nRd′×m, 最大可分性出发, 我们希望在新空间的每一维坐标轴上样本都尽可能分散(即每维特征尽可能分\\n散, 也就是Z 各行方差最大; 参见图10.4 所示, 原空间只有两维坐标, 现考虑降至一维, 希望在新坐标系', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 129, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='下样本尽可能分散, 图中画出了一种映射后的坐标系, 显然橘红色坐标方向样本更分散, 方差更大), 即寻\\n→_→\\n配套视频教程：https://www.bilibili.com/video/BV1Mh411e7VU\\n←_←', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 129, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='→_→\\n欢迎去各大电商平台选购纸质版南瓜书《机器学习公式详解第2 版》\\n←_←\\n找W ∈Rd×d′ 使协方差矩阵\\n1\\nmZZ⊤对角线元素之和(矩阵的迹）最大（即使Z 各行方差之和最大), 由\\n于Z = W⊤X, 而常系数\\n1\\nm 在最大化时并不发生影响, 求矩阵对角线元素之和即为矩阵的迹, 综上即得式\\n(10.16)。\\n另外, 中心化后X 的各行均值为零, 变换后Z = W⊤X 的各行均值仍为零, 这是因为Z 的第i 行\\n(1 ⩽i ⩽d′) 为\\n\\x08\\nw⊤\\ni x1, w⊤\\ni x2, . . . , w⊤\\ni xm\\n\\t\\n, 该行之和w⊤\\ni\\nPm\\nj=1 xj = w⊤\\ni 0 = 0 。\\n最后, 有关方差的公式, 有人认为应该除以样本数量m, 有人认为应该除以样本数量减1 即m −1 。\\n简单来说, 根据总体样本集求方差就除以总体样本数量, 而根据抽样样本集求方差就除以抽样样本集数量\\n减1; 总体样本集是真正想调查的对象集合, 而抽样样本集是从总体样本集中被选出来的部分样本组成的\\n集合, 用来估计总体样本集的方差; 一般来说, 总体样本集是不可得的, 我们拿到的都是抽样样本集。严格', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 130, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='上来说，样本方差应该除以n −1 才会得到总体样本的无偏估计, 若除以n 则得到的是有偏估计。\\n式(10.16) 描述的优化问题的求解详见式(10.17) 最后的解释。\\n10.5.3\\n式(10.17) 的推导\\n由式（10.15）可知，主成分分析的优化目标为\\nmin\\nW\\n−tr (WTXXTW)\\ns.t.\\nWTW = I\\n其中，X = (x1, x2, . . . , xm) ∈Rd×m, W = (w1, w2, . . . , wd′) ∈Rd×d′，I ∈Rd′×d′ 为单位矩阵。对于带矩\\n阵约束的优化问题，根据[1] 中讲述的方法可得此优化目标的拉格朗日函数为\\nL(W, Θ) = −tr (WTXXTW) + ⟨Θ, WTW −I⟩\\n= −tr (WTXXTW) + tr\\n\\x00ΘT(WTW −I)\\n\\x01\\n其中，Θ ∈Rd′×d′ 为拉格朗日乘子矩阵，其维度恒等于约束条件的维度，且其中的每个元素均为未知\\n的拉格朗日乘子，\\n⟨Θ, WTW−I⟩= tr\\n\\x00ΘT(WTW −I)\\n\\x01\\n为矩阵的内积\\n[2]。\\n若此时仅考虑约束wT\\ni wi = 1(i =\\n1, 2, ..., d′)，', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 130, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='[2]。\\n若此时仅考虑约束wT\\ni wi = 1(i =\\n1, 2, ..., d′)，\\n则拉格朗日乘子矩阵Θ 此时为对角矩阵，\\n令新的拉格朗日乘子矩阵为Λ = diag(λ1, λ2, ..., λd′) ∈\\nRd′×d′，则新的拉格朗日函数为\\nL(W, Λ) = −tr (WTXXTW) + tr\\n\\x00ΛT(WTW −I)\\n\\x01\\n对拉格朗日函数关于W 求导可得\\n∂L(W, Λ)\\n∂W\\n=\\n∂\\n∂W\\n\\x02\\n−tr (WTXXTW) + tr\\n\\x00ΛT(WTW −I)\\n\\x01\\x03\\n= −\\n∂\\n∂W tr (WTXXTW) +\\n∂\\n∂W tr\\n\\x00ΛT(WTW −I)\\n\\x01\\n由矩阵微分公式\\n∂\\n∂X tr (XTBX) = BX + BTX,\\n∂\\n∂X tr\\n\\x00BXTX\\n\\x01\\n= XBT + XB 可得\\n∂L(W, Λ)\\n∂W\\n= −2XXTW + WΛ + WΛT\\n= −2XXTW + W(Λ + ΛT)\\n= −2XXTW + 2WΛ\\n令\\n∂L(W, Λ)\\n∂W\\n= 0 可得\\n−2XXTW + 2WΛ = 0\\nXXTW = WΛ\\n→_→', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 130, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='∂L(W, Λ)\\n∂W\\n= 0 可得\\n−2XXTW + 2WΛ = 0\\nXXTW = WΛ\\n→_→\\n配套视频教程：https://www.bilibili.com/video/BV1Mh411e7VU\\n←_←', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 130, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='→_→\\n欢迎去各大电商平台选购纸质版南瓜书《机器学习公式详解第2 版》\\n←_←\\n将W 和Λ 展开可得\\nXXTwi = λiwi,\\ni = 1, 2, ..., d′\\n显然，此式为矩阵特征值和特征向量的定义式，其中λi, wi 分别表示矩阵XXT 的特征值和单位特征\\n向量。由于以上是仅考虑约束wT\\ni wi = 1 所求得的结果，而wi 还需满足约束wT\\ni wj = 0(i ̸= j)。观察\\nXXT 的定义可知，XXT 是一个实对称矩阵，实对称矩阵的不同特征值所对应的特征向量之间相互正交，\\n同一特征值的不同特征向量可以通过施密特正交化使其变得正交，所以通过上式求得的wi 可以同时满足\\n约束wT\\ni wi = 1, wT\\ni wj = 0(i ̸= j)。根据拉格朗日乘子法的原理可知，此时求得的结果仅是最优解的必要\\n条件，而且XXT 有d 个相互正交的单位特征向量，所以还需要从这d 个特征向量里找出d′ 个能使得目\\n标函数达到最优值的特征向量作为最优解。将XXTwi = λiwi 代入目标函数可得\\nmin\\nW −tr (WTXXTW) = max\\nW\\ntr (WTXXTW)\\n= max', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 131, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='min\\nW −tr (WTXXTW) = max\\nW\\ntr (WTXXTW)\\n= max\\nW\\nd′\\nX\\ni=1\\nwT\\ni XXTwi\\n= max\\nW\\nd′\\nX\\ni=1\\nwT\\ni · λiwi\\n= max\\nW\\nd′\\nX\\ni=1\\nλiwT\\ni wi\\n= max\\nW\\nd′\\nX\\ni=1\\nλi\\n显然，此时只需要令λ1, λ2, ..., λd′ 和w1, w2, . . . , wd′ 分别为矩阵XXT 的前d′ 个最大的特征值和单\\n位特征向量就能使得目标函数达到最优值。\\n10.5.4\\n根据式(10.17) 求解式(10.16)\\n注意式(10.16) 中W ∈Rd×d′, 只有d′ 列, 而式(10.17) 可以得到d 列, 如何根据式(10.17) 求解式\\n(10.16) 呢? 对XX⊤W = WΛ 两边同乘W⊤, 得\\nW⊤XX⊤W = W⊤WΛ = Λ\\n注意使用了约束条件W⊤W = I; 上式左边与式(10.16) 的优化目标对应矩阵相同, 而右边Λ ∈Rd′×d′\\n是由XXX⊤的d′ 个特征值组成的对角阵, 两边同时取矩阵的迹, 得\\ntr\\n\\x00W⊤XX⊤W\\n\\x01\\n= tr(Λ) =\\nd′\\nX', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 131, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='tr\\n\\x00W⊤XX⊤W\\n\\x01\\n= tr(Λ) =\\nd′\\nX\\ni=1\\nλi\\nd 个特征值, 因此当然是取出最大的前d′ 个特征值, 而W 即特征值对应的标准化特征向量组成的矩阵。\\n特别注意, 图10.5 只是得到了投影矩阵W, 而降维后的样本为Z = W⊤X。\\n10.6\\n核化线性降维\\n注意, 本节符号在第14 次印刷中进行了修订, 另外有一点需要注意的是，在上一节中用zi 表示xi 降\\n维后的像, 而本节用zi 表示xi 在高维特征空间中的像。\\n本节推导实际上有一个前提, 以式(10.19) 为例（式(10.21) 仅将zi 换为ϕ (xi) 而已), 那就是zi 已\\n经中心化(计算方差要用样本减去均值, 式(10.19) 是均值为零时特殊形式, 详见式(10.16) 的解释), 但\\nzi = ϕ (xi) 是xi 高维特征空间中的像, 即使xi 已进行中心化, 但zi 却不一定是中心化的, 此时本节推导\\n均不再成立。推广工作详见KPCA[3] 的附录A。\\n→_→\\n配套视频教程：https://www.bilibili.com/video/BV1Mh411e7VU\\n←_←', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 131, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='→_→\\n欢迎去各大电商平台选购纸质版南瓜书《机器学习公式详解第2 版》\\n←_←\\n10.6.1\\n式(10.19) 的解释\\n首先, 类似于式(10.14) 的推导后半部分内容可知Pm\\ni=1 ziz⊤\\ni = ZZ⊤, 其中Z 的每一列为一个样本, 设\\n高维空间的维度为h, 则Z ∈Rh×m, 其中m 为数据集样本数量。\\n其次, 式(10.19) 中的W 为从高维空间降至低维(维度为d ) 后的正交基, 在第14 次印刷中加入表述\\nW = (w1, w2, . . . , wd), 其中W ∈Rh×d, 降维过程为X = W⊤Z 。\\n最后, 式(10.19) 类似于式(10.17), 是为了求解降维投影矩阵W = (w1, w2, . . . , wd) 。但问题在于\\nZZ⊤∈Rh×h, 当维度h 很大时(注意本节为核化线性降维, 第六章核方法中高斯核会把样本映射至无穷\\n维), 此时根本无法求解Z⊤的特征值和特征向量。因此才有了后面的式(10.20)。\\n第14 次印刷及之后印次, 式(10.19) 为\\n\\x00Pm\\ni=1 ziz⊤\\ni\\n\\x01', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 132, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='第14 次印刷及之后印次, 式(10.19) 为\\n\\x00Pm\\ni=1 ziz⊤\\ni\\n\\x01\\nwj = λjwj, 而在之前的印次中表达有误, 实际应\\n该为\\n\\x00Pm\\ni=1 ziz⊤\\ni\\n\\x01\\nW = WΛ, 类似于式(10.17)。而这两种表达本质相同, λjwj 为WΛ 的第j 列, 仅此而\\n已。\\n10.6.2\\n式(10.20) 的解释\\n本节为核化线性降维, 而式(10.19) 是在维度为h 的高维空间运算, 式(10.20) 变形（咋一看似乎有点\\n无厘头) 的目的是为了避免直接在高维空间运算, 即想办法能够使用式(6.22) 的核技巧, 也就是后面的式\\n(10.24)。\\n第14 次印刷及之后印次该式没问题, 之前的式(10.20) 应该是：\\nW =\\n m\\nX\\ni=1\\nziz⊤\\ni\\n!\\nWΛ−1 =\\nm\\nX\\ni=1\\n\\x00zi\\n\\x00z⊤\\ni WΛ−1\\x01\\x01\\n=\\nm\\nX\\ni=1\\n(ziαi)\\n其中αi = z⊤\\ni WΛ−1 ∈R1×d, z⊤\\ni ∈R1×h, W ∈Rh×d, Λ ∈Rd×d 为对角阵。这个结果看似等号右侧也包含', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 132, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='i ∈R1×h, W ∈Rh×d, Λ ∈Rd×d 为对角阵。这个结果看似等号右侧也包含\\nW, 但将此式代入式(10.19) 后经化简可避免在高维空间的运算, 而将目标转化为求低维空间的αi ∈R1×d,\\n详见式(10.24) 的推导。\\n10.6.3\\n式(10.21) 的解释\\n该式即为将式(10.19) 中的zi 换为ϕ (xi) 的结果。\\n10.6.4\\n式(10.22) 的解释\\n该式即为将式(10.20) 中的zi 换为ϕ (xi) 的结果。\\n10.6.5\\n式(10.24) 的推导\\n已知zi = ϕ(xi)，类比X = {x1, x2, ..., xm} 可以构造Z = {z1, z2, ..., zm}，所以公式(10.21) 可变换\\n为\\n m\\nX\\ni=1\\nϕ(xi)ϕ(xi)T\\n!\\nwj =\\n m\\nX\\ni=1\\nzizT\\ni\\n!\\nwj = ZZTwj = λjwj\\n又由公式(10.22) 可知\\nwj =\\nm\\nX\\ni=1\\nϕ (xi) αj\\ni =\\nm\\nX\\ni=1\\nziαj\\ni = Zαj\\n其中，αj = (αj\\n1; αj\\n2; ...; αj', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 132, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='m\\nX\\ni=1\\nziαj\\ni = Zαj\\n其中，αj = (αj\\n1; αj\\n2; ...; αj\\nm) ∈Rm×1。所以公式(10.21) 可以进一步变换为\\nZZTZαj = λjZαj\\nZZTZαj = Zλjαj\\n→_→\\n配套视频教程：https://www.bilibili.com/video/BV1Mh411e7VU\\n←_←', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 132, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='→_→\\n欢迎去各大电商平台选购纸质版南瓜书《机器学习公式详解第2 版》\\n←_←\\n由于此时的目标是要求出wj，也就等价于要求出满足上式的αj，显然，此时满足ZTZαj = λjαj 的αj\\n一定满足上式，所以问题转化为了求解满足下式的αj：\\nZTZαj = λjαj\\n令ZTZ = K，那么上式可化为\\nKαj = λjαj\\n此式即为公式(10.24)，其中矩阵K 的第i 行第j 列的元素(K)ij = zT\\ni zj = ϕ(xi)Tϕ(xj) = κ (xi, xj)\\n10.6.6\\n式(10.25) 的解释\\n式(10.25) 仅需将第14 次印刷中式(10.22) 的wj 表达式转置后代入即可。\\n该式的意义在于, 求解新样本x ∈Rd×1 映射至高维空间ϕ(x) ∈Rh×1 后再降至低维空高维空间Rh×1\\n的运算。但是由于此处没有类似第6 章支持向量的概念, 可以发现式(10.25) 计算时需要对所有样本求和,\\n因此它的计算开销比较大。\\n注意, 此处书中符号使用略有混乱, 因为在式(10.19) 中zi 表示xi 在高维特征空间中的像, 而此处又', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 133, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='用zj 表示新样本x 映射为ϕ(x) 后再降维至Rd′×1 空间时的第j 维坐标。\\n10.7\\n流形学习\\n不要被“流形学习”的名字所欺骗, 本节开篇就明确说了, 它是一类借鉴了拓扑流形概念的降维方法而\\n已, 因此称为“流形学习”\\n。10.2 节MDS 算法的降维准则是要求原始空间中样本之间的距离在低维空间中\\n得以保持, 10.3 节PCA 算法的降维准则是要求低维子空间对样本具有最大可分性, 因为它们都是基于线\\n性变换来进行降维的方法（参见式(10.13)，故称为线性降维方法。\\n10.7.1\\n等度量映射(Isomap) 的解释\\n如图“西瓜书”10.8 所示, Isomap 算法与MDS 算法的区别仅在于距离矩阵D ∈Rm×m 的计算方法\\n不同。在MDS 算法中, 距离矩阵D ∈Rm×m 即为普通的样本之间欧氏距离; 而本节的Isomap 算法中, 距\\n离矩阵D ∈Rm×m 由“西瓜书”图10.8 的Step1 ~Step5 生成, 即遵循流形假设。当然, 对新样本降维时\\n也有不同, 这在“西瓜书”图10.8 下的一段话中已阐明。', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 133, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='也有不同, 这在“西瓜书”图10.8 下的一段话中已阐明。\\n另外解释一下测地线距离, 欧氏距离即两点之间的直线距离, 而测地线距离是实际中可以到达的路径,\\n如“西瓜书”图10.7(a) 中黑线(欧氏距离) 和红线(测地线距离)。\\n10.7.2\\n式(10.28) 的推导\\nwij =\\nP\\nk∈Qi\\nC−1\\njk\\nP\\nl,s∈Qi\\nC−1\\nls\\n由书中上下文可知，式(10.28) 是如下优化问题的解。\\nmin\\nw1,w2,...,wm\\nm\\nX\\ni=1\\n\\r\\n\\r\\n\\r\\n\\r\\n\\rxi −\\nX\\nj∈Qi\\nwijxj\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n2\\n2\\ns.t.\\nX\\nj∈Qi\\nwij = 1\\n→_→\\n配套视频教程：https://www.bilibili.com/video/BV1Mh411e7VU\\n←_←', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 133, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='→_→\\n欢迎去各大电商平台选购纸质版南瓜书《机器学习公式详解第2 版》\\n←_←\\n若令xi ∈Rd×1, Qi = {q1\\ni , q2\\ni , ..., qn\\ni }，则上述优化问题的目标函数可以进行如下恒等变形\\nm\\nX\\ni=1\\n\\r\\n\\r\\n\\r\\n\\r\\n\\rxi −\\nX\\nj∈Qi\\nwijxj\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n2\\n2\\n=\\nm\\nX\\ni=1\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\nX\\nj∈Qi\\nwijxi −\\nX\\nj∈Qi\\nwijxj\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n2\\n2\\n=\\nm\\nX\\ni=1\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\nX\\nj∈Qi\\nwij(xi −xj)\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n2\\n2\\n=\\nm\\nX\\ni=1\\n∥Xiwi∥2\\n2\\n=\\nm\\nX\\ni=1\\nwi\\nTXT\\ni Xiwi\\n其中wi = (wiq1\\ni , wiq2\\ni , ..., wiqn\\ni ) ∈Rn×1，Xi =\\n\\x00xi −xq1\\ni , xi −xq2\\ni , ..., xi −xqn\\ni\\n\\x01\\n∈Rd×n。同理，约束条件\\n也可以进行如下恒等变形\\nX\\nj∈Qi\\nwij = wi\\nTI = 1', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 134, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='∈Rd×n。同理，约束条件\\n也可以进行如下恒等变形\\nX\\nj∈Qi\\nwij = wi\\nTI = 1\\n其中I = (1, 1, ..., 1) ∈Rn×1 为n 行1 列的元素值全为1 的向量。因此，上述优化问题可以重写为\\nmin\\nw1,w2,...,wm\\nm\\nX\\ni=1\\nwi\\nTXT\\ni Xiwi\\ns.t. wi\\nTI = 1\\n显然，此问题为带约束的优化问题，因此可以考虑使用拉格朗日乘子法来进行求解。由拉格朗日乘子法可\\n得此优化问题的拉格朗日函数为\\nL(w1, w2, . . . , wm, λ) =\\nm\\nX\\ni=1\\nwi\\nTXT\\ni Xiwi + λ\\n\\x00wi\\nTI −1\\n\\x01\\n对拉格朗日函数关于wi 求偏导并令其等于0 可得\\n∂L(w1, w2, . . . , wm, λ)\\n∂wi\\n=\\n∂\\n\\x02Pm\\ni=1 wiTXT\\ni Xiwi + λ\\n\\x00wiTI −1\\n\\x01\\x03\\n∂wi\\n= 0\\n=\\n∂\\n\\x02\\nwiTXT\\ni Xiwi + λ\\n\\x00wiTI −1\\n\\x01\\x03\\n∂wi\\n= 0\\n又由矩阵微分公式\\n∂xT Bx\\n∂x\\n=\\n\\x00B + BT\\x01\\nx,\\n∂xT a\\n∂x\\n= a 可得\\n∂\\n\\x02\\nwiTXT', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 134, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='∂xT Bx\\n∂x\\n=\\n\\x00B + BT\\x01\\nx,\\n∂xT a\\n∂x\\n= a 可得\\n∂\\n\\x02\\nwiTXT\\ni Xiwi + λ\\n\\x00wiTI −1\\n\\x01\\x03\\n∂wi\\n= 2XT\\ni Xiwi + λI = 0\\nXT\\ni Xiwi = −1\\n2λI\\n若XT\\ni Xi 可逆，则\\nwi = −1\\n2λ(XT\\ni Xi)−1I\\n又因为wiTI = ITwi = 1，则上式两边同时左乘IT 可得\\nITwi = −1\\n2λIT(XT\\ni Xi)−1I = 1\\n−1\\n2λ =\\n1\\nIT(XT\\ni Xi)−1I\\n→_→\\n配套视频教程：https://www.bilibili.com/video/BV1Mh411e7VU\\n←_←', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 134, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='→_→\\n欢迎去各大电商平台选购纸质版南瓜书《机器学习公式详解第2 版》\\n←_←\\n将其代回wi = −1\\n2λ(XT\\ni Xi)−1I 即可解得\\nwi =\\n(XT\\ni Xi)−1I\\nIT(XT\\ni Xi)−1I\\n若令矩阵(XT\\ni Xi)−1 第j 行第k 列的元素为C−1\\njk ，则\\nwij = wiqj\\ni =\\nP\\nk∈Qi\\nC−1\\njk\\nP\\nl,s∈Qi\\nC−1\\nls\\n此即为公式(10.28)。显然，若XT\\ni Xi 可逆，此优化问题即为凸优化问题，且此时用拉格朗日乘子法求得的\\nwi 为全局最优解。\\n10.7.3\\n式(10.31) 的推导\\n以下推导需要使用预备知识中的10.2节：矩阵的F 范数与迹。\\n观察式(10.29), 求和号内实际是一个列向量的2 范数平方, 令vi = zi −P\\nj∈Qi wijzj, vi 的维度与zi\\n相同, vi ∈Rd′×1, 则式(10.29) 可重写为\\nmin\\nz1,z2,...,zm\\nm\\nX\\ni=1\\n∥vi∥2\\n2\\ns.t. vi = zi −\\nX\\nj∈Qi\\nwijzj, i = 1, 2, . . . , m', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 135, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='s.t. vi = zi −\\nX\\nj∈Qi\\nwijzj, i = 1, 2, . . . , m\\n令Z = (z1, z2, . . . , zi, . . . , zm) ∈Rd′×m, Ii = (0; 0; . . . ; 1; . . . ; 0) ∈Rm×1, 即Ii 为m × 1 的列向量, 除\\n第i 个元素等于1 之外其余元素均为零, 则\\nzi = ZIi\\n令(W)ij = wij （P237 页第1 行), 即W = (w1, w2, . . . , wi, . . . , wm)⊤∈Rm×m, 也就是说W 的第i 行的\\n转置（没错, 就是第i 行) 对应第i 个样数wi （这里符号之所以别扭是因为wij 已用来表示列向量wi 的\\n第j 个元素, 但为了与习惯保持一致即wij 表示W 的第i 行第j 列元素, 只能忍忍, 此处暂时别扭着）\\n，即\\nW = (w1, w2, . . . , wi, . . . , wm)⊤=\\n\\uf8ee\\n\\uf8ef\\n\\uf8ef\\n\\uf8ef\\n\\uf8ef\\n\\uf8ef\\n\\uf8ef\\n\\uf8ef\\n\\uf8ef\\n\\uf8ef\\n\\uf8ef\\n\\uf8f0\\nw11\\nw21\\n· · ·\\nwi1\\n· · ·\\nwm1\\nw12\\nw22\\n· · ·\\nwi2', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 135, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='\\uf8ef\\n\\uf8f0\\nw11\\nw21\\n· · ·\\nwi1\\n· · ·\\nwm1\\nw12\\nw22\\n· · ·\\nwi2\\n· · ·\\nwm2\\n.\\n.\\n.\\n.\\n.\\n.\\n...\\n.\\n.\\n.\\n...\\n.\\n.\\n.\\nw1j\\nw2j\\n· · ·\\nwij\\n· · ·\\nwmj\\n.\\n.\\n.\\n.\\n.\\n.\\n...\\n.\\n.\\n.\\n...\\n.\\n.\\n.\\nw1m\\nw2m\\n· · ·\\nwim\\n· · ·\\nwmm\\n\\uf8f9\\n\\uf8fa\\n\\uf8fa\\n\\uf8fa\\n\\uf8fa\\n\\uf8fa\\n\\uf8fa\\n\\uf8fa\\n\\uf8fa\\n\\uf8fa\\n\\uf8fa\\n\\uf8fb\\n⊤\\n对于wi ∈Rm×1 来说, 只有xi 的K 个近邻样本对应的下标对应的wij ̸= 0, j ∈Qi, 且它们的和等于\\n1 , 则\\nX\\nj∈Qi\\nwijzj = Zwi\\n因此\\nvi = zi −\\nX\\nj∈Qi\\nwijzj = ZIi −Zwi = Z (Ii −wi)\\n令V = (v1, v2, . . . , vi, . . . , vm) ∈Rd′×m, I = (I1, I2, . . . , Ii, . . . , Im) ∈Rm×m, 则\\nV = Z\\n\\x00I −W⊤\\x01\\n= Z\\n\\x00I⊤−W⊤\\x01\\n= Z(I −W)⊤\\n→_→', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 135, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='V = Z\\n\\x00I −W⊤\\x01\\n= Z\\n\\x00I⊤−W⊤\\x01\\n= Z(I −W)⊤\\n→_→\\n配套视频教程：https://www.bilibili.com/video/BV1Mh411e7VU\\n←_←', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 135, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='→_→\\n欢迎去各大电商平台选购纸质版南瓜书《机器学习公式详解第2 版》\\n←_←\\n根据前面的预备知识, 并将上式V 和式(10.30) 代入, 得式(10.31) 目标函数：\\nm\\nX\\ni=1\\n∥vi∥2\\n2 = ∥V∥2\\nF\\n= tr\\n\\x00VV⊤\\x01\\n= tr\\n\\x10\\x00Z(I −W)⊤\\x01 \\x00Z(I −W)⊤\\x01⊤\\x11\\n= tr\\n\\x00Z(I −W)⊤(I −W)Z⊤\\x01\\n= tr\\n\\x00ZMZZ⊤\\x01\\n接下来求解式(10.31)。\\n参考式(10.17) 的推导, 应用拉格朗日乘子法, 先写出拉格朗日函数\\nL(Z, Λ) = tr\\n\\x00ZMZ⊤\\x01\\n+\\n\\x00ZZ⊤−I\\n\\x01\\nΛ\\n令P = Z⊤(否则有点别扭), 则拉格朗日函数变为\\nL(P, Λ) = tr\\n\\x00P⊤MP\\n\\x01\\n+\\n\\x00P⊤P −I\\n\\x01\\n\\uffff\\n求导并令导数等于0 ：\\n∂L(P, Λ)\\n∂P\\n= ∂tr\\n\\x00P⊤MP\\n\\x01\\n∂P\\n+ ∂\\n\\x00P⊤P −I\\n\\x01\\n∂P\\nΛ\\n= 2MP −2PΛ = 0\\n特征值对角阵; 然后两边再同时左乘P⊤并取矩阵的迹, 注意P⊤P = I ∈Rd′×d′, 得tr\\n\\x00P⊤MP\\n\\x01\\n=\\ntr\\n\\x00P⊤PΛ\\n\\x01', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 136, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='\\x00P⊤MP\\n\\x01\\n=\\ntr\\n\\x00P⊤PΛ\\n\\x01\\n= tr(Λ) 因此, P = Z⊤是由M ∈Rm×m 最小的d′ 个特征值对应的特征向量组成的矩阵。\\n10.8\\n度量学习\\n回忆10.5.1 节的Isomap 算法相比与10.2 节的MDS 算法的区别在于距离矩阵的计算方法不同，\\nIsomap 算法在计算样本间距离时使用的（近似）测地线距离，而MDS 算法使用的是欧氏距离，也就是说\\n二者的距离度量不同。\\n10.8.1\\n式(10.34) 的解释\\n为了推导方便, 令u = (u1; u2; . . . ; ud) = xi −xj ∈Rd×1, 其中uk = xik −xjk, 则式(10.34) 重写为\\nu⊤Mu = ∥u∥2\\nM, 其中M ∈Rd×d, 具体到元素级别的表达：\\nu⊤Mu =\\nh\\nu1\\nu2\\n. . .\\nud\\ni\\n\\uf8ee\\n\\uf8ef\\n\\uf8ef\\n\\uf8ef\\n\\uf8ef\\n\\uf8ef\\n\\uf8f0\\nm11\\nm12\\n. . .\\nm1d\\nm21\\nm22\\n. . .\\nm2d\\n.\\n.\\n.\\n.\\n.\\n.\\n...\\n.\\n.\\n.\\nmd1\\nmd2\\n. . .\\nmdd\\n\\uf8f9\\n\\uf8fa\\n\\uf8fa\\n\\uf8fa\\n\\uf8fa\\n\\uf8fa\\n\\uf8fb\\n\\uf8ee\\n\\uf8ef\\n\\uf8ef\\n\\uf8ef\\n\\uf8ef\\n\\uf8ef\\n\\uf8f0\\nu1\\nu2\\n.\\n.\\n.\\nud\\n\\uf8f9', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 136, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='mdd\\n\\uf8f9\\n\\uf8fa\\n\\uf8fa\\n\\uf8fa\\n\\uf8fa\\n\\uf8fa\\n\\uf8fb\\n\\uf8ee\\n\\uf8ef\\n\\uf8ef\\n\\uf8ef\\n\\uf8ef\\n\\uf8ef\\n\\uf8f0\\nu1\\nu2\\n.\\n.\\n.\\nud\\n\\uf8f9\\n\\uf8fa\\n\\uf8fa\\n\\uf8fa\\n\\uf8fa\\n\\uf8fa\\n\\uf8fb\\n=\\nh\\nu1\\nu2\\n. . .\\nud\\ni\\n\\uf8ee\\n\\uf8ef\\n\\uf8ef\\n\\uf8ef\\n\\uf8ef\\n\\uf8ef\\n\\uf8f0\\nu1m11 + u2m12 + . . . + udm1d\\nu1m21 + u2m22 + . . . + udm2d\\n.\\n.\\n.\\nu1md1 + u2md2 + . . . + udmdd\\n\\uf8f9\\n\\uf8fa\\n\\uf8fa\\n\\uf8fa\\n\\uf8fa\\n\\uf8fa\\n\\uf8fb\\n= u1u1m11 + u1u2m12 + . . . + u1udm1d\\n+ u2u1m21 + u2u2m22 + . . . + u2udm2d\\n. . .\\n+ udu1md1 + udu2md2 + . . . + ududmdd\\n→_→\\n配套视频教程：https://www.bilibili.com/video/BV1Mh411e7VU\\n←_←', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 136, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='→_→\\n欢迎去各大电商平台选购纸质版南瓜书《机器学习公式详解第2 版》\\n←_←\\n注意, 对应到本式符号, 式(10.33) 的结果即为上面最后一个等式的对角线部分, 即\\nu1u1m11 + u2u2m22 + . . . + ududmdd\\n而式(10.32) 的结果则要更进一步, 去除对角线部分中的权重mii(1 ⩽i ⩽d) 部分, 即\\nu1u1 + u2u2 + . . . + udud\\n对比以上三个结果, 即式(10.32) 的平方欧氏距离, 式(10.33) 的加权平方欧氏距离, 式(10.34) 的马氏距\\n离, 可以细细体会度量矩阵究竟带来了什么。\\n因此, 所谓\\n“度量学习”\\n, 即将系统中的平方欧氏距离换为式(10.34) 的马氏距离, 通过优化某个目标函\\n数, 得到最恰当的度量矩阵M （新的距离度量计算方法）的过程。书中在式(10.34) (10.38) 介绍的NCA\\n即为一个具体的例子, 可以从中品味“度量学习”的本质。\\n对于度量矩阵M 要求半正定, 文中提到必有正交基P 使得M 能写为M = PP⊤, 此时马氏距离\\nu⊤Mu = u⊤PP⊤u =\\n\\r\\n\\rP⊤u', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 137, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='u⊤Mu = u⊤PP⊤u =\\n\\r\\n\\rP⊤u\\n\\r\\n\\r2\\n2 。\\n10.8.2\\n式(10.35) 的解释\\n这就是一种定义而已，\\n没什么别的意思。\\n传统近邻分类器使用多数投票法，\\n有投票权的样本为xi 最近\\n的K 个近邻, 即KNN; 但也可以将投票范围扩大到整个样本集, 但每个样本的投票权重不一样，\\n距离xi 越\\n近的样本投票权重越大，例如可取为第5 章式(5.19) 当βi = 1 时的高斯径向基函数exp\\n\\x10\\n−∥xi −xj∥2\\x11\\n。从式中可以看出, 若xj 与xi 重合, 则投票权重为1 , 距离越大该值越小。式(10.35) 的分母是对所有投\\n票值规一化至[0, 1] 范围, 使之为概率。\\n可能会有疑问：式(10.35) 分母求和变量l 是否应该包含xi 的下标即l = i ? 其实无所谓, 进一步说\\n其实是否进行规一化也无所谓, 熟悉KNN 的话就知道, 在预测时是比较各类投票数的相对大小, 各类样本\\n对xi 的投票权重的分母在式(10.35) 中相同, 因此不影响相对大小。\\n注意啊, 这里有计算投票权重时用到了距离度量, 所以可以进一步将其换为马氏距离, 通过优化某个目', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 137, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='注意啊, 这里有计算投票权重时用到了距离度量, 所以可以进一步将其换为马氏距离, 通过优化某个目\\n标(如式(10.38)）得到最优的度量矩阵M。\\n10.8.3\\n式(10.36) 的解释\\n先简单解释留一法(LOO), KNN 是选出样本xi 的在样本集中最近的K 个近邻, 而现在将范围扩大,\\n使用样本集中的所有样本进行投票, 每个样本的投票权重为式(10.35), 将各类样本的投票权重分别求和,\\n注意xi 自己的类别肯定与自己相同（现在是训练阶段, 还没到对末见样本的预测阶段, 训练集样本的类别\\n信息均已知), 但自己不能为自己投票吧, 所以要将自己除外, 即留一法。\\n假设训练集共有N 个类别, Ωn 表示第n 类样本的下标集合(1 ⩽n ⩽N), 对于样本xi 来说, 可以分\\n别计算N 个概率：\\npxi\\nn =\\nX\\nj∈Ωn\\npij, 1 ⩽n ⩽N\\n注意, 若样本xi 的类别为n∗, 则在根据上式计算pxi\\nn∗时要将xi 的下标去除, 即刚刚解释的留一法(自己\\n不能为自己投票)。pxi\\nn∗即为训练集将样本xi 预测为第n∗类的概率, 若pxi\\nn∗在所有的pxi', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 137, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='n∗即为训练集将样本xi 预测为第n∗类的概率, 若pxi\\nn∗在所有的pxi\\nn (1 ⩽n ⩽N) 中\\n最大, 则预测正确, 反之预测错误。\\n其中pxi\\nn∗即为式(10.36)。\\n10.8.4\\n式(10.37) 的解释\\n换为刚才式(10.36) 的符号, 式(10.37) 即为Pm\\ni=1 pxi\\nn∗, 也就是所有训练样本被训练集预测正确的概率\\n之和。我们当然希望这个概率和最大, 但若采用平方欧氏距离时, 对于某个训练集来说这个概率和是固定\\n的; 但若采用了马氏距离, 这个概率和与度量矩阵M 有关。\\n→_→\\n配套视频教程：https://www.bilibili.com/video/BV1Mh411e7VU\\n←_←', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 137, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='→_→\\n欢迎去各大电商平台选购纸质版南瓜书《机器学习公式详解第2 版》\\n←_←\\n10.8.5\\n式(10.38) 的解释\\n刚才式(10.37) 中提到希望寻找一个度量矩阵M 使训练样本被训练集预测正确的概率之和最大, 即\\nmaxM\\nPm\\ni=1 pxi\\nn∗, 但优化问题习惯是最小化, 所以改为minM −Pm\\ni=1 pxi\\nn∗即可, 而式(10.38) 目标函数中的\\n常数1 并不影响优化结果, 有没有无所谓的。\\n式(10.38) 中有关将M = PP⊤代入的形式参见前面式(10.34) 的解释最后一段。\\n10.8.6\\n式(10.39) 的解释\\n式(10.39) 是本节第二个“度量学习”的具体例子。优化目标函数是要求必连约束集合M 中的样本\\n对之间的距离之和尽可能的小，而约束条件则是要求勿连约束集合C 中的样本对之间的距离之和大于1 。\\n这里的“1”应该类似于第6 章SVM 中间隔大于“1”, 纯属约定, 没有推导。\\n参考文献\\n[1] Michael Grant. Lagrangian optimization with matrix constrains, 2015.', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 138, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='[2] Wikipedia contributors. Frobenius inner product, 2020.\\n[3] Bernhard Schölkopf, Alexander Smola, and Klaus-Robert Müller. Kernel principal component analy-\\nsis. In Artificial Neural Networks—ICANN’97: 7th International Conference Lausanne, Switzerland,\\nOctober 8–10, 1997 Proceeedings, pages 583–588. Springer, 2005.\\n→_→\\n配套视频教程：https://www.bilibili.com/video/BV1Mh411e7VU\\n←_←', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 138, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='→_→\\n欢迎去各大电商平台选购纸质版南瓜书《机器学习公式详解第2 版》\\n←_←\\n第11 章\\n特征选择与稀疏学习\\n11.1\\n子集搜索与评价\\n开篇给出了“特征选择”的概念, 并谈到特征选择与第10 章的降维有相似的动机。特征选择与降维的\\n区别在于特征选择是从所有特征中简单地选出相关特征, 选择出来的特征就是原来的特征; 降维则对原来\\n的特征进行了映射变换, 降维后的特征均不再是原来的特征。\\n本节涉及“子集评价”的式(14.1) 和式(14.2) 与第4 章的式(4.2) 和式(4.1) 相同, 这是因为“决策\\n树算法在构建树的同时也可看作进行了特征选择”(参见“11.7 阅读材料”)。接下来在11.2 节、11.3 节、\\n11.4 节分别介绍的三类特征选择方法: 过滤式(filter)、包裹式(wrapper) 和嵌入式(embedding)。\\n11.1.1\\n式(11.1) 的解释\\n此为信息增益的定义式，对数据集D 和属性子集A, 假设根据A 的取值将D 分为了V 个子集\\n\\x08\\nD1, D2, . . . , DV', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 139, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='\\x08\\nD1, D2, . . . , DV \\t\\n, 那么信息增益的定义为划分之前数据集D 的信息\\x00和划分之后每个子数据集Dv 的信\\n息\\x00的差。\\x00用来衡量一个系统的混舌程度, 因此划分前和划分后\\x00的差越大, 表示划分越有效, 划分带来\\n的”信息增益“越大。\\n11.1.2\\n式(11.2) 的解释\\nEnt(D) = −\\n|Y|\\nX\\ni=1\\npk log2 pk\\n此为信息熵的定义式，其中pk, k = 1, 2, . . . |Y| 表示D 中第i 类样本所占的比例。可以看出，样本越纯，\\n即pk →0 或pk →1 时，Ent(D) 越小，其最小值为0。此时必有pi = 1, p\\\\i = 0, i = 1, 2, . . . , |Y|。\\n11.2\\n过滤式选择\\n“过滤式方法先对数据集进行特征选择, 然后再训练学习器, 特征选择过程与后续学习器无关。这相当\\n于先用特征选择过程对初始特征进行’ 过滤’, 再用过滤后的特征来训练模型。\\n”, 这是本节开篇第一段原话,\\n之所以重写于此, 是因为这段话里包含了“过滤”的概念, 该概念并非仅针对特征选择, 那些所有先对数据', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 139, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='集进行某些预处理, 然后基于预处理结果再训练学习器的方法(预处理过程独立于训练学习器过程) 均可\\n以称之为“过滤式算法”\\n。特别地, 本节介绍的Relief 方法只是过滤式特征选择方法的其中一种而已。\\n从式(11.3) 可以看出, Relief 方法本质上基于“空间上相近的样本具有相近的类别标记”假设。Relief\\n基于样本与同类和异类的最近邻之间的距离来计算相关统计量δj, 越是满足前提假设, 原则上样本与同类\\n最近邻之间的距离diff\\n\\x00xj\\ni, xj\\ni,nh\\n\\x012 会越小, 样本与异类最近邻之间的距离diff\\n\\x00xj\\ni, xj\\ni, nm\\n\\x012 会越大，因此相\\n关统计量δj 越大，对应属性的分类能力就越强。\\n对于能处理多分类问题的扩展变量Relief-F, 由于有多个异类, 因此对所有异类最近邻进行加权平均,\\n各异类的权重为其在数据集中所占的比例。\\n11.2.1\\n包裹式选择\\n“与过滤式特征选择不考虑后续学习器不同, 包裹式特征选择直接把最终将要使用的学习器的性能作\\n为特征子集的评价准则。换言之, 包裹式特征选择的目的就是为给定学习器选择最有利于其性能、\\n“量身定', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 139, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='为特征子集的评价准则。换言之, 包裹式特征选择的目的就是为给定学习器选择最有利于其性能、\\n“量身定\\n做’ 的特征子集。\\n”, 这是本节开篇第一段原话, 之所以重写于此, 是因为这段话里包含了“包裹”的概念,\\n该概念并非仅针对特征选择, 那些所有基于学习器的性能作为评价准则对数据集进行预处理的方法(预处\\n理过程依赖训练所得学习器的测试性能）均可以称之为“包裹式算法”\\n。特别地, 本节介绍的LVW 方法只\\n是包裹式特征选择方法的其中一种而已。\\n→_→\\n配套视频教程：https://www.bilibili.com/video/BV1Mh411e7VU\\n←_←', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 139, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='→_→\\n欢迎去各大电商平台选购纸质版南瓜书《机器学习公式详解第2 版》\\n←_←\\n图11.1 中, 第1 行E = ∞表示初始化学习器误差为无穷大, 以至于第1 轮迭代第9 行的条件就一定\\n为真; 第2 行d = |A| 中的|A| 表示特征集A 的包含的特征个数; 第9 行E′ < E 表示学习器L 在特征子\\n集A′ 上的误差比当前特征子集A 上的误差更小, (E′ = E) ∨(d′ < d) 表示学习器L 在特征子集A′ 上的\\n误差与当前特征子集A 上的误差相当但A′ 中包含的特征数更小; 表示“逻辑与”\\n，V 表示“逻辑或”\\n。注\\n意到, 第5 行至第17 行的while 循环中t 并非一直增加, 当第9 行条件满足时t 会被清零。\\n最后, 本节LVW 算法基于拉斯维加斯方法框架, 可以仔细琢磨体会拉斯维加斯方法和蒙特卡罗方法\\n的区别。一个通俗的解释如下：\\n蒙特卡罗算法——采样越多, 越近似最优解；\\n拉斯维加斯算法——采样越多, 越有机会找到最优解。\\n举个例子, 假如筐里有100 个苹果, 让我每次闭眼拿1 个, 挑出最大的。于是我随机拿1 个，再随机拿', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 140, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='1 个跟它比，留下大的，再随机拿1 个...... 我每拿一次，留下的苹果都至少不比上次的小。拿的次数越多,\\n挑出的苹果就越大, 但我除非拿100 次, 否则无法肯定挑出了最大的。这个挑苹果的算法, 就属于蒙特卡罗\\n算法一一尽量找好的, 但不保证是最好的。而拉斯维加斯算法, 则是另一种情况。假如有一把锁, 给我100\\n把钥匙, 只有1 把是对的。于是我每次随机拿1 把钥匙去试, 打不开就再换1 把。我试的次数越多, 打开\\n(最优解) 的机会就越大, 但在打开之前, 那些错的钥匙都是没有用的。这个试钥匙的算法, 就是拉斯维加斯\\n的一一尽量找最好的, 但不保证能找到。\\n11.3\\n嵌入式选择与L1 正则化\\n“嵌入式特征选择是将特征选择过程与学习器训练过程融为一体，两者在同一个优化过程中完成，即\\n在学习器训练过程中自动地进行了特征选择。\\n”\\n，具体可以对比本节式(11.7) 的例子与前两节方法的本质\\n区别，细细体会本节第一段的这句有关“嵌入式”的概念描述。\\n11.3.1\\n式(11.5) 的解释\\n该式为线性回归的优化目标式，yi 表示样本i 的真实值，而w⊤xi 表示其预测值，这里使用预测值和', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 140, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='该式为线性回归的优化目标式，yi 表示样本i 的真实值，而w⊤xi 表示其预测值，这里使用预测值和\\n真实值差的平方衡量预测值偏离真实值的大小。\\n11.3.2\\n式(11.6) 的解释\\n该式为加入了L2 正规化项的优化目标，也叫“岭回归”\\n，λ 用来调节误差项和正规化项的相对重要性，\\n引入正规化项的目的是为了防止w 的分量过大而导致过拟合的风险。\\n11.3.3\\n式(11.7) 的解释\\n该式将11.6 中的L2 正规化项替换成了L1 正规化项，也叫LASSO 回归。关于L2 和L1 两个正规化\\n项的区别，\\n“西瓜书”图11.2 给出了很形象的解释。具体来说，结合L1 范数优化的模型参数分量取值尽\\n量稀疏，即非零分量个数尽量小，因此更容易取得稀疏解。\\n11.3.4\\n式(11.8) 的解释\\n从本式开始至本节结束, 都在介绍近端梯度下降求解L1 正则化问题。若将本式对应到式(11.7), 则本\\n式中f(w) = Pm\\ni=1\\n\\x00yi −w⊤xi\\n\\x012, 注意变量为w （若感觉不习惯就将其用x 替换好了)。最终推导结果仅\\n含f(w) 的一阶导数∇f(w) = −Pm\\ni=1 2\\n\\x00yi −w⊤xi', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 140, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='含f(w) 的一阶导数∇f(w) = −Pm\\ni=1 2\\n\\x00yi −w⊤xi\\n\\x01\\nxi 。\\n→_→\\n配套视频教程：https://www.bilibili.com/video/BV1Mh411e7VU\\n←_←', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 140, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='→_→\\n欢迎去各大电商平台选购纸质版南瓜书《机器学习公式详解第2 版》\\n←_←\\n11.3.5\\n式(11.9) 的解释\\n该式即为L-Lipschitz(利普希茨) 条件的定义。\\n简单来说, 该条件约束函数的变化不能太快。\\n将式(11.9)\\n变形则更为直观(注: 式中应该是2 范数, 而非2 范数平方):\\n∥∇f (x′) −∇f(x)∥2\\n∥x′ −x∥2\\n⩽L,\\n(∀x, x′)\\n进一步地, 若x′ →x, 即\\nlim\\nx′→x\\n∥∇f (x′) −∇f(x)∥2\\n∥x′ −x∥2\\n这明显是在求解函数∇f(x) 的导数绝对值（模值。因此, 式(11.9) 即要求f(x) 的二阶导数不大于L, 其\\n中L 称为Lipschitz 常数。\\n“Lipschitz 连续”可以形象得理解为: 以陆地为例, Lipschitz 连续就是说这块地上没有特别陡的坡; 其\\n中最陡的地方有多陡呢? 这就是所谓的Lipschitz 常数。\\n11.3.6\\n式(11.10) 的推导\\n首先注意优化目标式和11.7 LASSO 回归的联系和区别，该式中的x 对应到式11.7 的w，即我们优', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 141, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='化的目标。再解释下什么是L−Lipschitz 条件，根据维基百科的定义：它是一个比通常连续更强的光滑性\\n条件。直觉上，利普希茨连续函数限制了函数改变的速度，符合利普希茨条件的函数的斜率，必小于一个\\n称为利普希茨常数的实数（该常数依函数而定）\\n。注意这里存在一个笔误，在wiki 百科的定义中，式11.9\\n应该写成\\n|∇f (x′) −∇f(x)| ⩽L |x′ −x|\\n(∀x, x′)\\n移项得\\n|∇f (x′) −∇f(x)|\\n|x′ −x|\\n⩽L\\n(∀x, x′)\\n由于上式对所有的x, x′ 都成立，由导数的定义，上式可以看成是f(x) 的二阶导数恒不大于L。即\\n∇2f(x) ⩽L\\n得到这个结论之后，我们来推导式11.10。由泰勒公式，xk 附近的f(x) 通过二阶泰勒展开式可近似为\\nˆ\\nf(x) ≃f (xk) + ⟨∇f (xk) , x −xk⟩+ ∇2f(xk)\\n2\\n∥x −xk∥2\\n⩽f (xk) + ⟨∇f (xk) , x −xk⟩+ L\\n2 ∥x −xk∥2\\n= f (xk) + ∇f (xk)⊤(x −xk) + L\\n2 (x −xk)⊤(x −xk)', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 141, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='= f (xk) + ∇f (xk)⊤(x −xk) + L\\n2 (x −xk)⊤(x −xk)\\n= f(xk) + L\\n2\\n\\x12\\n(x −xk)⊤(x −xk) + 2\\nL∇f (xk)⊤(x −xk)\\n\\x13\\n= f(xk) + L\\n2\\n\\x12\\n(x −xk)⊤(x −xk) + 2\\nL∇f (xk)⊤(x −xk) + 1\\nL2 ∇f(xk)⊤∇f(xk)\\n\\x13\\n−1\\n2L∇f(xk)⊤∇f(xk)\\n= f(xk) + L\\n2\\n\\x12\\n(x −xk) + 1\\nL∇f (xk)\\n\\x13⊤\\x12\\n(x −xk) + 1\\nL∇f (xk)\\n\\x13\\n−1\\n2L∇f(xk)⊤∇f(xk)\\n= L\\n2\\n\\r\\n\\r\\n\\r\\n\\rx −\\n\\x12\\nxk −1\\nL∇f (xk)\\n\\x13\\r\\n\\r\\n\\r\\n\\r\\n2\\n2\\n+ const\\n其中const = f(xk) −\\n1\\n2L∇f (xk)⊤∇f (xk)\\n11.3.7\\n式(11.11) 的解释\\n这个很容易理解，因为2 范数的最小值为0，当xk+1 = xk −1\\nL∇f (xk) 时，ˆ\\nf(xk+1) ⩽ˆ\\nf(xk) 恒成立，\\n同理ˆ\\nf(xk+2) ⩽ˆ', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 141, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='L∇f (xk) 时，ˆ\\nf(xk+1) ⩽ˆ\\nf(xk) 恒成立，\\n同理ˆ\\nf(xk+2) ⩽ˆ\\nf(xk+1), · · · ，因此反复迭代能够使ˆ\\nf(x) 的值不断下降。\\n→_→\\n配套视频教程：https://www.bilibili.com/video/BV1Mh411e7VU\\n←_←', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 141, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='→_→\\n欢迎去各大电商平台选购纸质版南瓜书《机器学习公式详解第2 版》\\n←_←\\n11.3.8\\n式(11.12) 的解释\\n注意ˆ\\nf(x) 在式(11.11) 处取得最小值, 因此, 以下不等式肯定成立:\\nˆ\\nf\\n\\x12\\nxk −1\\nL∇f (xk)\\n\\x13\\n⩽ˆ\\nf (xk)\\n在式(11.10) 推导中有f(x) ⩽ˆ\\nf(x) 恒成立, 因此, 以下不等式肯定成立:\\nf\\n\\x12\\nxk −1\\nL∇f (xk)\\n\\x13\\n⩽ˆ\\nf\\n\\x12\\nxk −1\\nL∇f (xk)\\n\\x13\\n在式(11.10) 推导中还知道f (xk) = ˆ\\nf (xk), 因此\\nf\\n\\x12\\nxk −1\\nL∇f (xk)\\n\\x13\\n⩽ˆ\\nf\\n\\x12\\nxk −1\\nL∇f (xk)\\n\\x13\\n⩽ˆ\\nf (xk) = f (xk)\\n也就是说通过迭代xk+1 = xk −1\\nL∇f (xk) 可以使f(x) 的函数值逐步下降。\\n同理, 对于函数g(x) = f(x) + λ∥x∥1, 可以通过最小化ˆ\\ng(x) = ˆ\\nf(x) + λ∥x∥1 逐步求解。式(11.12) 就\\n是在最小化ˆ\\ng(x) = ˆ\\nf(x) + λ∥x∥1◦。', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 142, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='是在最小化ˆ\\ng(x) = ˆ\\nf(x) + λ∥x∥1◦。\\n以上优化方法被称为Majorization-Minimization。可以搜索相关资料做详细了解。\\n11.3.9\\n式(11.13) 的解释\\n这里将式11.12 的优化步骤拆分成了两步，\\n首先令z = xk −1\\nL∇f (xk) 以计算z，\\n然后再求解式11.13，\\n得到的结果是一致的。\\n11.3.10\\n式(11.14) 的推导\\n令优化函数\\ng(x) = L\\n2 ∥x −z∥2\\n2 + λ∥x∥1\\n= L\\n2\\nd\\nX\\ni=1\\n\\r\\n\\rxi −zi\\r\\n\\r2\\n2 + λ\\nd\\nX\\ni=1\\n\\r\\n\\rxi\\r\\n\\r\\n1\\n=\\nd\\nX\\ni=1\\n\\x12L\\n2\\n\\x00xi −zi\\x012 + λ\\n\\x0c\\n\\x0cxi\\x0c\\n\\x0c\\n\\x13\\n这个式子表明优化g(x) 可以被拆解成优化x 的各个分量的形式，对分量xi，其优化函数\\ng\\n\\x00xi\\x01\\n= L\\n2\\n\\x00xi −zi\\x012 + λ\\n\\x0c\\n\\x0cxi\\x0c\\n\\x0c\\n求导得\\ndg (xi)\\ndxi\\n= L\\n\\x00xi −zi\\x01\\n+ λsgn\\n\\x00xi\\x01\\n其中\\nsign\\n\\x00xi\\x01\\n=\\n(\\n1,\\nxi > 0\\n−1,\\nxi < 0', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 142, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='+ λsgn\\n\\x00xi\\x01\\n其中\\nsign\\n\\x00xi\\x01\\n=\\n(\\n1,\\nxi > 0\\n−1,\\nxi < 0\\n称为符号函数[1]，对于xi = 0 的特殊情况，由于|xi| 在xi = 0 点出不光滑，所以其不可导，需单独\\n讨论。令\\ndg(xi)\\ndxi\\n= 0 有\\nxi = zi −λ\\nL sign\\n\\x00xi\\x01\\n此式的解即为优化目标g(xi) 的极值点，因为等式两端均含有未知变量xi，故分情况讨论。\\n→_→\\n配套视频教程：https://www.bilibili.com/video/BV1Mh411e7VU\\n←_←', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 142, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='→_→\\n欢迎去各大电商平台选购纸质版南瓜书《机器学习公式详解第2 版》\\n←_←\\n1. 当zi > λ\\nL 时：a. 假设xi < 0，则sign(xi) = −1，那么有xi = zi + λ\\nL > 0 与假设矛盾；b. 假设\\nxi > 0，则sign(xi) = 1，那么有xi = zi −λ\\nL > 0 和假设相符和，下面来检验xi = zi −λ\\nL 是否是使\\n函数g(xi) 的取得最小值。当xi > 0 时，\\ndg (xi)\\ndxi\\n= L\\n\\x00xi −zi\\x01\\n+ λ\\n在定义域内连续可导，则g(xi) 的二阶导数\\nd2g (xi)\\ndxi2\\n= L\\n由于L 是Lipschitz 常数恒大于0，因为xi = zi −λ\\nL 是函数g(xi) 的最小值。\\n2. 当zi < −λ\\nL 时：a. 假设xi > 0，则sign(xi) = 1，那么有xi = zi −λ\\nL < 0 与假设矛盾；b. 假设\\nxi < 0，则sign(xi) = −1，那么有xi = zi + λ\\nL < 0 与假设相符，由上述二阶导数恒大于0 可知，\\nxi = zi + λ\\nL 是g(xi) 的最小值。', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 143, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='xi = zi + λ\\nL 是g(xi) 的最小值。\\n3. 当−λ\\nL ⩽zi ⩽λ\\nL 时：a. 假设xi > 0，则sign(xi) = 1，那么有xi = zi −λ\\nL ⩽0 与假设矛盾；b. 假设\\nxi < 0，则sign(xi) = −1，那么有xi = zi + λ\\nL ⩾0 与假设矛盾。\\n4. 最后讨论xi = 0 的情况，此时g(xi) = L\\n2 (zi)\\n2\\n• 当|zi| > λ\\nL 时，由上述推导可知g(xi) 的最小值在xi = zi −λ\\nL 处取得，因为\\ng(xi)|xi=0 −g(xi)|xi=zi−λ\\nL = L\\n2\\n\\x00zi\\x012 −\\n\\x12\\nλzi −λ2\\n2L\\n\\x13\\n= L\\n2\\n\\x12\\nzi −λ\\nL\\n\\x132\\n> 0\\n因此当|zi| > λ\\nL 时，xi = 0 不会是函数g(xi) 的最小值。\\n• 当−λ\\nL ⩽zi ⩽λ\\nL 时，对于任何∆x ̸= 0 有\\ng(∆x) = L\\n2\\n\\x00∆x −zi\\x012 + λ|∆x|\\n= L\\n2\\n\\x12\\n(∆x)2 −2∆x · zi + 2λ\\nL |∆x|\\n\\x13\\n+ L\\n2\\n\\x00zi\\x012\\n≥L\\n2\\n\\x12', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 143, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='(∆x)2 −2∆x · zi + 2λ\\nL |∆x|\\n\\x13\\n+ L\\n2\\n\\x00zi\\x012\\n≥L\\n2\\n\\x12\\n(∆x)2 −2∆x · zi + 2λ\\nL ∆x\\n\\x13\\n+ L\\n2\\n\\x00zi\\x012\\n≥L\\n2 (∆x)2 + L\\n2\\n\\x00zi\\x012\\n> g(xi)|xi=0\\n因此xi = 0 是g(xi) 的最小值点。\\n综上所述，11.14 成立。\\n该式称为软阈值(Soft Thresholding) 函数，很常见，建议掌握。另外，常见的变形是式(11.13) 中的\\nL = 1 或L = 2 时的形式，其解直接代入式(11.14) 即可。与软阈值函数相对的是硬阈值函数，是将式\\n(11.13) 中的1 范数替换为0 范数的优化问题的闭式解。\\n11.4\\n稀疏表示与字典学习\\n稀疏表示与字典学习实际上是信号处理领域的概念。本节内容核心就是K-SVD 算法。\\n→_→\\n配套视频教程：https://www.bilibili.com/video/BV1Mh411e7VU\\n←_←', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 143, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='→_→\\n欢迎去各大电商平台选购纸质版南瓜书《机器学习公式详解第2 版》\\n←_←\\n11.4.1\\n式(11.15) 的解释\\n这个式子表达的意思很容易理解，即希望样本xi 的稀疏表示αi 通过字典B 重构后和样本xi 的原始\\n表示尽量相似，如果满足这个条件，那么稀疏表示αi 是比较好的。后面的1 范数项是为了使表示更加稀\\n疏。\\n11.4.2\\n式(11.16) 的解释\\n为了优化11.15，我们采用变量交替优化的方式(有点类似EM 算法)，首先固定变量B，则11.15 求\\n解的是m 个样本相加的最小值，因为公式里没有样本之间的交互(即文中所述αu\\ni αv\\ni (u ̸= v) 这样的形式)，\\n因此可以对每个变量做分别的优化求出αi，求解方法见式(11.13)，式(11.14)。\\n11.4.3\\n式(11.17) 的推导\\n这是优化11.15 的第二步，\\n固定住αi, i = 1, 2, . . . , m，\\n此时式11.15 的第二项为一个常数，\\n优化11.15\\n即优化minB\\nPm\\ni=1 ∥xi −Bαi∥2\\n2。其写成矩阵相乘的形式为minB ∥X −BA∥2\\n2，将2 范数扩展到F 范数即', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 144, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='2。其写成矩阵相乘的形式为minB ∥X −BA∥2\\n2，将2 范数扩展到F 范数即\\n得优化目标为minB ∥X −BA∥2\\nF。\\n11.4.4\\n式(11.18) 的推导\\n这个公式难点在于推导BA = Pk\\nj=1 bjαj。大致的思路是bjαj 会生成和矩阵BA 同样维度的矩阵，\\n这个矩阵对应位置的元素是BA 中对应位置元素的一个分量，这样的分量矩阵一共有k 个，把所有分量\\n矩阵加起来就得到了最终结果。推导过程如下：\\nBA =\\n\\uf8ee\\n\\uf8ef\\n\\uf8ef\\n\\uf8ef\\n\\uf8ef\\n\\uf8ef\\n\\uf8ef\\n\\uf8ef\\n\\uf8ef\\n\\uf8ef\\n\\uf8f0\\nb1\\n1\\nb1\\n2\\n·\\n·\\n·\\nb1\\nk\\nb2\\n1\\nb2\\n2\\n·\\n·\\n·\\nb2\\nk\\n·\\n·\\n·\\n·\\n·\\n·\\n·\\n·\\n·\\n·\\n·\\n·\\nbd\\n1\\nbd\\n2\\n·\\n·\\n·\\nbd\\nk\\n\\uf8f9\\n\\uf8fa\\n\\uf8fa\\n\\uf8fa\\n\\uf8fa\\n\\uf8fa\\n\\uf8fa\\n\\uf8fa\\n\\uf8fa\\n\\uf8fa\\n\\uf8fb\\nd×k\\n·\\n\\uf8ee\\n\\uf8ef\\n\\uf8ef\\n\\uf8ef\\n\\uf8ef\\n\\uf8ef\\n\\uf8ef\\n\\uf8ef\\n\\uf8ef\\n\\uf8ef\\n\\uf8f0\\nα1\\n1\\nα1\\n2\\n·\\n·\\n·\\nα1\\nm\\nα2\\n1\\nα2\\n2\\n·\\n·\\n·\\nα2\\nm\\n·\\n·\\n·\\n·\\n·\\n·\\n·\\n·\\n·\\n·\\n·\\n·\\nαk\\n1\\nαk\\n2\\n·\\n·\\n·\\nαk\\nm\\n\\uf8f9\\n\\uf8fa\\n\\uf8fa\\n\\uf8fa\\n\\uf8fa\\n\\uf8fa\\n\\uf8fa\\n\\uf8fa\\n\\uf8fa\\n\\uf8fa\\n\\uf8fb\\nk×m\\n=\\n\\uf8ee\\n\\uf8ef\\n\\uf8ef\\n\\uf8ef', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 144, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='2\\n·\\n·\\n·\\nαk\\nm\\n\\uf8f9\\n\\uf8fa\\n\\uf8fa\\n\\uf8fa\\n\\uf8fa\\n\\uf8fa\\n\\uf8fa\\n\\uf8fa\\n\\uf8fa\\n\\uf8fa\\n\\uf8fb\\nk×m\\n=\\n\\uf8ee\\n\\uf8ef\\n\\uf8ef\\n\\uf8ef\\n\\uf8ef\\n\\uf8ef\\n\\uf8ef\\n\\uf8ef\\n\\uf8ef\\n\\uf8ef\\n\\uf8f0\\nPk\\nj=1 b1\\njαj\\n1\\nPk\\nj=1 b1\\njαj\\n2\\n·\\n·\\n·\\nPk\\nj=1 b1\\njαj\\nm\\nPk\\nj=1 b2\\njαj\\n1\\nPk\\nj=1 b2\\njαj\\n2\\n·\\n·\\n·\\nPk\\nj=1 b2\\njαj\\nm\\n·\\n·\\n·\\n·\\n·\\n·\\n·\\n·\\n·\\n·\\n·\\n·\\nPk\\nj=1 bd\\njαj\\n1\\nPk\\nj=1 bd\\njαj\\n2\\n·\\n·\\n·\\nPk\\nj=1 bd\\njαj\\nm\\n\\uf8f9\\n\\uf8fa\\n\\uf8fa\\n\\uf8fa\\n\\uf8fa\\n\\uf8fa\\n\\uf8fa\\n\\uf8fa\\n\\uf8fa\\n\\uf8fa\\n\\uf8fb\\nd×m\\n→_→\\n配套视频教程：https://www.bilibili.com/video/BV1Mh411e7VU\\n←_←', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 144, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='→_→\\n欢迎去各大电商平台选购纸质版南瓜书《机器学习公式详解第2 版》\\n←_←\\nbjαj =\\n\\uf8ee\\n\\uf8ef\\n\\uf8ef\\n\\uf8ef\\n\\uf8ef\\n\\uf8ef\\n\\uf8ef\\n\\uf8ef\\n\\uf8ef\\n\\uf8ef\\n\\uf8f0\\nb1\\nj\\nb2\\nj\\n·\\n·\\n·\\nbd\\nj\\n\\uf8f9\\n\\uf8fa\\n\\uf8fa\\n\\uf8fa\\n\\uf8fa\\n\\uf8fa\\n\\uf8fa\\n\\uf8fa\\n\\uf8fa\\n\\uf8fa\\n\\uf8fb\\n·\\nh\\nαj\\n1\\nαj\\n2\\n·\\n·\\n·\\nαj\\nm\\ni\\n=\\n\\uf8ee\\n\\uf8ef\\n\\uf8ef\\n\\uf8ef\\n\\uf8ef\\n\\uf8ef\\n\\uf8ef\\n\\uf8ef\\n\\uf8ef\\n\\uf8ef\\n\\uf8f0\\nb1\\njαj\\n1\\nb1\\njαj\\n2\\n·\\n·\\n·\\nb1\\njαj\\nm\\nb2\\njαj\\n1\\nb2\\njαj\\n2\\n·\\n·\\n·\\nb2\\njαj\\nm\\n·\\n·\\n·\\n·\\n·\\n·\\n·\\n·\\n·\\n·\\n·\\n·\\nbd\\njαj\\n1\\nbd\\njαj\\n2\\n·\\n·\\n·\\nbd\\njαj\\nm\\n\\uf8f9\\n\\uf8fa\\n\\uf8fa\\n\\uf8fa\\n\\uf8fa\\n\\uf8fa\\n\\uf8fa\\n\\uf8fa\\n\\uf8fa\\n\\uf8fa\\n\\uf8fb\\nd×m\\n求和可得：\\nk\\nX\\nj=1\\nbjαj =\\nk\\nX\\nj=1\\n\\uf8eb\\n\\uf8ec\\n\\uf8ec\\n\\uf8ec\\n\\uf8ec\\n\\uf8ec\\n\\uf8ec\\n\\uf8ec\\n\\uf8ec\\n\\uf8ec\\n\\uf8ed\\n\\uf8ee\\n\\uf8ef\\n\\uf8ef\\n\\uf8ef\\n\\uf8ef\\n\\uf8ef\\n\\uf8ef\\n\\uf8ef\\n\\uf8ef\\n\\uf8ef\\n\\uf8f0\\nb1\\nj\\nb2\\nj\\n·\\n·\\n·\\nbd\\nj\\n\\uf8f9\\n\\uf8fa\\n\\uf8fa\\n\\uf8fa\\n\\uf8fa\\n\\uf8fa\\n\\uf8fa\\n\\uf8fa\\n\\uf8fa\\n\\uf8fa\\n\\uf8fb\\n·\\nh\\nαj\\n1\\nαj\\n2\\n·\\n·\\n·\\nαj\\nm\\ni\\n\\uf8f6\\n\\uf8f7\\n\\uf8f7\\n\\uf8f7\\n\\uf8f7\\n\\uf8f7\\n\\uf8f7\\n\\uf8f7\\n\\uf8f7\\n\\uf8f7\\n\\uf8f8\\n=\\n\\uf8ee\\n\\uf8ef\\n\\uf8ef\\n\\uf8ef\\n\\uf8ef\\n\\uf8ef\\n\\uf8ef\\n\\uf8ef\\n\\uf8ef\\n\\uf8ef\\n\\uf8f0', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 145, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='m\\ni\\n\\uf8f6\\n\\uf8f7\\n\\uf8f7\\n\\uf8f7\\n\\uf8f7\\n\\uf8f7\\n\\uf8f7\\n\\uf8f7\\n\\uf8f7\\n\\uf8f7\\n\\uf8f8\\n=\\n\\uf8ee\\n\\uf8ef\\n\\uf8ef\\n\\uf8ef\\n\\uf8ef\\n\\uf8ef\\n\\uf8ef\\n\\uf8ef\\n\\uf8ef\\n\\uf8ef\\n\\uf8f0\\nPk\\nj=1 b1\\njαj\\n1\\nPk\\nj=1 b1\\njαj\\n2\\n·\\n·\\n·\\nPk\\nj=1 b1\\njαj\\nm\\nPk\\nj=1 b2\\njαj\\n1\\nPk\\nj=1 b2\\njαj\\n2\\n·\\n·\\n·\\nPk\\nj=1 b2\\njαj\\nm\\n·\\n·\\n·\\n·\\n·\\n·\\n·\\n·\\n·\\n·\\n·\\n·\\nPk\\nj=1 bd\\njαj\\n1\\nPk\\nj=1 bd\\njαj\\n2\\n·\\n·\\n·\\nPk\\nj=1 bd\\njαj\\nm\\n\\uf8f9\\n\\uf8fa\\n\\uf8fa\\n\\uf8fa\\n\\uf8fa\\n\\uf8fa\\n\\uf8fa\\n\\uf8fa\\n\\uf8fa\\n\\uf8fa\\n\\uf8fb\\nd×m\\n得证。\\n将矩阵B 分解成矩阵列bj, j = 1, 2, . . . , k 带来一个好处，即和11.16 的原理相同，矩阵列与列之间\\n无关，因此可以分别优化各个列，即将min\\nB ∥. . . B . . . ∥2\\nF 转化成了min\\nbi ∥. . . bi . . . ∥2\\nF，得到第三行的等式\\n之后，再利用文中介绍的K-SVD 算法求解即可。\\n11.5\\nK-SVD 算法', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 145, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='F，得到第三行的等式\\n之后，再利用文中介绍的K-SVD 算法求解即可。\\n11.5\\nK-SVD 算法\\n本节前半部分铺垫概念，后半部分核心就是K-SVD。作为字典学习的最经典的算法，K-SVD[2] 自\\n2006 年发表以来已逾万次引用。理解K-SVD 的基础是SVD，即奇异值分解，参见“西瓜书”附录A.3。\\n对于任意实矩阵A ∈Rm×n, 都可分解为A = UΣV⊤, 其中U ∈Rm×m, V ∈Rn×n, 分别为m 阶和\\nn 阶正交矩阵(复数域时称为酉矩阵), 即U⊤U = I, V⊤V = I (逆矩阵等于自身的转置), Σ ∈Rm×n, 且除\\n(Σ)ii = σi 之外其它位置的元素均为零, σi 称为奇异值, 可以证明, 矩阵A 的秩等于非零奇异值的个数。\\n正如西瓜书附录A.3 所述, K-SVD 分解中主要使用SVD 解决低秩矩阵近似问题。\\n之所以称为K-SVD,\\n原文献中专门有说明:\\nWe shall call this algorithm ”K-SVD” to parallel the name K-means.\\nWhile K-means applies K', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 145, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='While K-means applies K\\ncomputations of means to update the codebook, K-SVD obtains the updated dictionary by K SVD com-\\nputations, each determining one column.\\n具体来说, 就是原文献中的字典共有K 个原子(列), 因此需要迭代K 次，这类似于K 均值算法欲将\\n数据聚成K 个簇, 需要计算K 次均值。\\n→_→\\n配套视频教程：https://www.bilibili.com/video/BV1Mh411e7VU\\n←_←', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 145, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='→_→\\n欢迎去各大电商平台选购纸质版南瓜书《机器学习公式详解第2 版》\\n←_←\\nK-SVD 算法伪代码详如图11-1所示, 其中符号与本节符号有差异。具体来说, 原文献中字典矩阵用D\\n表示(书中用B ), 稀疏系数用xi 表示(书中用αi ), 数据集用Y 表示(书中用X)。\\n图11-1 K-SVD 算法在论文中的描述\\n在初始化字典矩阵D 以后, K-SVD 算法迭代过程分两步: 第1 步Sparse Coding Stage 就是普通的已\\n知字典矩阵D 的稀疏表示问题, 可以使用很多现成的算法完成此步, 不再详述; K-SVD 的核心创新点在第\\n2 步Codebook Update Stage, 在该步骤中分K 次分别更新字典矩阵D 中每一列, 更新第k 列dk 时其它\\n各列都是固定的, 如原文献式(21) 所示:\\n∥Y −DX∥2\\nF =\\n\\r\\n\\r\\n\\r\\n\\r\\n\\rY −\\nK\\nX\\nj=1\\ndjxj\\nT\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n2\\nF\\n=\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n \\nY −\\nX\\nj̸=k\\ndjxj\\nT\\n!\\n−dkxk\\nT\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n2\\nF\\n=\\n\\r\\n\\rEk −dkxk\\nT\\n\\r\\n\\r2\\nF .', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 146, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='!\\n−dkxk\\nT\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n2\\nF\\n=\\n\\r\\n\\rEk −dkxk\\nT\\n\\r\\n\\r2\\nF .\\n注意, 矩阵dkxk\\nT 的秩为1 (其中, xk\\nT 表示稀疏系数矩阵X 的第k 行, 以区别于其第k 列xk ), 对比\\n西瓜书附录A.3 中的式(A.34), 这就是一个低秩矩阵近似问题, 即对于给定矩阵Ek, 求其最优1 秩近似矩\\n阵dkxk\\nT ; 此时可对Ek 进行SVD 分解, 类似于西瓜书附录式(A.35), 仅保留最大的1 个奇异值; 具体来说\\n→_→\\n配套视频教程：https://www.bilibili.com/video/BV1Mh411e7VU\\n←_←', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 146, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='→_→\\n欢迎去各大电商平台选购纸质版南瓜书《机器学习公式详解第2 版》\\n←_←\\nEk = U∆V⊤, 仅保留∆中最大的奇异值∆(1, 1), 则dkxk\\nT = U1∆(1, 1)V⊤\\n1 , 其中U1, V1 分别为U, V 的第\\n1 列, 此时dk = U1, xk\\nT = ∆(1, 1)V⊤\\n1 。但这样更新会破坏第1 步中得到的稀疏系数的稀疏性!\\n为了保证第1 步中得到的稀疏系数的稀疏性, K-SVD 并不直接对Ek 进行SVD 分解, 而是根据xk\\nT 仅\\n取出与xk\\nT 非零元素对应的部分列, 例如行向量xk\\nT 只有第1、3、5、8、9 个元素非零, 则仅取出Ek 的第\\n1、3、5、8、9 列组成矩阵进行SVD 分解ER\\nk = U∆V⊤, 则\\n˜\\ndk = U1,\\n˜\\nxk\\nT = ∆(1, 1)V⊤\\n1\\n即得到更新后的˜\\ndk 和˜\\nxk\\nT (注意, 此时的行向量˜\\nxk\\nT 长度仅为原xk\\nT 非零元素个数, 需要按原xk\\nT 对其\\n余位置填0)。如此迭代K 次即得更新后的字典矩阵˜\\nD, 以供下一轮Sparse Coding 使用。K −SVD 原文', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 147, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='D, 以供下一轮Sparse Coding 使用。K −SVD 原文\\n献中特意提到, 在K 次迭代中要使用最新的稀疏系数˜\\nxk\\nT , 但并没有说是否要用最新的˜\\ndk (推测应该也要\\n用最新的˜\\ndk )。\\n11.6\\n压缩感知\\n虽然压缩感知与稀疏表示关系密切, 但它是彻彻底底的信号处理领域的概念。\\n“西瓜书”在本章有几个\\n专业术语翻译与信号处理领域人士的习惯翻译略不一样：比如第258 页的Restricted Isometry Property\\n(RIP)“西瓜书”翻译为“限定等距性”, 信号处理领域一般翻译为“有限等距性质”; 第259 页的Basis\\nPursuit De-Noising、第261 页的Basis Pursuit 和Matching Pursuit 中的“Pursuit”\\n“西瓜书”翻译为\\n“寻踪”, 信号处理领域一般翻译为“追踪”, 即基追踪降噪、基追踪、匹配追踪。\\n11.6.1\\n式(11.21) 的解释\\n将式(11.21) 进行变形\\n(1 −δk) ⩽∥Aks∥2\\n2\\n∥s∥2\\n2\\n⩽(1 + δk)\\n注意不等式中间, 若s 为输入信号, 则分母∥s∥2', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 147, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='2\\n∥s∥2\\n2\\n⩽(1 + δk)\\n注意不等式中间, 若s 为输入信号, 则分母∥s∥2\\n2 为输入信号的能量, 分子∥Aks∥2\\n2 为对应的观测信号的能\\n量, 即RIP 要求观测信号与输入信号的能量之比在一定的范围之内; 例如当δk 等于0 时, 观测信号与输入\\n信号的能量相等, 即实现了等距变换，相关文献可以参考[3]; RIP 放松了对矩阵A 的约束(而且A 并非\\n方阵), 因此称为“有限”等距性质。\\n11.6.2\\n式(11.25) 的解释\\n该式即为核范数定义: 矩阵的核范数（迹范数）为矩阵的奇异值之和。\\n有关“凸包”的概念, 引用百度百科里的两句原话: 在二维欧几里得空间中, 凸包可想象为一条刚好包\\n著所有点的橡皮圈; 用不严谨的话来讲, 给定二维平面上的点集, 凸包就是将最外层的点连接起来构成的凸\\n多边形, 它能包含点集中所有的点。\\n个人理解, 将rank(X) 的“凸包”是X 的核范数∥X∥∗这件事简单理解为∥X∥∗是rank(X) 的上限即\\n可, 即∥X∥∗恒大于rank(X), 类似于式(11.10) 中的式子恒大于f(x) 。\\n参考文献', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 147, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='参考文献\\n[1] Wikipedia contributors. Sign function, 2020.\\n[2] Michal Aharon, Michael Elad, and Alfred Bruckstein. K-svd: An algorithm for designing overcomplete\\ndictionaries for sparse representation. IEEE Transactions on signal processing, 54(11):4311–4322, 2006.\\n[3] 杨孝春. 欧氏空间中的等距变换与等距映射. 四川工业学院学报, 1999.\\n→_→\\n配套视频教程：https://www.bilibili.com/video/BV1Mh411e7VU\\n←_←', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 147, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='→_→\\n欢迎去各大电商平台选购纸质版南瓜书《机器学习公式详解第2 版》\\n←_←\\n第12 章\\n计算学习理论\\n正如本章开篇所述，计算学习理论研究目的是分析学习任务的困难本质，为学习算法提供理论保证，\\n并根据分析结果指导算法设计。例如，\\n“西瓜书”定理12.1、定理12.3、定理12.6 所表达意思的共同点是，\\n泛化误差与经验误差之差的绝对值以很大概率(1 −δ) 很小，且这个差的绝对值随着训练样本个数(m) 的\\n增加而减小，随着模型复杂度（定理12.1 为假设空间包含的假设个数|H|，定理12.3 中为假设空间的VC\\n维，定理12.6 中为(经验)Rademacher 复杂度）的减小而减小。因此，若想要得到一个泛化误差很小的模\\n型，足够的训练样本是前提，最小化经验误差是实现途径，另外还要选择性能相同的模型中模型复杂度最\\n低的那一个；\\n“最小化经验误差”即常说的经验风险最小化，\\n“选择模型复杂度最低的那一个”即结构风险\\n最小化，可以参见“西瓜书”6.4 节最后一段的描述，尤其是式(6.42) 所表达的含义。\\n12.1\\n基础知识', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 148, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='12.1\\n基础知识\\n统计学中有总体集合和样本集合之分, 比如要统计国内本科生对机器学习的掌握情况, 此时全国所有\\n的本科生就是总体集合, 但总体集合往往太大而不具有实际可操作性, 一般都是取总体集合的一部分, 比如\\n从双一流A 类、双一流B 类、一流学科建设高校、普通高校中各找一部分学生(即样本集合) 进行调研,\\n以此来了解国内本科生对机器学习的掌握情况。在机器学习中, 样本空间(参见1.2 节) 对应总体集合, 而\\n我们手头上的样例集D 对应样本集合, 样例集D 是从样本空间中采样而得, 分布D 可理解为当从样本空\\n间采样获得样例集D 时每个样本被采到的概率, 我们用D(t) 表示样本空间第t 个样本被采到的概率。\\n12.1.1\\n式(12.1) 的解释\\n该式为泛化误差的定义式，所谓泛化误差，是指当样本x 从真实的样本分布D 中采样后其预测值\\nh(x) 不等于真实值y 的概率。在现实世界中，我们很难获得样本分布D，我们拿到的数据集可以看做是\\n从样本分布D 中独立同分布采样得到的。在西瓜书中，我们拿到的数据集，称为样例集D[也叫观测集、\\n样本集，注意与花体D 的区别]。\\n12.1.2', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 148, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='样本集，注意与花体D 的区别]。\\n12.1.2\\n式(12.2) 的解释\\n该式为经验误差的定义式，\\n所谓经验误差，\\n是指观测集D 中的样本xi, i = 1, 2, · · · , m 的预测值h(xi)\\n和真实值yi 的期望误差。\\n12.1.3\\n式(12.3) 的解释\\n假设我们有两个模型h1 和h2，将它们同时作用于样本x 上，那么他们的”不合“度定义为这两个模\\n型预测值不相同的概率。\\n12.1.4\\n式(12.4) 的解释\\nJensen 不等式：这个式子可以做很直观的理解，比如说在二维空间上，凸函数可以想象成开口向上的\\n抛物线，假如我们有两个点x1, x2，那么f(E(x)) 表示的是两个点的均值的纵坐标，而E(f(x)) 表示的是\\n两个点纵坐标的均值，因为两个点的均值落在抛物线的凹处，所以均值的纵坐标会小一些。\\n12.1.5\\n式(12.5) 的解释\\n随机变量的观测值是随机的, 进一步地, 随机过程的每个时刻都是一个随机变量。\\n式中,\\n1\\nm\\nPm\\ni=1 xi 表示m 个独立随机变量各自的某次观测值的平均,\\n1\\nm\\nPm\\ni=1 E (xi) 表示m 个独立随', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 148, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='1\\nm\\nPm\\ni=1 E (xi) 表示m 个独立随\\n机变量各自的期望的平均。\\n→_→\\n配套视频教程：https://www.bilibili.com/video/BV1Mh411e7VU\\n←_←', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 148, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='→_→\\n欢迎去各大电商平台选购纸质版南瓜书《机器学习公式详解第2 版》\\n←_←\\n式(12.5) 表示事件\\n1\\nm\\nPm\\ni=1 xi −1\\nm\\nPm\\ni=1 E (xi) ⩾ϵ 出现的概率不大于(i.e., ⩽) e−2mϵ2; 式(12.6) 的事\\n件\\n\\x0c\\n\\x0c 1\\nm\\nPm\\ni=1 xi −1\\nm\\nPm\\ni=1 E (xi)\\n\\x0c\\n\\x0c ⩾ϵ 等价于以下事件:\\n1\\nm\\nm\\nX\\ni=1\\nxi −1\\nm\\nm\\nX\\ni=1\\nE (xi) ⩾ϵ\\n∨\\n1\\nm\\nm\\nX\\ni=1\\nxi −1\\nm\\nm\\nX\\ni=1\\nE (xi) ⩽−ϵ\\n其中, ∨表示逻辑或(以上其实就是将绝对值表达式拆成两部分而已)。这两个子事件并无交集, 因此\\n总概率等于两个子事件概率之和; 而\\n1\\nm\\nPm\\ni=1 xi −1\\nm\\nPm\\ni=1 E (xi) ⩽−ϵ 与式(12.5) 表达的事情对称, 因此\\n概率相同。\\nHoeffding 不等式表达的意思是\\n1\\nm\\nPm\\ni=1 xi 和\\n1\\nm\\nPm\\ni=1 E (xi) 两个值应该比较接近, 二者之差大于ϵ 的\\n概率很小(不大于2e−2mϵ2 )。', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 149, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='概率很小(不大于2e−2mϵ2 )。\\n如果对Hoeffding 不等式的证明感兴趣，可以参考Hoeffding 在1963 年发表的论文[1]，这篇文章也\\n被引用了逾万次。\\n12.1.6\\n式(12.7) 的解释\\nMcDiarmid 不等式：首先解释下前提条件：\\nsup\\nx1,...,xm,x′\\ni\\n|f (x1, . . . , xm) −f (x1, . . . , xi−1, x′\\ni, xi+1, . . . , xm)| ⩽ci\\n表示当函数f 某个输入xi 变到x′\\ni 的时候，其变化的上确sup 仍满足不大于ci。所谓上确界sup 可以理\\n解成变化的极限最大值，可能取到也可能无穷逼近。当满足这个条件时，McDiarmid 不等式指出：函数值\\nf(x1, . . . , xm) 和其期望值E (f(x1, . . . , xm)) 也相近，从概率的角度描述是：它们之间差值不小于ϵ 这样\\n的事件出现的概率不大于exp\\n\\x10\\n−2ϵ2\\n∑\\ni c2\\ni\\n\\x11\\n，可以看出当每次变量改动带来函数值改动的上限越小，函数值和\\n其期望越相近。\\n12.2\\nPAC 学习', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 149, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='，可以看出当每次变量改动带来函数值改动的上限越小，函数值和\\n其期望越相近。\\n12.2\\nPAC 学习\\n本节内容几乎都是概念, 建议多读几遍，仔细琢磨一下。\\n概率近似正确(Probably Approximately Correct, PAC) 学习, 可以读为[pæk] 学习。\\n本节第2 段讨论的目标概念, 可简单理解为真实的映射函数;\\n本节第3 段讨论的假设空间, 可简单理解为学习算法不同参数时的存在, 例如线性分类超平面f(x) =\\nw⊤x + b, 每一组(w, b) 取值就是一个假设;\\n本节第4 段讨论的可分的(separable) 和不可分的(non-separable), 例如西瓜书第100 页的图5.4, 若\\n假设空间是线性分类器, 则(a)(b)(c) 是可分的, 而(d) 是不可分的; 当然, 若假设空间为椭圆分类器(分类\\n边界为椭圆), 则(d) 也是可分的;\\n本节第5 段提到的“等效的假设”指的是第7 页图1.3 中的A 和B 两条曲线都可以完美拟合有限的\\n样本点, 故称之为“等效”的假设; 另外本段最后还给出了概率近似正确的含义, 即“以较大概率学得误差', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 149, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='满足预设上限的模型”\\n。\\n定义12.1 PAC 辨识的式(12.9) 表示输出假设h 的泛化误差E(h) ⩽ϵ 的概率不小于1 −δ; 即“学习\\n算法L 能以较大概率(至少1 −δ ) 学得目标概念c 的近似(误差最多为ϵ )”\\n。\\n定义12.2 PAC 可学习的核心在于, 需要的样本数目m 是1/ϵ, 1/δ, size(x), size(c) 的多项式函数。\\n定义12.3 PAC 学习算法的核心在于, 完成PAC 学习所需要的时间是1/ϵ, 1/δ, size(x), size(c) 的多项\\n式函数。\\n定义12.4 样本复杂度指完成PAC 学习过程需要的最少的样本数量, 而在实际中当然也希望用最少的\\n样本数量完成学习过程。\\n在定义12.4 之后, 抛出来三个问题:\\n→_→\\n配套视频教程：https://www.bilibili.com/video/BV1Mh411e7VU\\n←_←', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 149, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='→_→\\n欢迎去各大电商平台选购纸质版南瓜书《机器学习公式详解第2 版》\\n←_←\\n• 研究某任务在什么样的条件下可学得较好的模型？\\n（定义12.2）\\n• 某算法在什么样的条件下可进行有效的学习?（定义12.3）\\n• 需多少训练样例才能获得较好的模型？\\n（定义12.4）\\n有限假设空间指H 中包含的假设个数是有限的, 反之则为无限假设空间; 无限假设空间更为常见, 例\\n如能够将图5.4(a)(b)(c) 中的正例和反例样本分开的线性超平面个数是无限多的。\\n12.2.1\\n式(12.9) 的解释\\nPAC 辨识的定义：E(h) 表示算法L 在用观测集D 训练后输出的假设函数h，它的泛化误差(见公\\n式12.1)。这个概率定义指出，如果h 的泛化误差不大于ϵ 的概率不小于1 −δ，那么我们称学习算法L\\n能从假设空间H 中PAC 辨识概念类C。\\n12.3\\n有限假设空间\\n本节内容分两部分, 第1 部分“可分情形”时, 可以达到经验误差b\\nE(h) = 0, 做的事情是以1 −δ 概率\\n学得目标概念的ϵ 近似, 即式(12.12); 第2 部分“不可分情形”时, 无法达到经验误差b', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 150, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='学得目标概念的ϵ 近似, 即式(12.12); 第2 部分“不可分情形”时, 无法达到经验误差b\\nE(h) = 0, 做的事\\n情是以1 −δ 概率学得minh∈H E(h) 的ϵ 近似, 即式(12.20)。无论哪种情形, 对于h ∈H, 可以得到该假\\n设的泛化误差E(h) 与经验误差b\\nE(h) 的关系, 即“当样例数目m 较大时, h 的经验误差是泛化误差很好\\n的近似”, 即式(12.18); 实际研究中经常需要推导类似的泛化误差上下界。\\n从式12.10 到式12.14 的公式是为了回答一个问题：到底需要多少样例才能学得目标概念c 的有效近\\n似。只要训练集D 的规模能使学习算法L 以概率1 −δ 找到目标假设的ϵ 近似即可。下面就是用数学公\\n式进行抽象。\\n12.3.1\\n式(12.10) 的解释\\nP(h(x) = y) = 1 −P(h(x) ̸= y) 因为它们是对立事件，P(h(x) ̸= y) = E(h) 是泛化误差的定义(见\\n12.1)，由于我们假定了泛化误差E(h) > ϵ，因此有1 −E(h) < 1 −ϵ。\\n12.3.2\\n式(12.11) 的解释', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 150, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='12.3.2\\n式(12.11) 的解释\\n先解释什么是h 与D\\n“表现一致”\\n，\\n12.2 节开头阐述了这样的概念，\\n如果h 能将D 中所有样本按与真实\\n标记一致的方式完全分开，\\n我们称问题对学习算法是一致的。\\n即(h (x1) = y1)∧. . .∧(h (xm) = ym) 为True。\\n因为每个事件是独立的，\\n所以上式可以写成P ((h (x1) = y1) ∧. . . ∧(h (xm) = ym)) = Qm\\ni=1 P (h (xi) = yi)。\\n根据对立事件的定义有：Qm\\ni=1 P (h (xi) = yi) = Qm\\ni=1 (1 −P (h (xi) ̸= yi))，又根据公式(12.10)，有\\nm\\nY\\ni=1\\n(1 −P (h (xi) ̸= yi)) <\\nm\\nY\\ni=1\\n(1 −ϵ) = (1 −ϵ)m\\n12.3.3\\n式(12.12) 的推导\\n首先解释为什么”我们事先并不知道学习算法L 会输出H 中的哪个假设“，因为一些学习算法对\\n用一个观察集D 的输出结果是非确定的，比如感知机就是个典型的例子，训练样本的顺序也会影响感知', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 150, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='用一个观察集D 的输出结果是非确定的，比如感知机就是个典型的例子，训练样本的顺序也会影响感知\\n机学习到的假设h 参数的值。泛化误差大于ϵ 且经验误差为0 的假设(即在训练集上表现完美的假设)\\n出现的概率可以表示为P(h ∈H : E(h) > ϵ ∧b\\nE(h) = 0)，根据式12.11，每一个这样的假设h 都满足\\nP(E(h) > ϵ ∧b\\nE(h) = 0) < (1 −ϵ)m，假设一共有|H| 这么多个这样的假设h，因为每个假设h 满足\\n→_→\\n配套视频教程：https://www.bilibili.com/video/BV1Mh411e7VU\\n←_←', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 150, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='→_→\\n欢迎去各大电商平台选购纸质版南瓜书《机器学习公式详解第2 版》\\n←_←\\nE(h) > ϵ 且b\\nE(h) = 0 是互斥的，因此总的概率P(h ∈H : E(h) > ϵ ∧b\\nE(h) = 0) 就是这些互斥事件之和，\\n即\\nP\\n\\x10\\nh ∈H : E(h) > ϵ ∧b\\nE(h) = 0\\n\\x11\\n=\\n|H|\\nX\\ni\\nP\\n\\x10\\nE(hi) > ϵ ∧b\\nE(hi) = 0\\n\\x11\\n< |H|(1 −ϵ)m\\n小于号依据公式(12.11)。\\n第二个小于号实际上是要证明|H|(1 −ϵ)m < |H|e−mϵ，即证明(1 −ϵ)m < e−mϵ，其中ϵ ∈(0, 1]，m\\n是正整数，推导如下：\\n当ϵ = 1 时，显然成立，当ϵ ∈(0, 1) 时，因为左式和右式的值域均大于0，所以可以左右两边同时取\\n对数，又因为对数函数是单调递增函数，所以即证明m ln(1 −ϵ) < −mϵ，即证明ln(1 −ϵ) < −ϵ，这个式\\n子很容易证明：令f(ϵ) = ln(1 −ϵ) + ϵ，其中ϵ ∈(0, 1)，f ′(ϵ) = 1 −\\n1\\n1−ϵ = 0 ⇒ϵ = 0 取极大值0，因此', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 151, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='1\\n1−ϵ = 0 ⇒ϵ = 0 取极大值0，因此\\nln(1 −ϵ) < −ϵ 也即|H|(1 −ϵ)m < |H|e−mϵ 成立。\\n12.3.4\\n式(12.13) 的解释\\n回到我们要回答的问题：到底需要多少样例才能学得目标概念c 的有效近似。只要训练集D 的规模\\n能使学习算法L 以概率1 −δ 找到目标假设的ϵ 近似即可。根据式12.12，学习算法L 生成的假设大于目\\n标假设的ϵ 近似的概率为P\\n\\x10\\nh ∈H : E(h) > ϵ ∧b\\nE(h) = 0\\n\\x11\\n< |H|e−mϵ，因此学习算法L 生成的假设落在\\n目标假设的ϵ 近似的概率为1 −P\\n\\x10\\nh ∈H : E(h) > ϵ ∧b\\nE(h) = 0\\n\\x11\\n≥1 −|H|e−mϵ，这个概率我们希望至少\\n是1 −δ，因此1 −δ ⩽1 −|H|e−mϵ ⇒|H|e−mϵ ⩽δ\\n12.3.5\\n式(12.14) 的推导\\n|H|e−mϵ ⩽δ\\ne−mϵ ⩽\\nδ\\n|H|\\n−mϵ ⩽ln δ −ln |H|\\nm ⩾1\\nϵ\\n\\x12\\nln |H| + ln 1\\nδ\\n\\x13', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 151, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='|H|\\n−mϵ ⩽ln δ −ln |H|\\nm ⩾1\\nϵ\\n\\x12\\nln |H| + ln 1\\nδ\\n\\x13\\n这个式子告诉我们，在假设空间H 是PAC 可学习的情况下，输出假设h 的泛化误差ϵ 随样本数目\\nm 增大而收敛到0，收敛速率为O( 1\\nm)。这也是我们在机器学习中的一个共识，即可供模型训练的观测集\\n样本数量越多，机器学习模型的泛化性能越好。\\n12.3.6\\n引理12.1 的解释\\n根据式(12.2), b\\nE(h) =\\n1\\nm\\nPm\\ni=1 I (h (xi) ̸= yi), 而指示函数I(·) 取值非0 即1 , 也就是说0 ≤\\nI (h (xi) ̸= yi) ≤1; 对于式(12.1) 的E(h) 实际上表示I (h (xi) ̸= yi) 为1 的期望E (I (h (xi) ̸= yi)) (泛化\\n误差表示样本空间中任取一个样本, 其预测类别不等于真实类别的概率), 当假设h 确定时, 泛化误差固定\\n不变, 因此可记为E(h) =\\n1\\nm\\nPm\\ni=1 E (I (h (xi) ̸= yi)) 。\\n此时, 将b', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 151, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='1\\nm\\nPm\\ni=1 E (I (h (xi) ̸= yi)) 。\\n此时, 将b\\nE(h) 和E(h) 代入式(12.15) 到式(12.17), 对比式(12.5) 和式(12.6) 的Hoeffding 不等式可\\n知, 式(12.15) 对应式(12.5), 式(12.16) 与式(12.15) 对称, 式(12.17) 对应式(12.6)。\\n→_→\\n配套视频教程：https://www.bilibili.com/video/BV1Mh411e7VU\\n←_←', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 151, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='→_→\\n欢迎去各大电商平台选购纸质版南瓜书《机器学习公式详解第2 版》\\n←_←\\n12.3.7\\n式(12.18) 的推导\\n令δ = 2e−2mϵ2，则ϵ =\\nq\\nln(2/δ)\\n2m ，由式(12.17)\\nP(|E(h) −b\\nE(h)| ⩾ϵ) ⩽2 exp\\n\\x00−2mϵ2\\x01\\nP(|E(h) −b\\nE(h)| ⩾ϵ) ⩽δ\\nP(|E(h) −b\\nE(h)| ⩽ϵ) ⩾1 −δ\\nP(−ϵ ⩽E(h) −b\\nE(h) ⩽ϵ) ⩾1 −δ\\nP( b\\nE(h) −ϵ ⩽E(h) ⩽b\\nE(h) + ϵ) ⩾1 −δ\\n带入ϵ =\\nq\\nln(2/δ)\\n2m\\n得证。\\n这个式子进一步阐明了当观测集样本数量足够大的时候，h 的经验误差是其泛化误差很好的近似。\\n12.3.8\\n式(12.19) 的推导\\n令h1, h2, . . . , h|H| 表示假设空间H 中的假设，有\\nP(∃h ∈H : |E(h) −b\\nE(h)| > ϵ)\\n=P\\n\\x10\\x10\\x0c\\n\\x0c\\n\\x0cEh1 −b\\nEh1\\n\\x0c\\n\\x0c\\n\\x0c > ϵ\\n\\x11\\n∨. . . ∨\\n\\x10\\n|Eh|H| −b\\nEh|H||>ϵ\\n\\x11\\x11\\n⩽\\nX\\nh∈H\\nP(|E(h) −b', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 152, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='\\x10\\n|Eh|H| −b\\nEh|H||>ϵ\\n\\x11\\x11\\n⩽\\nX\\nh∈H\\nP(|E(h) −b\\nE(h)| > ϵ)\\n这一步是很好理解的，存在一个假设h 使得|E(h) −b\\nE(h)| > ϵ 的概率可以表示为对假设空间内\\n所有的假设hi, i ∈1, . . . , |H|，使得\\n\\x0c\\n\\x0c\\n\\x0cEhi −b\\nEhi\\n\\x0c\\n\\x0c\\n\\x0c > ϵ 这个事件成立的” 或” 事件。因为P(A ∨B) =\\nP(A) + P(B) −P(A ∧B)，而P(A ∧B) ⩾0，所以最后一行的不等式成立。\\n由式12.17：\\nP(|E(h) −b\\nE(h)| ⩾ϵ) ⩽2 exp\\n\\x00−2mϵ2\\x01\\n⇒\\nX\\nh∈H\\nP(|E(h) −b\\nE(h)| > ϵ) ⩽2|H| exp\\n\\x00−2mϵ2\\x01\\n因此：\\nP(∃h ∈H : |E(h) −b\\nE(h)| > ϵ) ⩽\\nX\\nh∈H\\nP(|E(h) −b\\nE(h)| > ϵ)\\n⩽2|H| exp\\n\\x00−2mϵ2\\x01\\n其对立事件：\\nP(∀h ∈H : |E(h) −b\\nE(h)| ⩽ϵ) = 1 −P(∃h ∈H : |E(h) −b\\nE(h)| > ϵ)\\n⩾1 −2|H| exp', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 152, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='E(h)| > ϵ)\\n⩾1 −2|H| exp\\n\\x00−2mϵ2\\x01\\n令δ = 2|H|e−2mϵ2，则ϵ =\\nq\\nln |H|+ln(2/δ)\\n2m\\n，带入上式中即可得到\\nP\\n \\n∀h ∈H : |E(h) −b\\nE(h)| ⩽\\nr\\nln |H| + ln(2/δ)\\n2m\\n!\\n⩾1 −δ\\n其中∀h ∈H 这个前置条件可以省略。\\n12.3.9\\n式(12.20) 的解释\\n这个式子是”不可知PAC 可学习“的定义式，不可知是指当目标概念c 不在算法L 所能生成的假设\\n空间H 里。可学习是指如果H 中泛化误差最小的假设是arg minh∈H E(h)，且这个假设的泛化误差满足\\n其与目标概念的泛化误差的差值不大于ϵ 的概率不小于1 −δ。我们称这样的假设空间H 是不可知PAC\\n可学习的。\\n→_→\\n配套视频教程：https://www.bilibili.com/video/BV1Mh411e7VU\\n←_←', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 152, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='→_→\\n欢迎去各大电商平台选购纸质版南瓜书《机器学习公式详解第2 版》\\n←_←\\n12.4\\nVC 维\\n不同于12.3 节的有限假设空间，从本节开始，本章剩余内容均针对无限假设空间。\\n12.4.1\\n式(12.21) 的解释\\n这个是增长函数的定义式。增长函数ΠH(m) 表示假设空间H 对m 个样本所能赋予标签的最大可能\\n的结果数。比如对于两个样本的二分类问题，一共有4 中可能的标签组合[[0, 0], [0, 1], [1, 0], [1, 1]]，如果假\\n设空间H1 能赋予这两个样本两种标签组合[[0, 0], [1, 1]]，则ΠH1(2) = 2。显然，H 对样本所能赋予标签\\n的可能结果数越多，H 的表示能力就越强。增长函数可以用来反映假设空间H 的复杂度。\\n12.4.2\\n式(12.22) 的解释\\n值得指出的是，这个式子的前提假设有误，应当写成对假设空间H，m ∈N，0 < ϵ < 1，存在h ∈H\\n详细证明参见原论文On the uniform convergence of relative frequencies of events to their probabilities', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 153, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='[2]，在该论文中，定理的形式如下：\\nTheorem 2 The probability that the relative frequency of at least one event in class S differs from its\\nprobability in an experiment of size l by more then ε, for l ≧2/ε2, satisfies the inequality\\nP\\n\\x00π(l) > ε\\n\\x01\\n≦4mS(2l)e−ε2l/8.\\n注意定理描述中使用的是“at least one event in class S”, 因此应该是class S 中“存在”one event\\n而不是class S 中的“任意”event。\\n另外, 该定理为基于增长函数对无限假设空间的泛化误差分析, 与上一节有限假设空间的定理12.1。\\n在\\n证明定理12.1 的式(12.19) 过程中, 实际证明的结论是\\nP(∃h ∈H : |E(h) −b\\nE(h)| > ϵ) ⩽2|H|e−2mϵ2', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 153, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='P(∃h ∈H : |E(h) −b\\nE(h)| > ϵ) ⩽2|H|e−2mϵ2\\n根据该结论可得式(12.19) 的原型（式(12.19) 就是将ϵ 用δ 表示):\\nP(∀h ∈H : |E(h) −b\\nE(h)| ⩽ϵ) ⩽1 −2|H|e−2mϵ2\\n这是因为事件∃h ∈H : |E(h) −b\\nE(h)| > ϵ 与事件∀h ∈H : |E(h) −b\\nE(h)| ⩽ϵ 为对立事件。\\n注意到当使用|E(h) −b\\nE(h)| > ϵ 表达时对应于“存在”, 当使用|E(h) −b\\nE(h)| ⩽ϵ 表达时则对应于\\n“任意”\\n。\\n综上所述, 式(12.22) 使用|E(h) −b\\nE(h)| > ϵ, 所以这里应该对应于“存在”\\n。\\n12.4.3\\n式(12.23) 的解释\\n这是VC 维的定义式：VC 维的定义是能被H 打散的最大示例集的大小。\\n“西瓜书”中例12.1 和例\\n12.2 给出了形象的例子。\\n式(12.23) 中的{m : ΠH(m) = 2m} 表示一个集合, 集合的元素是能使ΠH(m) = 2m 成立的所有m;', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 153, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='最外层的max 表示取集合的最大值。注意, 这里仅讨论二分类问题。注意，VC 维的定义式上的底数2 表\\n示这个问题是2 分类的问题。如果是n 分类的问题，那么定义式中底数需要变为n。\\nVC 维的概念还是很容易理解的, 有个常见的思维误区西瓜书也指出来了, 即“这并不意味着所有大小\\n为d 的示例集都能被假设空间H 打散”, 也就是说只要“存在大小为d 的示例集能被假设空间H 打散”\\n即可, 这里的区别与前面“定理12.2 的解释”中提到的“任意”与“存在”的关系一样。\\n→_→\\n配套视频教程：https://www.bilibili.com/video/BV1Mh411e7VU\\n←_←', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 153, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='→_→\\n欢迎去各大电商平台选购纸质版南瓜书《机器学习公式详解第2 版》\\n←_←\\n12.4.4\\n引理12.2 的解释\\n首先解释下数学归纳法的起始条件” 当m = 1, d = 0 或d = 1 时，定理成立”，当m = 1, d = 0 时，\\n由VC 维的定义(式12.23) VC(H) = max {m : ΠH(m) = 2m} = 0 可知ΠH(1) < 2，否则d 可以取到1，\\n又因为ΠH(m) 为整数，所以ΠH(1) ∈[0, 1]，式12.24 右边为P0\\ni=0\\n \\n1\\ni\\n!\\n= 1，因此不等式成立。当\\nm = 1, d = 1 时，因为一个样本最多只能有两个类别，所以ΠH(1) = 2，不等式右边为P1\\ni=0\\n \\n1\\ni\\n!\\n= 2，\\n因此不等式成立。\\n再介绍归纳过程，这里采样的归纳方法是假设式(12.24) 对(m −1, d −1) 和(m −1, d) 成立，推导\\n出其对(m, d) 也成立。证明过程中引入观测集D = {x1, x2, . . . , xm} 和观测集D′ = {x1, x2, . . . , xm−1}，', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 154, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='其中D 比D′ 多一个样本xm，它们对应的假设空间可以表示为：\\nH|D = {(h (x1) , h (x2) , . . . , h (xm)) |h ∈H}\\nH|D′ = {(h (x1) , h (x2) , . . . , h (xm−1)) |h ∈H}\\n如果假设h ∈H 对xm 的分类结果为+1，或为−1，那么任何出现在H|D′ 中的串都会在H|D 中出\\n现一次或者两次。这里举个例子就很容易理解了，假设m = 3：\\nH|D = {(+, −, −), (+, +, −), (+, +, +), (−, +, −), (−, −, +)}\\nH|D′ = {(+, +), (+, −), (−, +), (−, −)}\\n其中串(+, +) 在H|D 中出现了两次(+, +, +), (+, +, −)，H|D′ 中得其他串(+, −), (−, +), (−, −) 均\\n只在H|D 中出现了一次。这里的原因是每个样本是二分类的，所以多出的样本xm 要么取+，要么取−，\\n要么都取到(至少两个假设h 对xm 做出了不一致的判断)。', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 154, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='要么都取到(至少两个假设h 对xm 做出了不一致的判断)。\\n记号HD′|D 表示在H|D 中出现了两次的H|D′\\n组成的集合，比如在上例中HD′|D = {(+, +)}，有\\n\\x0c\\n\\x0cH|D\\n\\x0c\\n\\x0c =\\n\\x0c\\n\\x0cH|D′\\n\\x0c\\n\\x0c +\\n\\x0c\\n\\x0cHD′|D\\n\\x0c\\n\\x0c\\n由于H|D′ 表示限制在样本集D′ 上的假设空间H 的表达能力(即所有假设对样本集D′ 所能赋予的\\n标记种类数)，\\n样本集D′ 的数目为m −1，\\n根据增长函数的定义，\\n假设空间H 对包含m −1 个样本的集合\\n所能赋予的最大标记种类数为ΠH(m −1)，因此|H|D′| ⩽ΠH(m −1)。又根据数学归纳法的前提假设，有：\\n\\x0c\\n\\x0cH|D′\\n\\x0c\\n\\x0c ⩽ΠH(m −1) ⩽\\nd\\nX\\ni=0\\n \\nm −1\\ni\\n!\\n由记号H|D′ 的定义可知，|H|D′| ⩾\\nj\\n|H|D|\\n2\\nk\\n，又由于|H|D′| 和|HD′|D| 均为整数，因此|HD′|D| ⩽\\nj\\n|H|D|\\n2\\nk\\n，由于样本集D 的大小为m，根据增长函数的概念，有\\n\\x0c\\n\\x0cHD′|D\\n\\x0c\\n\\x0c ⩽\\nj\\n|H|D|\\n2\\nk\\n⩽ΠH(m −1)。假设', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 154, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='HD′|D\\n\\x0c\\n\\x0c ⩽\\nj\\n|H|D|\\n2\\nk\\n⩽ΠH(m −1)。假设\\nQ 表示能被HD′|D 打散的集合，因为根据HD′|D 的定义，HD 必对元素xm 给定了不一致的判定，因此\\nQ ∪{xm} 必能被H|D 打散，由前提假设H 的VC 维为d，因此HD′|D 的VC 维最大为d −1，综上有\\n\\x0c\\n\\x0cHD′|D\\n\\x0c\\n\\x0c ⩽ΠH(m −1) ⩽\\nd−1\\nX\\ni=0\\n \\nm −1\\ni\\n!\\n因此：\\n\\x0c\\n\\x0cH|D\\n\\x0c\\n\\x0c =\\n\\x0c\\n\\x0cH|D′\\n\\x0c\\n\\x0c +\\n\\x0c\\n\\x0cHD′|D\\n\\x0c\\n\\x0c\\n⩽\\nd\\nX\\ni=0\\n \\nm −1\\ni\\n!\\n+\\nd+1\\nX\\ni=0\\n \\nm −1\\ni\\n!\\n=\\nd\\nX\\ni=0\\n  \\nm −1\\ni\\n!\\n+\\n \\nm −1\\ni −1\\n!!\\n=\\nd\\nX\\ni=0\\n \\nm\\ni\\n!\\n→_→\\n配套视频教程：https://www.bilibili.com/video/BV1Mh411e7VU\\n←_←', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 154, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='→_→\\n欢迎去各大电商平台选购纸质版南瓜书《机器学习公式详解第2 版》\\n←_←\\n注：最后一步依据组合公式，推导如下：\\n \\nm −1\\ni\\n!\\n+\\n \\nm −1\\ni −1\\n!\\n=\\n(m −1)!\\n(m −1 −i)!i! +\\n(m −1)!\\n(m −1 −i + 1)!(i −1)!\\n=\\n(m −1)!(m −i)\\n(m −i)(m −1 −i)!i! +\\n(m −1)!i\\n(m −i)!(i −1)!i\\n= (m −1)!(m −i) + (m −1)!i\\n(m −i)!i!\\n= (m −1)!(m −i + i)\\n(m −i)!i!\\n= (m −1)!m\\n(m −i)!i!\\n=\\nm!\\n(m −i)!i! =\\n \\nm\\ni\\n!\\n12.4.5\\n式(12.28) 的解释\\nΠH(m) ⩽\\nd\\nX\\ni=0\\n \\nm\\ni\\n!\\n⩽\\nd\\nX\\ni=0\\n \\nm\\ni\\n! \\x10m\\nd\\n\\x11d−i\\n=\\n\\x10m\\nd\\n\\x11d\\nd\\nX\\ni=0\\n \\nm\\ni\\n! \\x12 d\\nm\\n\\x13i\\n⩽\\n\\x10m\\nd\\n\\x11d\\nm\\nX\\ni=0\\n \\nm\\ni\\n! \\x12 d\\nm\\n\\x13i\\n=\\n\\x10m\\nd\\n\\x11d\\x12\\n1 + d\\nm\\n\\x13m\\n<\\n\\x10e · m\\nd\\n\\x11d', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 155, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='i\\n! \\x12 d\\nm\\n\\x13i\\n=\\n\\x10m\\nd\\n\\x11d\\x12\\n1 + d\\nm\\n\\x13m\\n<\\n\\x10e · m\\nd\\n\\x11d\\n第一步到第二步和第三步到第四步均因为m ⩾d，第四步到第五步是由于二项式定理[3]：(x + y)n =\\nPn\\nk=0\\n \\nn\\nk\\n!\\nxn−kyk，其中令k = i, n = m, x = 1, y =\\nd\\nm 得\\n\\x00 m\\nd\\n\\x01d Pm\\ni=0\\n \\nm\\ni\\n!\\n\\x00 d\\nm\\n\\x01i =\\n\\x00 m\\nd\\n\\x01d (1 + d\\nm)m，\\n最后一步的不等式即需证明\\n\\x001 + d\\nm\\n\\x01m ⩽ed，因为\\n\\x001 + d\\nm\\n\\x01m =\\n\\x001 + d\\nm\\n\\x01 m\\nd d，根据自然对数底数e 的定义\\n[4]，\\n\\x001 + d\\nm\\n\\x01 m\\nd d < ed，注意原文中用的是⩽，但是由于e = lim d\\nm →0\\n\\x001 + d\\nm\\n\\x01 m\\nd 的定义是一个极限，所以\\n应该是用<。\\n12.4.6\\n式(12.29) 的解释\\n这里应该是作者的笔误，\\n根据式12.22，\\nE(h)−b\\nE(h) 应当被绝对值符号包裹。\\n将式12.28 带入式12.22\\n得\\nP\\n\\x10\\n|E(h) −b\\nE(h)| > ϵ\\n\\x11\\n⩽4\\n\\x122em', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 155, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='得\\nP\\n\\x10\\n|E(h) −b\\nE(h)| > ϵ\\n\\x11\\n⩽4\\n\\x122em\\nd\\n\\x13d\\nexp\\n\\x12\\n−mϵ2\\n8\\n\\x13\\n令4\\n\\x00 2em\\nd\\n\\x01d exp\\n\\x10\\n−mϵ2\\n8\\n\\x11\\n= δ 可解得\\nδ =\\ns\\n8d ln 2em\\nd\\n+ 8 ln 4\\nδ\\nm\\n带入式12.22，则定理得证。这个式子是用VC 维表示泛化界，可以看出，泛化误差界只与样本数量m 有\\n关，收敛速率为\\nq\\nln m\\nm\\n(书上简化为\\n1\\n√m)。\\n→_→\\n配套视频教程：https://www.bilibili.com/video/BV1Mh411e7VU\\n←_←', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 155, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='→_→\\n欢迎去各大电商平台选购纸质版南瓜书《机器学习公式详解第2 版》\\n←_←\\n12.4.7\\n式(12.30) 的解释\\n这个是经验风险最小化的定义式。即从假设空间中找出能使经验风险最小的假设。\\n12.4.8\\n定理12.4 的解释\\n首先回忆PAC 可学习的概念，见定义12.2，而可知/不可知PAC 可学习之间的区别仅仅在于概念类\\nc 是否包含于假设空间H 中。令\\nδ′ = δ\\n2\\nr\\n(ln 2/δ′)\\n2m\\n= ϵ\\n2\\n结合这两个标记的转换，由推论12.1 可知：\\nb\\nE(g) −ϵ\\n2 ⩽E(g) ⩽b\\nE(g) + ϵ\\n2\\n至少以1 −δ/2 的概率成立。写成概率的形式即：\\nP\\n\\x10\\n|E(g) −b\\nE(g)| ⩽ϵ\\n2\\n\\x11\\n⩾1 −δ/2\\n即P\\n\\x10\\x10\\nE(g) −b\\nE(g) ⩽ϵ\\n2\\n\\x11\\n∧\\n\\x10\\nE(g) −b\\nE(g) ⩾−ϵ\\n2\\n\\x11\\x11\\n⩾1 −δ/2，因此P\\n\\x10\\nE(g) −b\\nE(g) ⩽ϵ\\n2\\n\\x11\\n⩾1 −δ/2 且\\nP\\n\\x10\\nE(g) −b\\nE(g) ⩾−ϵ\\n2\\n\\x11\\n⩾1 −δ/2 成立。再令\\ns\\n8d ln 2em\\nd\\n+ 8 ln 4\\nδ′\\nm\\n= ϵ\\n2', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 156, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='\\x11\\n⩾1 −δ/2 成立。再令\\ns\\n8d ln 2em\\nd\\n+ 8 ln 4\\nδ′\\nm\\n= ϵ\\n2\\n由式12.29 可知\\nP\\n\\x10\\x0c\\n\\x0c\\n\\x0cE(h) −b\\nE(h)\\n\\x0c\\n\\x0c\\n\\x0c ⩽ϵ\\n2\\n\\x11\\n⩾1 −δ\\n2\\n同理，\\nP\\n\\x10\\nE(h) −b\\nE(h) ⩽ϵ\\n2\\n\\x11\\n⩾1−δ/2 且P\\n\\x10\\nE(h) −b\\nE(h) ⩾−ϵ\\n2\\n\\x11\\n⩾1−δ/2 成立。\\n由P\\n\\x10\\nE(g) −b\\nE(g) ⩾−ϵ\\n2\\n\\x11\\n⩾\\n1−δ/2 和P\\n\\x10\\nE(h) −b\\nE(h) ⩽ϵ\\n2\\n\\x11\\n⩾1−δ/2 均成立可知则事件E(g)−b\\nE(g) ⩾−ϵ\\n2 和事件E(h)−b\\nE(h) ⩽ϵ\\n2\\n同时成立的概率为：\\nP\\n\\x10\\x10\\nE(g) −b\\nE(g) ⩾−ϵ\\n2\\n\\x11\\n∧\\n\\x10\\nE(h) −b\\nE(h) ⩽ϵ\\n2\\n\\x11\\x11\\n=P\\n\\x10\\nE(g) −b\\nE(g) ⩾−ϵ\\n2\\n\\x11\\n+ P\\n\\x10\\nE(h) −b\\nE(h) ⩽ϵ\\n2\\n\\x11\\n−P\\n\\x10\\x10\\nE(g) −b\\nE(g) ⩾−ϵ\\n2\\n\\x11\\n∨\\n\\x10\\nE(h) −b\\nE(h) ⩽ϵ\\n2\\n\\x11\\x11\\n⩾1 −δ/2 + 1 −δ/2 −1\\n=1 −δ\\n即\\nP\\n\\x10\\x10\\nE(g) −b', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 156, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='2\\n\\x11\\x11\\n⩾1 −δ/2 + 1 −δ/2 −1\\n=1 −δ\\n即\\nP\\n\\x10\\x10\\nE(g) −b\\nE(g) ⩾−ϵ\\n2\\n\\x11\\n∧\\n\\x10\\nE(h) −b\\nE(h) ⩽ϵ\\n2\\n\\x11\\x11\\n⩾1 −δ\\n因此\\nP\\n\\x10\\nb\\nE(g) −E(g) + E(h) −b\\nE(h) ⩽ϵ\\n2 + ϵ\\n2\\n\\x11\\n= P\\n\\x10\\nE(h) −E(g) ⩽b\\nE(h) −b\\nE(g) + ϵ\\n\\x11\\n⩾1 −δ\\n再由h 和g 的定义，h 表示假设空间中经验误差最小的假设，g 表示泛化误差最小的假设，将这两个假设\\n共用作用于样本集D，则一定有b\\nE(h) ⩽b\\nE(g)，因此上式可以简化为：\\nP (E(h) −E(g) ⩽ϵ) ⩾1 −δ\\n根据式12.32 和式12.34，可以求出m 为关于(1/ϵ, 1/δ, size(x), size(c)) 的多项式，因此根据定理12.2，定\\n理12.5，得到结论任何VC 维有限的假设空间H 都是(不可知)PAC 可学习的。\\n→_→\\n配套视频教程：https://www.bilibili.com/video/BV1Mh411e7VU\\n←_←', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 156, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='→_→\\n欢迎去各大电商平台选购纸质版南瓜书《机器学习公式详解第2 版》\\n←_←\\n12.5\\nRademacher 复杂度\\n上一节中介绍的基于VC 维的泛化误差界是分布无关、数据独立的，本节将要介绍的Rademacher 复\\n杂度则在一定程度上考虑了数据分布。\\n12.5.1\\n式(12.36) 的解释\\n这里解释从第一步到第二步的推导，\\n因为前提假设是2 分类问题，\\nyk ∈{−1, +1}，\\n因此I (h(xi) ̸= yi) ≡\\n1−yih(xi)\\n2\\n。这是因为假如yi = +1, h(xi) = +1 或yi = −1, h(xi) = −1，有I (h(xi) ̸= yi) = 0 = 1−yih(xi)\\n2\\n；\\n反之，假如yi = −1, h(xi) = +1 或yi = +1, h(xi) = −1，有I (h(xi) ̸= yi) = 1 = 1−yih(xi)\\n2\\n。\\n12.5.2\\n式(12.37) 的解释\\n由公式12.36 可知，经验误差b\\nE(h) 和\\n1\\nm\\nPm\\ni=1 yih (xi) 呈反比的关系，因此假设空间中能使经验误差\\n最小的假设h 即是使\\n1\\nm\\nPm', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 157, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='最小的假设h 即是使\\n1\\nm\\nPm\\ni=1 yih (xi) 最大的h。\\n12.5.3\\n式(12.38) 的解释\\n上确界sup 这个概念前面已经解释过，\\n见式(12.7) 的解析。\\n相比于式(12.37), 样例真实标记yi 换为了\\nRademacher 随机变量σi, arg maxh∈H 换为了上确界suph∈H◦该式表示, 对于样例集D = {x1, x2, . . . , xm},\\n假设空间H 中的假设对其预测结果{h (x1) , h (x2) , . . . , h (xm)} 与随机变量集合σ = {σ1, σ2, . . . , σm} 的\\n契合程度。接下来解释一下该式的含义。1\\nm\\nPm\\ni=1 σih (xi) 中的σ = {σ1, σ2, . . . , σm} 表示单次随机生成\\n的结果（生成后就固定不动), 而{h (x1) , h (x2) , . . . , h (xm)} 表示某个假设h ∈H 的预测结果, 至于\\n1\\nm\\nPm\\ni=1 σih (xi) 的取值则取决于本次随机生成的σ 和假设h 的预测结果的契合程度。\\n进一步地, suph∈H\\n1\\nm', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 157, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='进一步地, suph∈H\\n1\\nm\\nPm\\ni=1 σih (xi) 中的σ = {σ1, σ2, . . . , σm} 仍表示单次随机生成的结果(生成后就\\n固定不动), 但此时需求解的是假设空间H 中所有假设与σ 最契合的那个h 。\\n例如, σ = {−1, +1, −1, +1} （即m = 4, 这里σ 仅为本次随机生成结果而已, 下次生成结果可能是另\\n一组结果), 假设空间H = {h1, h2, h3}, 其中\\n{h1 (x1) , h1 (x2) , h1 (x3) , h1 (x4)} = {−1, −1, −1, −1}\\n{h2 (x1) , h2 (x2) , h2 (x3) , h2 (x4)} = {−1, +1, −1, −1}\\n{h3 (x1) , h3 (x2) , h3 (x3) , h3 (x4)} = {+1, +1, +1, +1}\\n易知\\n1\\nm\\nPm\\ni=1 σih1 (xi) = 0, 1\\nm\\nPm\\ni=1 σih2 (xi) = 2\\n4, 1\\nm\\nPm\\ni=1 σih3 (xi) = 0, 因此\\nsup\\nh∈H\\n1\\nm\\nm\\nX\\ni=1', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 157, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='m\\nPm\\ni=1 σih3 (xi) = 0, 因此\\nsup\\nh∈H\\n1\\nm\\nm\\nX\\ni=1\\nσih (xi) = 2\\n4\\n12.5.4\\n式(12.39) 的解释\\nEσ\\n\"\\nsup\\nh∈H\\n1\\nm\\nm\\nX\\ni=1\\nσih (xi)\\n#\\n[解析]：这个式子可以用来衡量假设空间H 的表达能力，对变量σ 求期望可以理解为当变量σ 包含所有\\n可能的结果时，假设空间H 中最契合的假设h 和变量的平均契合程度。因为前提假设是2 分类的问题，\\n因此σi 一共有2m 种，\\n这些不同的σi 构成了数据集D = {(x1, y1), (x2, y2), . . . , (xm, ym)} 的”\\n对分\\n“(12.4\\n节)，如果一个假设空间的表达能力越强，那么就越有可能对于每一种σi，假设空间中都存在一个h 使得\\nh(xi) 和σi 非常接近甚至相同，对所有可能的σi 取期望即可衡量假设空间的整体表达能力，这就是这个\\n式子的含义。\\n→_→\\n配套视频教程：https://www.bilibili.com/video/BV1Mh411e7VU\\n←_←', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 157, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='→_→\\n欢迎去各大电商平台选购纸质版南瓜书《机器学习公式详解第2 版》\\n←_←\\n12.5.5\\n式(12.40) 的解释\\n对比式12.39，\\n这里使用函数空间F 代替了假设空间H，\\n函数f 代替了假设h，\\n很容易理解，\\n因为假设\\nh 即可以看做是作用在数据xi 上的一个映射，\\n通过这个映射可以得到标签yi。\\n注意前提假设实值函数空间\\nF : Z →R，即映射f 将样本zi 映射到了实数空间，这个时候所有的σi 将是一个标量即σi ∈{+1, −1}。\\n12.5.6\\n式(12.41) 的解释\\n这里所要求的是F 关于分布D 的Rademacher 复杂度，因此从D 中采出不同的样本Z，计算这些\\n样本对应的Rademacher 复杂度的期望。\\n12.5.7\\n定理12.5 的解释\\n首先令记号\\nb\\nEZ(f) = 1\\nm\\nm\\nX\\ni=1\\nf (zi)\\nΦ(Z) = sup\\nf∈F\\n\\x10\\nE[f] −b\\nEZ(f)\\n\\x11\\n即b\\nEZ(f) 表示函数f 作为假设下的经验误差，Φ(Z) 表示泛化误差和经验误差的差的上确界。再令\\nZ′ 为只与Z 有一个示例(样本) 不同的训练集，不妨设zm ∈Z 和z′', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 158, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='Z′ 为只与Z 有一个示例(样本) 不同的训练集，不妨设zm ∈Z 和z′\\nm ∈Z′ 为不同的示例，那么有\\nΦ (Z′) −Φ(Z) = sup\\nf∈F\\n\\x10\\nE[f] −b\\nEZ′(f)\\n\\x11\\n−sup\\nf∈F\\n\\x10\\nE[f] −b\\nEZ(f)\\n\\x11\\n⩽sup\\nf∈F\\n\\x10\\nb\\nEZ(f) −b\\nEZ′(f)\\n\\x11\\n= sup\\nf∈F\\nPm\\ni=1 f(zi) −Pm\\ni=1 f(z′\\ni)\\nm\\n= sup\\nf∈F\\nf (zm) −f (z′\\nm)\\nm\\n⩽1\\nm\\n第一个不等式是因为上确界的差不大于差的上确界[5]，第四行的等号由于Z′ 与Z 只有zm 不相同，\\n最后一行的不等式是因为前提假设F : Z →[0, 1]，即f(zm), f(z′\\nm) ∈[0, 1]。同理\\nΦ(Z) −Φ (Z′) = sup\\nf∈F\\nf (z′\\nm) −f (zm)\\nm\\n⩽1\\nm\\n综上二式有：\\n|Φ(Z) −Φ (Z′)| ⩽1\\nm\\n将Φ 看做函数f(注意这里的f 不是Φ 定义里的f)，\\n那么可以套用McDiarmid 不等式的结论式12.7\\nP (Φ(Z) −EZ[Φ(Z)] ⩾ϵ) ⩽exp\\n\\x12 −2ϵ2', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 158, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='P (Φ(Z) −EZ[Φ(Z)] ⩾ϵ) ⩽exp\\n\\x12 −2ϵ2\\nP\\ni c2\\ni\\n\\x13\\n令exp\\n\\x10\\n−2ϵ2\\n∑\\ni c2\\ni\\n\\x11\\n= δ 可以求得ϵ =\\nq\\nln(1/δ)\\n2m ，所以\\nP\\n \\nΦ(Z) −EZ[Φ(Z)] ⩾\\nr\\nln(1/δ)\\n2m\\n!\\n⩽δ\\n由逆事件的概率定义得\\nP\\n \\nΦ(Z) −EZ[Φ(Z)] ⩽\\nr\\nln(1/δ)\\n2m\\n!\\n⩾1 −δ\\n→_→\\n配套视频教程：https://www.bilibili.com/video/BV1Mh411e7VU\\n←_←', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 158, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='→_→\\n欢迎去各大电商平台选购纸质版南瓜书《机器学习公式详解第2 版》\\n←_←\\n即书中式12.44 的结论。下面来估计EZ[Φ(Z)] 的上界：\\nEZ[Φ(Z)] = EZ\\n\\x14\\nsup\\nf∈F\\n\\x10\\nE[f] −b\\nEZ(f)\\n\\x11\\x15\\n= EZ\\n\\x14\\nsup\\nf∈F\\nEZ′\\nh\\nb\\nEZ′(f) −b\\nEZ(f)\\ni\\x15\\n⩽EZ,Z′\\n\\x14\\nsup\\nf∈F\\n\\x10\\nb\\nEZ′(f) −b\\nEZ(f)\\n\\x11\\x15\\n= EZ,Z′\\n\"\\nsup\\nf∈F\\n1\\nm\\nm\\nX\\ni=1\\n(f (z′\\ni) −f (zi))\\n#\\n= Eσ,Z,Z′\\n\"\\nsup\\nf∈F\\n1\\nm\\nm\\nX\\ni=1\\nσi (f (z′\\ni) −f (zi))\\n#\\n⩽Eσ,Z′\\n\"\\nsup\\nf∈F\\n1\\nm\\nm\\nX\\ni=1\\nσif (z′\\ni)\\n#\\n+ Eσ,Z\\n\"\\nsup\\nf∈F\\n1\\nm\\nm\\nX\\ni=1\\n−σif (zi)\\n#\\n= 2Eσ,Z\\n\"\\nsup\\nf∈F\\n1\\nm\\nm\\nX\\ni=1\\nσif (zi)\\n#\\n= 2Rm(F)\\n第二行等式是外面套了一个对服从分布D 的示例集Z′ 求期望，因为EZ′∼D[ b\\nEZ′(f)] = E(f)，而采样', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 159, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='EZ′(f)] = E(f)，而采样\\n出来的Z′ 和Z 相互独立，因此有EZ′∼D[ b\\nEZ(f)] = b\\nEZ(f)。\\n第三行不等式基于上确界函数sup 是个凸函数，\\n将supf∈F 看做是凸函数f，\\n将b\\nEZ′(f)−b\\nEZ(f) 看做变\\n量x 根据Jesen 不等式(式12.4)，\\n有EZ\\nh\\nsupf∈F EZ′\\nh\\nb\\nEZ′(f) −b\\nEZ(f)\\nii\\n⩽EZ,Z′\\nh\\nsupf∈F\\n\\x10\\nb\\nEZ′(f) −b\\nEZ(f)\\n\\x11i\\n，\\n其中EZ,Z′[·] 是EZ[EZ′[·]] 的简写形式。\\n第五行引入对Rademacher 随机变量的期望，由于函数值空间是标量，因为σi 也是标量，即σi ∈\\n{−1, +1}，且σi 总以相同概率可以取到这两个值，因此可以引入Eσ 而不影响最终结果。\\n第六行利用了上确界的和不小于和的上确界[5]，因为第一项中只含有变量z′，所以可以将EZ 去掉，\\n因为第二项中只含有变量z，所以可以将EZ′ 去掉。\\n第七行利用σ 是对称的，所以−σ 的分布和σ 完全一致，所以可以将第二项中的负号去除，又因为', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 159, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='第七行利用σ 是对称的，所以−σ 的分布和σ 完全一致，所以可以将第二项中的负号去除，又因为\\nZ 和Z′ 均是从D 中i.i.d. 采样得到的数据，因此可以将第一项中的z′\\ni 替换成z，将Z′ 替换成Z。\\n最后根据定义式12.41 可得EZ[Φ(Z)] = 2Rm(F)，式(12.42) 得证。\\n12.6\\n定理12.6 的解释\\n针对二分类问题, 定理12.5 给出了“泛化误差”和“经验误差”的关系, 即:\\n• 式(12.47) 基于Rademacher 复杂度Rm(H) 给出了泛化误差E(h) 的上界;\\n• 式(12.48) 基于经验Rademacher 复杂度b\\nRD(H) 给出了泛化误差E(h) 的上界。\\n可能大家都会有疑问：定理12.6 的设定其实也适用于定理12.5, 即值域为二值的{−1, +1} 也属于值\\n域为连续值的[0, 1] 的一种特殊情况, 这一点从接下来的式(12.49) 的转换可以看出。那么, 为什么还要针\\n对二分类问题专门给出定理12.6 呢?\\n根据(经验)Rademacher 复杂度的定义可以知道, Rm(H) 和b', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 159, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='根据(经验)Rademacher 复杂度的定义可以知道, Rm(H) 和b\\nRD(H) 均大于零(参见前面有关式(12.39)\\n的解释, 书中式(12.39) 下面的一行也提到该式取值范围是[0, 1]); 因此, 相比于定理12.5 来说, 定理12.6 的\\n上界更紧, 因为二者的界只有中间一项关于(经验)Rademacher 复杂度的部分不同, 在定理12.5 中是两倍\\n的(经验)Rademacher 复杂度, 而在定理12.6 中是一倍的(经验)Rademacher 复杂度, 而(经验)Rademacher\\n复杂度大于零。\\n→_→\\n配套视频教程：https://www.bilibili.com/video/BV1Mh411e7VU\\n←_←', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 159, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='→_→\\n欢迎去各大电商平台选购纸质版南瓜书《机器学习公式详解第2 版》\\n←_←\\n因此, 为二分类问题量身定制的定理12.6 相比于通用的定理12.5 来说, 二者的区别在于定理12.6 考\\n虑了二分类的特殊情况, 得到了比定理12.5 更紧的泛化误差界, 仅此而已。\\n下面做一些证明：\\n(1) 首先通过式(12.49) 将值域为{−1, +1} 的假设空间H 转化为值域为[0, 1] 的函数空间FH ;\\n(2) 接下来是该证明最核心部分, 即证明式(12.50) 的结论b\\nRZ (FH) = 1\\n2 b\\nRD(H) : 第1 行等号就是定\\n义12.8; 第2 行等号就是根据式(12.49) 将fh (xi, yi) 换为I (h (xi) ̸= yi); 第3 行等号类似于式(12.36) 的\\n第2 个等号; 第4 行等号说明如下:\\nsup\\nh∈H\\n1\\nm\\nm\\nX\\ni=1\\nσi\\n1 −yih (xi)\\n2\\n= sup\\nh∈H\\n1\\n2m\\nm\\nX\\ni=1\\nσi + sup\\nh∈H\\n1\\n2m\\nm\\nX\\ni=1\\n−yiσih (xi)\\n2\\n其中suph∈H\\n1\\n2m\\nPm', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 160, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='h∈H\\n1\\n2m\\nm\\nX\\ni=1\\n−yiσih (xi)\\n2\\n其中suph∈H\\n1\\n2m\\nPm\\ni=1 σi 与h 无关, 所以suph∈H\\n1\\n2m\\nPm\\ni=1 σi =\\n1\\n2m\\nPm\\ni=1 σi, 即第4 行等号; 第5 行等号是\\n由于Eσ\\n\\x02 1\\nm\\nPm\\ni=1 σi\\n\\x03\\n= 0, 例如当m = 2 时, 所有可能得σ 包括(−1, −1), (−1, +1), (+1, −1) 和(+1, +1),\\n求期望后显然结果等于0 ; 第6 行等号正如边注所说, “−yiσi 与σi 分布相同”(原因跟定理12.5 中证明\\nEZ[Φ(Z)] ⩽2Rm(F) 相同, 即求期望时要针对所有可能的σ 参见“西瓜书”第282 页第8 行); 第7 行等\\n号再次使用了定义12.8。\\n(3) 关于式(12.51), 根据式(12.50) 的结论, 可证明如下:\\nRm (FH) = EZ\\nh\\nb\\nRZ (FH)\\ni\\n= ED\\n\\x141\\n2\\nb\\nRD(H)\\n\\x15\\n= 1\\n2ED\\nh\\nb\\nRD(H)\\ni\\n= 1\\n2Rm(H)\\n其中第2 个等号由Z 变为D 只是符号根据具体情况的适时变化而已。', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 160, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='i\\n= 1\\n2Rm(H)\\n其中第2 个等号由Z 变为D 只是符号根据具体情况的适时变化而已。\\n(4) 最后, 将式(12.49) 定义的fh 替换定理12.5 中的函数f, 则\\nE[f(z)] = E[I(h(x) ̸= y)] = E(h)\\n1\\nm\\nm\\nX\\ni=1\\nf (zi) = 1\\nm\\nm\\nX\\ni=1\\nI (h (xi) ̸= yi) = b\\nE(h)\\n将式(12.51) 代入式(12.42), 即用1\\n2Rm(H) 替换式(12.42) 的Rm(F), 式(12.47) 得证;\\n将式(12.50) 代入式(12.43), 即用1\\n2 b\\nRD(H) 替换式(12.43) 的b\\nRZ(F), 式(12.48) 得证。\\n这里有个疑问在于，定理12.5 的前提是“实值函数空间F : Z →[0, 1] ”, 而式(12.49) 得到的函数\\nfh(z) 的值域实际为{0, 1}, 仍是离散的而非实值的; 当然, 定理12.5 的证明也只需要其函数值在[0, 1] 范\\n围内即可, 并不需要其连续。\\n12.6.1\\n式(12.52) 的证明', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 160, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='围内即可, 并不需要其连续。\\n12.6.1\\n式(12.52) 的证明\\n比较繁琐，同书上所示，参见Foundations of Machine Learning[6]\\n12.6.2\\n式(12.53) 的推导\\n根据式12.28 有ΠH(m) ⩽\\n\\x00 e·m\\nd\\n\\x01d，根据式12.52 有Rm(H) ⩽\\nq\\n2 ln ΠH(m)\\nm\\n，因此ΠH(m) ⩽\\nq\\n2d ln em\\nd\\nm\\n，\\n再根据式12.47 E(h) ⩽b\\nE(h) + Rm(H) +\\nq\\nln(1/δ)\\n2m\\n即证。\\n12.7\\n稳定性\\n上上节中介绍的基于VC 维的泛化误差界是分布无关、数据独立的，上一节介绍的Rademacher 复杂\\n度则在一定程度上考虑了数据分布，但二者得到的结果均与具体学习算法无关；本节将要介绍的稳定性分\\n析可以获得与算法有关的分析结果。算法的“稳定性”考察的是算法在输入发生变化时，输出是否会随之\\n发生较大的变化。\\n→_→\\n配套视频教程：https://www.bilibili.com/video/BV1Mh411e7VU\\n←_←', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 160, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='→_→\\n欢迎去各大电商平台选购纸质版南瓜书《机器学习公式详解第2 版》\\n←_←\\n12.7.1\\n泛化/经验/留一损失的解释\\n根据式(12.54) 上方关于损失函数的描述：\\n“刻画了假设的预测标记与真实标记之间的差别”\\n，这里针\\n对的是二分类，预测标记和真实标记均只能取和两个值，它们之间的“差别”又能是什么呢？\\n因此，当“差别”取为时，式(12.54) 的泛化损失就是式(12.1) 的泛化误差，式(12.55) 的经验损失\\n就是式(12.2) 的经验误差，如果类似于式(12.1) 和式(12.2) 继续定义留一误差，那么式(12.56) 就对应\\n于留一误差。\\n12.7.2\\n式(12.57) 的解释\\n根据三角不等式[7]，有|a + b| ≤|a| + |b|，将a = ℓ(LD, z) −ℓ(LDi)，b = ℓ(LDi,z) −ℓ\\n\\x00LD\\\\i,z\\n\\x01\\n带入\\n即可得出第一个不等式，根据D\\\\i 表示移除D 中第i 个样本，Di 表示替换D 中第i 个样本，那么a, b\\n的变动均为一个样本，根据式12.57，a ⩽β, b ⩽β，因此a + b ⩽2β。\\n12.7.3\\n定理12.8 的解释', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 161, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='12.7.3\\n定理12.8 的解释\\n西瓜书在该定理下方已明确给出该定理的意义, 即“定理12.8 给出了基于稳定性分析推导出的学习算\\n法L 学得假设的泛化误差界”, 式(12.58) 和式(12.59) 分别基于经验损失和留一损失给出了泛化损失的\\n上界。接下来讨论两个相关问题:\\n(1) 定理12.8 的条件包括损失函数有界, 即0 ⩽ℓ(LD, z) ⩽M; 如本节第1 条注解“泛化/经验/留一\\n损失的解释”中所述, 若“差别”取为I (LD(x), y), 则泛化损失对应于泛化误差, 此时上限M = 1 。\\n(2) 在前面泛化误差上界的推导中\\n（例如定理12.1、\\n定理12.3、\\n定理12.6、\\n定理12.7), 上界中与样本数\\nm 有关的项收玫率均为O(1/√m), 但在该定理中却是O(β√m); 一般来讲, 随着样本数m 的增加, 经验误\\n差/损失应该收玫于泛化误差/损失, 因此这里假设β = 1/m (书中式(12.59) 下方第3 行写为β = O(1/m)\\n), 而在第2 条注解“定义12.10 的解释”中已经提到β 的取值的确会随着样本数m 的增多会变小, 虽然', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 161, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='书中并没有严格去讨论β 随m 增多的变化规律, 但至少直觉上是对的。\\n12.7.4\\n式(12.60) 的推导\\n将β =\\n1\\nm 带入至式(12.58) 即得证。\\n12.7.5\\n经验损失最小化\\n顾名思义, “经验损失最小化”指通过最小化经验损失来求得假设函数。\\n这里, “对于损失函数ℓ, 若学习算法L 所输出的假设满足经验损失最小化, 则称算法L 满足经验风险\\n最小化原则, 简称算法是ERM 的”\\n。在”西瓜书”第278 页, 若学习算法L 输出的假设h 满足式(12.30),\\n则也称L 为满足经验风险最小化原则的算法。而很明显, 式(12.30) 是在最小化经验误差。\\n那么最小化经验误差和最小化经验损失有什么区别么?\\n在” 西瓜书“第286 页左下角边注中提到, “最小化经验误差和最小化经验损失有时并不相同, 这是由\\n于存在某些病态的损失函数ℓ使得最小化经验损失并不是最小化经验误差”\\n。\\n对于“误差”\\n、\\n“损失”\\n、\\n“风险”等概念的辨析，参见“西瓜书”第2 章2.1 节的注解。\\n12.7.6\\n定理(12.9) 的证明的解释', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 161, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='12.7.6\\n定理(12.9) 的证明的解释\\n首先明确几个概念，ERM 表示算法L 满足经验风险最小化(Empirical Risk Minimization)。由于L\\n满足经验误差最小化，则可令g 表示假设空间中具有最小泛化损失的假设，即\\nℓ(g, D) = min\\nh∈H ℓ(h, D)\\n→_→\\n配套视频教程：https://www.bilibili.com/video/BV1Mh411e7VU\\n←_←', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 161, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='→_→\\n欢迎去各大电商平台选购纸质版南瓜书《机器学习公式详解第2 版》\\n←_←\\n再令\\nϵ′ = ϵ\\n2\\nδ\\n2 = 2 exp\\n\\x10\\n−2m (ϵ′)2\\x11\\n将ϵ′ = ϵ\\n2 带入到δ\\n2 = 2 exp\\n\\x10\\n−2m (ϵ′)2\\x11\\n可以解得m =\\n2\\nϵ2 ln 4\\nδ，由Hoeffding 不等式12.6，\\nP\\n \\x0c\\n\\x0c\\n\\x0c\\n\\x0c\\n\\x0c\\n1\\nm\\nm\\nX\\ni=1\\nxi −1\\nm\\nm\\nX\\ni=1\\nE (xi)\\n\\x0c\\n\\x0c\\n\\x0c\\n\\x0c\\n\\x0c ⩾ϵ\\n!\\n⩽2 exp\\n\\x00−2mϵ2\\x01\\n其中\\n1\\nm\\nPm\\ni=1 E (xi) = ℓ(g, D)，1\\nm\\nPm\\ni=1 xi = b\\nℓ(g, D)，带入可得\\nP(|ℓ(g, D) −b\\nℓ(g, D)| ⩾ϵ\\n2) ⩽δ\\n2\\n根据逆事件的概率可得\\nP(|ℓ(g, D) −b\\nℓ(g, D)| ⩽ϵ\\n2) ⩾1 −δ\\n2\\n即文中|ℓ(g, D) −b\\nℓ(g, D)| ⩽ϵ\\n2 至少以1 −δ/2 的概率成立。\\n由\\n2\\nm + (4 + M)\\nq\\nln(2/δ)\\n2m\\n= ϵ\\n2 可以求解出\\n√m =\\n(4 + M)\\nq\\nln(2/δ)\\n2\\n+\\nq', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 162, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='2m\\n= ϵ\\n2 可以求解出\\n√m =\\n(4 + M)\\nq\\nln(2/δ)\\n2\\n+\\nq\\n(4 + M)2 ln(2/δ)\\n2\\n−4 × ϵ\\n2 × (−2)\\n2 × ϵ\\n2\\n即m = O\\n\\x00 1\\nϵ2 ln 1\\nδ\\n\\x01\\n。\\n由P(|ℓ(g, D) −b\\nℓ(g, D)| ⩽ϵ\\n2) ⩾1 −δ\\n2 可以按照同公式12.31 中介绍的相同的方法推导出\\nP(ℓ(L, D) −ℓ(g, D) ⩽ϵ) ⩾1 −δ\\n又因为m 为与(1/ϵ, 1/δ, size(x), size(c)) 相关的多项式的值，因此根据定理12.2，定理12.5，得到结\\n论H 是(不可知)PAC 可学习的。\\n参考文献\\n[1] Wassily Hoeffding. Probability inequalities for sums of bounded random variables. Journal of the\\nAmerican statistical association, 58(301):13–30, 1963.', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 162, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='[2] Vladimir N Vapnik and A Ya Chervonenkis. On the uniform convergence of relative frequencies of\\nevents to their probabilities. In Measures of complexity, pages 11–30. Springer, 2015.\\n[3] Wikipedia contributors. Binomial theorem, 2020.\\n[4] Wikipedia contributors. E, 2020.\\n[5] robjohn. Supremum of the difference of two functions, 2013.\\n[6] Mehryar Mohri, Afshin Rostamizadeh, and Ameet Talwalkar. Foundations of machine learning. 2018.\\n[7] Wikipedia contributors. Triangle inequality, 2020.\\n→_→', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 162, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='→_→\\n配套视频教程：https://www.bilibili.com/video/BV1Mh411e7VU\\n←_←', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 162, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='→_→\\n欢迎去各大电商平台选购纸质版南瓜书《机器学习公式详解第2 版》\\n←_←\\n第13 章\\n半监督学习\\n13.1\\n未标记样本\\n“西瓜书”两张插图可谓本节亮点: 图13.1 直观地说明了使用末标记样本后带来的好处; 图13.2 对比\\n了主动学习、(纯) 半监督学习和直推学习, 尤其是巧妙地将主动学习的概念融入进来。\\n直推学习是综合运用手头上已有的少量有标记样本和大量末标记样本, 对这些大量末标记样本预测其\\n标记; 而(纯) 半监督学习是综合运用手头上已有的少量有标记样本和大量末标记样本, 对新的末标记样本\\n预测其标记。\\n对于直推学习, 当然可以仅利用有标记样本训练一个学习器, 再对末标记样本进行预测, 此即传统的监\\n督学习; 对于(纯) 半监督学习, 当然也可以舍弃大量末标记样本, 仅利用有标记样本训练一个学习器, 再对\\n新的末标记样本进行预测。但图13.1 直观地说明了使用末标记样本后带来的好处, 然而利用了末标记样本\\n后是否真的会如图13.1 所示带来预期的好处呢? 此即13.7 节阅读材料中提到的安全半监督学习。', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 163, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='后是否真的会如图13.1 所示带来预期的好处呢? 此即13.7 节阅读材料中提到的安全半监督学习。\\n接下来在13.2 节、13.3 节、13.4 节、13.5 节介绍的四种半监督学习方法, 都可以应用于直推学习, 但\\n若要应用于(纯) 半监督学习, 则要有额外的考虑, 尤其是13.4 节介绍的图半监督学习, 因为该节最后一段\\n也明确提到“构图过程仅能考虑训练样本集, 难以判知新样本在图中的位置, 因此, 在接收到新样本时, 或\\n是将其加入原数据集对图进行重构并重新进行标记传播, 或是需引入额外的预测机制”\\n。\\n13.2\\n生成式方法\\n本节与9.4.3 节的高斯混合聚类密切相关, 有关9.4.3 节的公式推导参见附录, 建议将高斯混合聚类的\\n内容理解之后再学习本节算法。\\n13.2.1\\n式(13.1) 的解释\\n高斯混合分布的定义式。该式即为9.4.3 节的式(9.29)，式(9.29) 中的k 个混合成分对应于此处的N\\n个可能的类别。\\n13.2.2\\n式(13.2) 的推导', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 163, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='个可能的类别。\\n13.2.2\\n式(13.2) 的推导\\n首先, 该式的变量Θ ∈{1, 2, . . . , N} 即为式(9.30) 中的zj ∈{1, 2, . . . , k} 。\\n从公式第1 行到第2 行是对概率进行边缘化(marginalization)；通过引入Θ 并对其求和PN\\ni=1 以抵\\n消引入的影响。从公式第2 行到第3 行推导如下\\np(y = j, Θ = i|x) = p(y = j, Θ = i, x)\\np(x)\\n= p(y = j, Θ = i, x)\\np(Θ = i, x)\\n· p(Θ = i, x)\\np(x)\\n= p(y = j|Θ = i, x) · p(Θ = i|x)\\np(y = j | x) 表示x 的类别y 为第j 个类别标记的后验概率（注意条件是已知x);\\np(y = j, Θ = i | x) 表示x 的类别y 为第j 个类别标记且由第i 个高斯混合成分生成的后验概率（注\\n意条件是已知x );\\np(y = j | Θ = i, x) 表示第i 个高斯混合成分生成的x 其类别y 为第j 个类别标记的概率（注意条件', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 163, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='是已知Θ 和x, 这里修改了西瓜书式(13.3) 下方对p(y = j | Θ = i, x) 的表述);\\np(Θ = i | x) 表示x 由第i 个高斯混合成分生成的后验概率（注意条件是已知x) 。\\n“西瓜书”第296 页第2 行提到“假设样本由高斯混合模型生成, 且每个类别对应一个高斯混合成分”\\n, 也就是说, 如果已知x 是由哪个高斯混合成分生成的, 也就知道了其类别。而p(y = j | Θ = i, x) 表示已\\n→_→\\n配套视频教程：https://www.bilibili.com/video/BV1Mh411e7VU\\n←_←', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 163, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='→_→\\n欢迎去各大电商平台选购纸质版南瓜书《机器学习公式详解第2 版》\\n←_←\\n知Θ 和x 的条件概率（已知Θ 就足够, 不需x 的信息), 因此\\np(y = j | Θ = i, x) =\\n\\uf8f1\\n\\uf8f2\\n\\uf8f3\\n1,\\ni = j\\n0,\\ni ̸= j\\n13.2.3\\n式(13.3) 的推导\\n根据式(13.1)\\np(x) =\\nN\\nX\\ni=1\\nαi · p (x|µi, Σi)\\n因此\\np(Θ = i|x) = p(Θ = i, x)\\nP(x)\\n=\\nαi · p (x|µi, Σi)\\nPN\\ni=1 αi · p (x|µi, Σi)\\n13.2.4\\n式(13.4) 的推导\\n第二项很好解释，当不知道类别信息的时候，样本xj 的概率可以用式13.1 表示，所有无类别信息的\\n样本Du 的似然是所有样本的乘积，因为ln 函数是单调的，所以也可以将ln 函数作用于这个乘积消除因\\n为连乘产生的数值计算问题。第一项引入了样本的标签信息，由\\np(y = j|Θ = i, x) =\\n(\\n1,\\ni = j\\n0,\\ni ̸= j\\n可知，这项限定了样本xj 只可能来自于yj 所对应的高斯分布。\\n13.2.5\\n式(13.5) 的解释', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 164, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='13.2.5\\n式(13.5) 的解释\\n参见式(13.3)，这项可以理解成样本xj 属于类别标签i(或者说由第i 个高斯分布生成) 的后验概率。\\n其中αi, µiΣi 可以通过有标记样本预先计算出来。即：\\nαi =\\nli\\n|Dl|, where |Dl| = PN\\ni=1 li\\nµi = 1\\nli\\nP\\n(xj,yj)∈Dl∧yj=i xj\\nΣi = 1\\nli\\nP\\n(xj,yj)∈Dl∧yj=i (xj −µi) (xj −µi)⊤\\n其中li 表示第i 类样本的有标记样本数目, |Dl| 为有标记样本集样本总数, ∧为“逻辑与”\\n。\\n13.2.6\\n式(13.6) 的解释\\n这项可以由\\n∂LL(Dl ∪Du)\\n∂µi\\n= 0\\n而得，将式13.4 的两项分别记为：\\nLL(Dl) =\\nX\\n(xj,yj∈Dl)\\nln\\n N\\nX\\ns=1\\nαs · p(xj|µs, Σs) · p(yi|Θ = s, xj)\\n!\\n=\\nX\\n(xj,yj∈Dl)\\nln\\n\\x10\\nαyj · p(xj|µyj, Σyj)\\n\\x11\\nLL(Du) =\\nX\\nxj∈Du\\nln\\n N\\nX\\ns=1\\nαs · p(xj|µs, Σs)\\n!\\n→_→', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 164, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='X\\nxj∈Du\\nln\\n N\\nX\\ns=1\\nαs · p(xj|µs, Σs)\\n!\\n→_→\\n配套视频教程：https://www.bilibili.com/video/BV1Mh411e7VU\\n←_←', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 164, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='→_→\\n欢迎去各大电商平台选购纸质版南瓜书《机器学习公式详解第2 版》\\n←_←\\n首先，LL(Dl) 对µi 求偏导，LL(Dl) 求和号中只有yj = i 的项能留下来，即\\n∂LL (Dl)\\n∂µi\\n=\\nX\\n(xj,yj)∈Dl∧yj=i\\n∂ln (αi · p (xj|µi, Σi))\\n∂µi\\n=\\nX\\n(xj,yj)∈Dl∧yj=i\\n1\\np (xj|µi, Σi) · ∂p (xj|µi, Σi)\\n∂µi\\n=\\nX\\n(xj,yj)∈Dl∧yj=i\\n1\\np (xj|µi, Σi) · p (xj|µi, Σi) · Σ−1\\ni\\n(xj −µi)\\n=\\nX\\n(xj,yj)∈Dl∧yj=i\\nΣ−1\\ni\\n(xj −µi)\\nLL(Du) 对µi 求导，参考9.33 的推导：\\n∂LL (Du)\\n∂µi\\n=\\nX\\nxj∈Du\\nαi\\nPN\\ns=1 αs · p (xj|µs, Σs)\\n· p (xj|µi, Σi) · Σ−1\\ni\\n(xj −µi)\\n=\\nX\\nxj∈Du\\nγji · Σ−1\\ni\\n(xj −µi)\\n综上，\\n∂LL (Dl ∪Du)\\n∂µi\\n=\\nX\\n(xj,yj)∈Dl∧yj=i\\nΣ−1\\ni', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 165, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='综上，\\n∂LL (Dl ∪Du)\\n∂µi\\n=\\nX\\n(xj,yj)∈Dl∧yj=i\\nΣ−1\\ni\\n(xj −µi) +\\nX\\nxj∈Du\\nγji · Σ−1\\ni\\n(xj −µi)\\n= Σ−1\\ni\\n\\uf8eb\\n\\uf8ed\\nX\\n(xj,yj)∈Dl∧yj=i\\n(xj −µi) +\\nX\\nxj∈Du\\nγji · (xj −µi)\\n\\uf8f6\\n\\uf8f8\\n= Σ−1\\ni\\n\\uf8eb\\n\\uf8ed\\nX\\n(xj,yj)∈Dl∧yj=i\\nxj +\\nX\\nxj∈Du\\nγji · xj −\\nX\\n(xj,yj)∈Dl∧yj=i\\nµi −\\nX\\nxj∈Du\\nγji · µi\\n\\uf8f6\\n\\uf8f8\\n令∂LL(Dl∪Du)\\n∂µi\\n= 0，两边同时左乘Σi 并移项：\\nX\\nxj∈Du\\nγji · µi +\\nX\\n(xj,yj)∈Dl∧yj=i\\nµi =\\nX\\nxj∈Du\\nγji · xj +\\nX\\n(xj,yj)∈Dl∧yj=i\\nxj\\n上式中，µi 可以作为常量提到求和号外面，而P\\n(xj,yj)∈Dl∧yj=i 1 = li，即第i 类样本的有标记样本数\\n目，因此\\n\\uf8eb\\n\\uf8edX\\nxj∈Du\\nγji +\\nX\\n(xj,yj)∈Dl∧yj=i\\n1\\n\\uf8f6\\n\\uf8f8µi =\\nX\\nxj∈Du\\nγji · xj +', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 165, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='X\\n(xj,yj)∈Dl∧yj=i\\n1\\n\\uf8f6\\n\\uf8f8µi =\\nX\\nxj∈Du\\nγji · xj +\\nX\\n(xj,yj)∈Dl∧yj=i\\nxj\\n即得式(13.6)。\\n13.2.7\\n式(13.7) 的解释\\n首先LL(Dl) 对Σi 求偏导，类似于式(13.6)\\n∂LL (Dl)\\n∂Σi\\n=\\nX\\n(xj,yj)∈Dl∧yj=i\\n∂ln (αi · p (xj|µi, Σi))\\n∂Σi\\n=\\nX\\n(xj,yj)∈Dl∧yj=i\\n1\\np (xj|µi, Σi) · ∂p (xj|µi, Σi)\\n∂Σi\\n=\\nX\\n(xj,yj)∈Dl∧yj=i\\n1\\np (xj|µi, Σi) · p (xj|µi, Σi) ·\\n\\x10\\nΣ−1\\ni\\n(xj −µi) (xj −µi)⊤−I\\n\\x11\\n· 1\\n2Σ−1\\ni\\n=\\nX\\n(xj,yj)∈Dl∧yj=i\\n\\x10\\nΣ−1\\ni\\n(xj −µi) (xj −µi)⊤−I\\n\\x11\\n· 1\\n2Σ−1\\ni\\n→_→\\n配套视频教程：https://www.bilibili.com/video/BV1Mh411e7VU\\n←_←', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 165, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='→_→\\n欢迎去各大电商平台选购纸质版南瓜书《机器学习公式详解第2 版》\\n←_←\\n然后LL(Du) 对Σi 求偏导，类似于式(9.35)\\n∂LL (Du)\\n∂Σi\\n=\\nX\\nxj∈Du\\nγji ·\\n\\x10\\nΣ−1\\ni\\n(xj −µi) (xj −µi)⊤−I\\n\\x11\\n· 1\\n2Σ−1\\ni\\n综合可得：\\n∂LL (Dl ∪Du)\\n∂Σi\\n=\\nX\\nxj∈Du\\nγji ·\\n\\x10\\nΣ−1\\ni\\n(xj −µi) (xj −µi)⊤−I\\n\\x11\\n· 1\\n2Σ−1\\ni\\n+\\nX\\n(xj,yj)∈Dl∧yj=i\\n\\x10\\nΣ−1\\ni\\n(xj −µi) (xj −µi)⊤−I\\n\\x11\\n· 1\\n2Σ−1\\ni\\n=\\n\\uf8eb\\n\\uf8edX\\nxj∈Du\\nγji ·\\n\\x10\\nΣ−1\\ni\\n(xj −µi) (xj −µi)⊤−I\\n\\x11\\n+\\nX\\n(xj,yj)∈Dl∧yj=i\\n\\x10\\nΣ−1\\ni\\n(xj −µi) (xj −µi)⊤−I\\n\\x11\\n\\uf8f6\\n\\uf8f8· 1\\n2Σ−1\\ni\\n令∂LL(Dl∪Du)\\n∂Σi\\n= 0，两边同时右乘2Σi 并移项：\\nX\\nxj∈Du\\nγji · Σ−1\\ni\\n(xj −µi) (xj −µi)⊤+\\nX\\n(xj,yj∈Dl∧yj=i\\nΣ−1\\ni', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 166, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='i\\n(xj −µi) (xj −µi)⊤+\\nX\\n(xj,yj∈Dl∧yj=i\\nΣ−1\\ni\\n(xj −µi) (xj −µi)⊤\\n=\\nX\\nxj∈Du\\nγji · I +\\nX\\n(xj,yj)∈Dl∧yj=i\\nI\\n=\\n\\uf8eb\\n\\uf8edX\\nxj∈Du\\nγji + li\\n\\uf8f6\\n\\uf8f8I\\n两边同时左乘以Σi：\\nX\\nxj∈Du\\nγji · (xj −µi) (xj −µi)⊤+\\nX\\n(xj,yj)∈Dl∧yj=i\\n(xj −µi) (xj −µi)⊤=\\n\\uf8eb\\n\\uf8edX\\nxj∈Du\\nγji + li\\n\\uf8f6\\n\\uf8f8Σi\\n即得式(13.7)。\\n13.2.8\\n式(13.8) 的解释\\n类似于式(9.36)，写出LL(Dl ∪Du) 的拉格朗日形式\\nL (Dl ∪Du, λ) = LL (Dl ∪Du) + λ\\n N\\nX\\ns=1\\nαs −1\\n!\\n= LL (Dl) + LL (Du) + λ\\n N\\nX\\ns=1\\nαs −1\\n!\\n类似于式(9.37)，对αi 求偏导。对于LL(Du)，求导结果与式(9.37) 的推导过程一样\\n∂LL (Du)\\n∂αi\\n=\\nX\\nxj∈Du\\n1\\nPN\\ns=1 αs · p (xj|µs, Σs)', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 166, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='∂αi\\n=\\nX\\nxj∈Du\\n1\\nPN\\ns=1 αs · p (xj|µs, Σs)\\n· p (xj|µi, Σi)\\n→_→\\n配套视频教程：https://www.bilibili.com/video/BV1Mh411e7VU\\n←_←', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 166, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='→_→\\n欢迎去各大电商平台选购纸质版南瓜书《机器学习公式详解第2 版》\\n←_←\\n对于LL(Dl)，类似于式(13.6) 和式(13.7) 的推导过程\\n∂LL (Dl)\\n∂αi\\n=\\nX\\n(xj,yj)∈Dl∧yj=i\\n∂ln (αi · p (xj|µi, Σi))\\n∂αi\\n=\\nX\\n(xj,yj)∈Dl∧yj=i\\n1\\nαi · p (xj|µi, Σi) · ∂(αi · p (xj|µi, Σi))\\n∂αi\\n=\\nX\\n(xj,yj)∈Dl∧yj=i\\n1\\nαi · p (xj|µi, Σi) · p (xj|µi, Σi)\\n=\\nX\\n(xj,yj)∈Dl∧yj=i\\n1\\nαi\\n= 1\\nαi\\n·\\nX\\n(xj,yj)∈Dl∧yj=i\\n1 = li\\nαi\\n上式推导过程中，重点注意变量是αi ，p(xj|µi, Σi) 是常量；最后一行αi 相对于求和变量为常量，因\\n此作为公因子提到求和号外面；li 为第i 类样本的有标记样本数目。\\n综合两项结果：\\n∂L (Dl ∪Du, λ)\\n∂αi\\n= li\\nαi\\n+\\nX\\nxj∈Du\\np (xj|µi, Σi)\\nPN\\ns=1 αs · p (xj|µs, Σs)', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 167, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='+\\nX\\nxj∈Du\\np (xj|µi, Σi)\\nPN\\ns=1 αs · p (xj|µs, Σs)\\n+ λ\\n令\\n∂LL(Dl ∪Du)\\n∂αi\\n= 0 并且两边同乘以αi，得\\nαi · li\\nαi\\n+\\nX\\nxj∈Du\\nαi · p (xj|µi, Σi)\\nPN\\ns=1 αs · p (xj|µs, Σs)\\n+ λ · αi = 0\\n结合式(9.30) 发现，求和号内即为后验概率γji, 即\\nli +\\nX\\nxi∈Du\\nγji + λαi = 0\\n对所有混合成分求和，得\\nN\\nX\\ni=1\\nli +\\nN\\nX\\ni=1\\nX\\nxi∈Du\\nγji +\\nN\\nX\\ni=1\\nλαi = 0\\n这里PN\\ni=1 αi = 1 ，因此PN\\ni=1 λαi = λ PN\\ni=1 αi = λ，根据9.30 中γji 表达式可知\\nN\\nX\\ni=1\\nγji =\\nN\\nX\\ni=1\\nαi · p(xj|µi, Σi)\\nΣN\\ns=1αs · p(xj|µs, Σs) =\\nPN\\ni=1 αi · p(xj|µi, Σi)\\nPN\\ns=1 αs · p(xj|µs, Σs)\\n= 1\\n再结合加法满足交换律，所以\\nN\\nX\\ni=1\\nX', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 167, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='s=1 αs · p(xj|µs, Σs)\\n= 1\\n再结合加法满足交换律，所以\\nN\\nX\\ni=1\\nX\\nxi∈Du\\nγji =\\nX\\nxi∈Du\\nN\\nX\\ni=1\\nγji =\\nX\\nxi∈Du\\n1 = u\\n以上分析过程中，P\\nxj∈Du 形式与Pu\\nj=1 等价，其中u 为未标记样本集的样本个数；PN\\ni=1 li = l 其中\\nl 为有标记样本集的样本个数；将这些结果代入\\nN\\nX\\ni=1\\nli +\\nN\\nX\\ni=1\\nX\\nxi∈Du\\nγji +\\nN\\nX\\ni=1\\nλαi = 0\\n解出l + u + λ = 0 且l + u = m 其中m 为样本总个数，移项即得λ = −m，最后带入整理解得\\nli +\\nX\\nxj∈Du\\nγji −λαi = 0\\n即li + P\\nxj∈Du γji −mαi = 0, 整理即得式13.8。\\n→_→\\n配套视频教程：https://www.bilibili.com/video/BV1Mh411e7VU\\n←_←', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 167, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='→_→\\n欢迎去各大电商平台选购纸质版南瓜书《机器学习公式详解第2 版》\\n←_←\\n13.3\\n半监督SVM\\n从本节名称“半监督SVM”即可知道与第6 章的SVM 内容联系紧密。建议理解了SVM 之后再学\\n习本节算法，会发现实际很简单；否则会感觉无从下手，难以理解。\\n由本节开篇的两段介绍可知，S3VM 是SVM 在半监督学习上的推广，是此类算法的总称而非某个具\\n体的算法，其最著名的代表是TSVM。\\n13.3.1\\n图13.3 的解释\\n注意对比S3VM 划分超平面穿过的区域与SVM 划分超平面穿过的区域的差别，明显S3VM 划分超\\n平面周围样本较少，也就是“数据低密度区域”\\n，即“低密度分隔”\\n。\\n13.3.2\\n式(13.9) 的解释\\n这个公式和式(6.35) 基本一致，除了引入了无标记样本的松弛变量ξi, i = l + 1, · · · m 和对应的权重\\n系数Cu 和无标记样本的标记指派ˆ\\nyi。因此，欲理解本节内容应该先理解SVM，否则会感觉无从下手，难\\n以理解。\\n13.3.3\\n图13.4 的解释\\n解释一下第6 行:\\n(1) ˆ\\nyiˆ', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 168, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='以理解。\\n13.3.3\\n图13.4 的解释\\n解释一下第6 行:\\n(1) ˆ\\nyiˆ\\nyj < 0 意味着末标记样本xi, xj 在此次迭代中被指派的标记ˆ\\nyi, ˆ\\nyj 相反(正例+1 和反例−1 各\\n1 个);\\n(2) ξi > 0 意味着末标记样本xi 在此次迭代中为支持向量: (a) 在间隔带内但仍与自己标记同侧\\n(0 < ξi < 1), (b) 在间隔带内但与自己标记异侧(1 < ξi < 2), (c) 不在间隔带且与自己标记异侧(ξi > 2)，\\n三种情况分别如图13-1所示。\\n图13-1 ξi 的三种情况\\n(3) ξi + ξj > 2 分两种情况。(I) (ξi > 1) ∧(ξj > 1), 表示都位于自己指派标记异侧, 交换它们的标记\\n后, 二者就都位于自己新指派标记同侧了, 如图13-2所示。\\n→_→\\n配套视频教程：https://www.bilibili.com/video/BV1Mh411e7VU\\n←_←', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 168, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='→_→\\n欢迎去各大电商平台选购纸质版南瓜书《机器学习公式详解第2 版》\\n←_←\\n交换\\n图13-2 (1 < ξi, ξj < 2)\\n可以发现, 当1 < ξi, ξj < 2 时, 交换之后虽然松弛变量仍然大于0 , 但至少ξi + ξj 比交换之前变小了;\\n若进一步的, 当ξi, ξj > 2 时, 则交换之后ξi + ξj 将变为0 , 如图13-3所示。\\n交换\\n图13-3 (ξi > 2) ∧(ξj > 2)\\n可以发现, 交换之后两个样本均被分类正确, 因此松弛变量均等于0。至于ξi, ξj 其中之一位于1 ∼2\\n之间, 另一个大于2 , 情况类似, 不单列出分析。\\n(II) (0 < ξi < 1) ∧(ξj > 2 −ξi), 表示有一个与自己标记同侧, 有一个与自己标记异侧, 此时可分两种\\n情况。\\n(II.1) 1 < ξj < 2, 表示样本与自己标记异侧, 但仍在间隔带内，如图13-4所示。\\n交换\\n图13-4 (ξi + ξj > 2) ∧(0 < ξi < 1) ∧(1 < ξj < 2)', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 169, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='图13-4 (ξi + ξj > 2) ∧(0 < ξi < 1) ∧(1 < ξj < 2)\\n可以发现, 此时两个样本位置超平面同一侧, 交换标记之后似乎没发生什么变化, 但是仔细观察会发现\\n交换之后ξi + ξj 比交换之前变小了。\\n→_→\\n配套视频教程：https://www.bilibili.com/video/BV1Mh411e7VU\\n←_←', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 169, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='→_→\\n欢迎去各大电商平台选购纸质版南瓜书《机器学习公式详解第2 版》\\n←_←\\n(II.2) ξj > 2, 表示样本在间隔带外，如图13-5所示。\\n交换\\n图13-5 (ξi + ξj > 2) ∧(0 < ξi < 1) ∧(ξj > 2)\\n可以发现, 交换之后其中之一被正确分类, ξi + ξj 比交换之前也变小了。综上所述, 当ξi + ξj > 2 时,\\n交换指派标记ˆ\\nyi, ˆ\\nyj 可以使ξi + ξj 下降, 也就是说分类结果会得到改善。再解释一下第11 行: 逐步增长\\nCu, 但不超过Cl, 末标记样本的权重小于有标记样本。\\n13.3.4\\n式(13.10) 的解释\\n将该式变形为C+\\nu\\nC−\\nu = u−\\nu+ , 即样本个数多的权重小, 样本个数少的权重大, 总体上保持二者的作用相同。\\n13.4\\n图半监督学习\\n本节共讲了两种方法，其中式(13.11) ~ 式(13.17) 讲述了一个针对二分类问题的标记传播方法，式\\n(13.18) ~ 式(13.21) 讲述了一个针对多分类问题的标记传播方法，两种方法的原理均为两种方法的原理均\\n为“相似的样本应具有相似的标记”', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 170, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='为“相似的样本应具有相似的标记”\\n，只是面向的问题不同，而且具体实现的方法也不同。\\n13.4.1\\n式(13.12) 的推导\\n注意, 该方法针对二分类问题的标记传播方法。我们希望能量函数E(f) 越小越好, 注意到式(13.11)\\n的0 < (W)ij ⩽1, 且样本xi 和样本xj 越相似(即∥xi −xj∥2 越小) 则(W)ij 越大, 因此要求式(13.12)\\n中的(f (xi) −f (xj))2 相应地越小越好(即“相似的样本应具有相似的标记”), 如此才能达到能量函数\\nE(f) 越小的目的。首先对式(13.12) 的第1 行式子进行展开整理:\\nE(f) = 1\\n2\\nm\\nX\\ni=1\\nm\\nX\\nj=1\\n(W)ij (f (xi) −f (xj))2\\n= 1\\n2\\nm\\nX\\ni=1\\nm\\nX\\nj=1\\n(W)ij\\n\\x00f 2 (xi) −2f (xi) f (xj) + f 2 (xj)\\n\\x01\\n= 1\\n2\\nm\\nX\\ni=1\\nm\\nX\\nj=1\\n(W)ijf 2 (xi) + 1\\n2\\nm\\nX\\ni=1\\nm\\nX\\nj=1\\n(W)ijf 2 (xj) −\\nm\\nX\\ni=1\\nm\\nX\\nj=1', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 170, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='2\\nm\\nX\\ni=1\\nm\\nX\\nj=1\\n(W)ijf 2 (xj) −\\nm\\nX\\ni=1\\nm\\nX\\nj=1\\n(W)ijf (xi) f (xj)\\n然后证明Pm\\ni=1\\nPm\\nj=1(W)ijf 2 (xi) = Pm\\ni=1\\nPm\\nj=1(W)ijf 2 (xj), 并变形:\\nm\\nX\\ni=1\\nm\\nX\\nj=1\\n(W)ijf 2 (xj) =\\nm\\nX\\nj=1\\nm\\nX\\ni=1\\n(W)jif 2 (xi) =\\nm\\nX\\ni=1\\nm\\nX\\nj=1\\n(W)ijf 2 (xi)\\n=\\nm\\nX\\ni=1\\nf 2 (xi)\\nm\\nX\\nj=1\\n(W)ij\\n→_→\\n配套视频教程：https://www.bilibili.com/video/BV1Mh411e7VU\\n←_←', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 170, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='→_→\\n欢迎去各大电商平台选购纸质版南瓜书《机器学习公式详解第2 版》\\n←_←\\n其中, 第1 个等号是把变量i, j 分别用j, i 替代(统一替换公式中的符号并不影响公式本身); 第2 个等\\n号是由于W 是对称矩阵(即(W)ij = W)ji ), 并交换了求和号次序(类似于多重积分中交换积分号次序),\\n到此完成了该步骤的证明; 第3 个等号是由于f 2 (xi) 与求和变量j 无关, 因此拿到了该求和号外面(与\\n求和变量无关的项相对于该求和变量相当于常数), 该步骤的变形主要是为了得到di 。令di = Pm\\nj=1(W)ij\\n( 既是W 第i 行元素之和, 实际亦是第j 列元素之和, 因为由于W 是对称矩阵, 即(W)ij = W)ji, 因此\\ndi = Pm\\nj=1(W)ji, 即第i 列元素之和), 则\\nE(f) =\\nm\\nX\\ni=1\\ndif 2 (xi) −\\nm\\nX\\ni=1\\nm\\nX\\nj=1\\n(W)ijf (xi) f (xj)\\n即式(13.12) 的第3 行, 其中第一项Pm\\ni=1 dif 2 (xi) 可以写为如下矩阵形式:\\n= f TDf\\n第二项Pm\\ni=1\\nPm', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 171, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='i=1 dif 2 (xi) 可以写为如下矩阵形式:\\n= f TDf\\n第二项Pm\\ni=1\\nPm\\nj=1(W)ijf (xi) f (xj) 也可以写为如下矩阵形式:\\nm\\nX\\ni=1\\nm\\nX\\nj=1\\n(W)ijf (xi) f (xj)\\n=\\nh\\nf (x1)\\nf (x2)\\n· · ·\\nf (xm)\\ni\\n\\uf8ee\\n\\uf8ef\\n\\uf8ef\\n\\uf8ef\\n\\uf8ef\\n\\uf8ef\\n\\uf8f0\\n(W)11\\n(W)12\\n· · ·\\n(W)1m\\n(W)21\\n(W)22\\n· · ·\\n(W)2m\\n.\\n.\\n.\\n.\\n.\\n.\\n...\\n.\\n.\\n.\\n(W)m1\\n(W)m2\\n· · ·\\n(W)mm\\n\\uf8f9\\n\\uf8fa\\n\\uf8fa\\n\\uf8fa\\n\\uf8fa\\n\\uf8fa\\n\\uf8fb\\n\\uf8ee\\n\\uf8ef\\n\\uf8ef\\n\\uf8ef\\n\\uf8ef\\n\\uf8ef\\n\\uf8f0\\nf (x1)\\nf (x2)\\n.\\n.\\n.\\nf (xm)\\n\\uf8f9\\n\\uf8fa\\n\\uf8fa\\n\\uf8fa\\n\\uf8fa\\n\\uf8fa\\n\\uf8fb\\n= f TW f\\n所以E(f) = f TD −f TW f = f T(D −W )f, 即式(13.12)。\\n13.4.2\\n式(13.13) 的推导\\n本式就是将式(13.12) 用分块矩阵形式表达而已, 拆分为标记样本和末标记样本两部分。', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 171, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='本式就是将式(13.12) 用分块矩阵形式表达而已, 拆分为标记样本和末标记样本两部分。\\n另外解释一下该式之前一段话中第一句的含义:“具有最小能量的函数f 在有标记样本上满足f (xi) =\\nyi(i = 1, 2, . . . , l), 在末标记样本上满足∆f = 0 ”, 前半句是很容易理解的, 有标记样本上满足f (xi) =\\nyi(i = 1, 2, . . . , l), 这时末标记样本的f (xi) 是待求变量且应该使E(f) 最小, 因此应将式(13.12) 对末标\\n记样本的f (xi) 求导并令导数等于0 即可, 此即表达式∆f = 0, 此处可以查看该算法的原始文献。\\n13.4.3\\n式(13.14) 的推导\\n将式(13.13) 根据矩阵运算规则进行变形, 这里第一项西瓜书中的符号有歧义，\\n应该表示成\\nh\\nf T\\nl\\nf T\\nu\\ni\\n即一个R1×(l+u) 的行向量。根据矩阵乘法的定义，有：\\nE(f) =\\nh\\nf T\\nl\\nf T\\nu\\ni \"\\nDll −W ll\\n−W lu\\n−W ul\\nDuu −W uu\\n# \"\\nf l\\nf u\\n#\\n=\\nh\\nf T', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 171, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='−W lu\\n−W ul\\nDuu −W uu\\n# \"\\nf l\\nf u\\n#\\n=\\nh\\nf T\\nl (Dll −W ll) −f T\\nuW ul\\n−f T\\nl W lu + f T\\nu (Duu −W uu)\\ni \"\\nf l\\nf u\\n#\\n=\\n\\x00f T\\nl (Dll −W ll) −f T\\nuW ul\\n\\x01\\nf l +\\n\\x00−f T\\nl W lu + f T\\nu (Duu −W uu)\\n\\x01\\nf u\\n= f T\\nl (Dll −W ll) f l −f T\\nuW ulf l −f T\\nl W luf u + f T\\nu (Duu −W uu) f u\\n= f T\\nl (Dll −W ll) f l −2f T\\nuW ulf l + f T\\nu (Duu −W uu) f u\\n其中最后一步，f T\\nl W luf u =\\n\\x00f T\\nl W luf u\\n\\x01T = f T\\nu W ulf l，因为这个式子的结果是一个标量。\\n→_→\\n配套视频教程：https://www.bilibili.com/video/BV1Mh411e7VU\\n←_←', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 171, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='→_→\\n欢迎去各大电商平台选购纸质版南瓜书《机器学习公式详解第2 版》\\n←_←\\n13.4.4\\n式(13.15) 的推导\\n首先，基于式(13.14) 对f u 求导：\\n∂E(f)\\n∂f u\\n= ∂f T\\nl (Dll −W ll) f l −2f T\\nuW ulf l + f T\\nu (Duu −W uu) f u\\n∂f u\\n= −2W ulf l + 2 (Duu −W uu) f u\\n令结果等于0 即得13.15。\\n注意式中各项的含义:\\nf u 即函数f 在末标记样本上的预测结果;\\nDuu, W uu, W ul 均可以由式(13.11) 得到;\\nf l 即函数f 在有标记样本上的预测结果(即已知标记, 详见“西瓜书”P301 倒数第3 行);\\n也就是说可以根据式(13.15) 根据Dl 上的标记信息(即f l ) 求得末标记样本的标记(即f u ), 式\\n(13.17) 仅是式(13.15) 的进一步变形化简, 不再细述。\\n仔细回顾该方法, 实际就是根据“相似的样本应具有相似的标记”的原则, 构建了目标函数式(13.12),', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 172, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='求解式(13.12) 得到了使用标记样本信息表示的末标记样本的预测标记。\\n13.4.5\\n式(13.16) 的解释\\n根据矩阵乘法的定义计算可得该式，其中需要注意的是，对角矩阵D 的拟等于其各个对角元素的倒\\n数。\\n13.4.6\\n式(13.17) 的推导\\n第一项到第二项是根据矩阵乘法逆的定义：(AB)−1 = B−1A−1，在这个式子中\\nPuu = D−1\\nuuWuu\\nPul = D−1\\nuuWul\\n均可以根据Wij 计算得到，因此可以通过标记fl 计算未标记数据的标签fu。\\n13.4.7\\n式(13.18) 的解释\\n其中Y 的第i 行表示第i 个样本的类别; 具体来说, 对于前l 个有标记样本来说, 若第i 个样本的类\\n别为j(1 ≤j ≤|Y|), 则Y 的第行第j 列即为1 , 第行其余元素为0 ; 对于后u 个末标记样本来说, Y 统\\n一为零。注意|Y| 表示集合Y 的势, 即包含元素(类别) 的个数。\\n13.4.8\\n式(13.20) 的解释\\nF∗= lim\\nt→∞F(t) = (1 −α)(I −αS)−1Y\\n[解析]：由式(13.19)', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 172, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='F∗= lim\\nt→∞F(t) = (1 −α)(I −αS)−1Y\\n[解析]：由式(13.19)\\nF(t + 1) = αSF(t) + (1 −α)Y\\n→_→\\n配套视频教程：https://www.bilibili.com/video/BV1Mh411e7VU\\n←_←', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 172, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='→_→\\n欢迎去各大电商平台选购纸质版南瓜书《机器学习公式详解第2 版》\\n←_←\\n当t 取不同的值时，有：\\nt = 0 : F(1) = αSF(0) + (1 −α)Y\\n= αSY + (1 −α)Y\\nt = 1 : F(2) = αSF(1) + (1 −α)Y = αS(αSY + (1 −α)Y) + (1 −α)Y\\n= (αS)2Y + (1 −α)\\n \\n1\\nX\\ni=0\\n(αS)i\\n!\\nY\\nt = 2 : F(3) = αSF(2) + (1 −α)Y\\n= αS\\n \\n(αS)2Y + (1 −α)\\n \\n1\\nX\\ni=0\\n(αS)i\\n!\\nY\\n!\\n+ (1 −α)Y\\n= (αS)3Y + (1 −α)\\n \\n2\\nX\\ni=0\\n(αS)i\\n!\\nY\\n可以观察到规律\\nF(t) = (αS)tY + (1 −α)\\n t−1\\nX\\ni=0\\n(αS)i\\n!\\nY\\n则\\nF∗= lim\\nt→∞F(t) = lim\\nt→∞(αS)tY + lim\\nt→∞(1 −α)\\n t−1\\nX\\ni=0\\n(αS)i\\n!\\nY\\n其中第一项由于S = D−1\\n2 WD−1', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 173, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='t−1\\nX\\ni=0\\n(αS)i\\n!\\nY\\n其中第一项由于S = D−1\\n2 WD−1\\n2 的特征值介于[-1, 1] 之间[1]，而α ∈(0, 1)，所以limt→∞(αS)t = 0，第\\n二项由等比数列公式\\nlim\\nt→∞\\nt−1\\nX\\ni=0\\n(αS)i = I −limt→∞(αS)t\\nI −αS\\n=\\nI\\nI −αS = (I −αS)−1\\n综合可得式(13.20)。\\n13.4.9\\n式(13.21) 的推导\\n这里主要是推导式(13.21) 的最优解即为式(13.20)。将式(13.21) 的目标函数进行变形。\\n第1 部分:\\n先将范数平方拆开为四项\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n1\\n√di\\nFi −\\n1\\np\\ndj\\nFj\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n2\\n=\\n \\n1\\n√di\\nFi −\\n1\\np\\ndj\\nFj\\n!  \\n1\\n√di\\nFi −\\n1\\np\\ndj\\nFj\\n!⊤\\n= 1\\ndi\\nFiF⊤\\ni + 1\\ndj\\nFjF⊤\\nj −\\n1\\np\\ndidj\\nFiF⊤\\nj −\\n1\\np\\ndjdi\\nFjF⊤\\ni\\n其中Fi ∈R1×|Y| 表示矩阵F 的第i 行, 即第i 个示例xi 的标记向量。将第1 项中的Pm', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 173, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='i,j=1 写为两\\n个和求号Pm\\ni=1\\nPm\\ni=1 的形式, 并将上面拆分的四项中的前两项代入, 得\\nm\\nX\\ni,j=1\\n(W)ij\\n1\\ndi\\nFiF⊤\\ni =\\nm\\nX\\ni=1\\n1\\ndi\\nFiF⊤\\ni\\nm\\nX\\nj=1\\n(W)ij =\\nm\\nX\\ni=1\\n1\\ndi\\nFiF⊤\\ni · di =\\nm\\nX\\ni=1\\nFiF⊤\\ni\\nm\\nX\\ni,j=1\\n(W)ij\\n1\\ndj\\nFjF⊤\\nj =\\nm\\nX\\nj=1\\n1\\ndj\\nFjF⊤\\nj\\nm\\nX\\ni=1\\n(W)ij =\\nm\\nX\\nj=1\\n1\\ndj\\nFjF⊤\\nj · dj =\\nm\\nX\\nj=1\\nFjF⊤\\nj\\n→_→\\n配套视频教程：https://www.bilibili.com/video/BV1Mh411e7VU\\n←_←', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 173, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='→_→\\n欢迎去各大电商平台选购纸质版南瓜书《机器学习公式详解第2 版》\\n←_←\\n以上化简过程中, 两个求和号可以交换求和次序; 又因为W 为对称阵, 因此对行求和与对列求和效果\\n一样, 即di = Pm\\nj=1(W)ij = Pm\\nj=1(W)ji （已在式(13.12) 推导时说明）\\n。显然,\\nm\\nX\\ni=1\\nFiF⊤\\ni =\\nm\\nX\\nj=1\\nFjF⊤\\nj =\\nm\\nX\\ni=1\\n∥Fi∥2 = ∥F∥2\\nF = tr\\n\\x00FF⊤\\x01\\n以上推导过程中, 第1 个等号显然成立, 因为二者仅是求和变量名称不同; 第2 个等号即将FiF⊤\\ni 写\\n为∥Fi∥2 形式; 从第2 个等号的结果可以看出这明显是在求矩阵F 各元素平方之和, 也就是矩阵F 的\\nFrobenius 范数（简称F 范数）的平方, 即第3 个等号; 根据矩阵F 范数与矩阵的迹的关系有第4 个等号\\n(详见本章预备知识: 矩阵的F 范数与迹)。接下来, 将上面拆分的四项中的第三项代入，得\\nm\\nX\\ni,j=1\\n(W)ij\\n1\\np\\ndidj\\nFiF⊤\\nj =\\nm\\nX\\ni,j=1\\n(S)ijFiF⊤\\nj = tr\\n\\x00S⊤FF⊤\\x01\\n= tr', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 174, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='FiF⊤\\nj =\\nm\\nX\\ni,j=1\\n(S)ijFiF⊤\\nj = tr\\n\\x00S⊤FF⊤\\x01\\n= tr\\n\\x00SFF⊤\\x01\\n具体来说, 以上化简过程为:\\nS =\\n\\uf8ee\\n\\uf8ef\\n\\uf8ef\\n\\uf8ef\\n\\uf8ef\\n\\uf8ef\\n\\uf8f0\\n(S)11\\n(S)12\\n· · ·\\n(S)1m\\n(S)21\\n(S)22\\n· · ·\\n(S)2m\\n.\\n.\\n.\\n.\\n.\\n.\\n...\\n.\\n.\\n.\\n(S)m1\\n(S)m2\\n· · ·\\n(S)mm\\n\\uf8f9\\n\\uf8fa\\n\\uf8fa\\n\\uf8fa\\n\\uf8fa\\n\\uf8fa\\n\\uf8fb\\n= D−1\\n2 WD−1\\n2\\n=\\n\\uf8ee\\n\\uf8ef\\n\\uf8ef\\n\\uf8ef\\n\\uf8ef\\n\\uf8ef\\n\\uf8f0\\n1\\n√d1\\n1\\n√d2\\n...\\n1\\n√dm\\n\\uf8f9\\n\\uf8fa\\n\\uf8fa\\n\\uf8fa\\n\\uf8fa\\n\\uf8fa\\n\\uf8fb\\n\\uf8ee\\n\\uf8ef\\n\\uf8ef\\n\\uf8ef\\n\\uf8ef\\n\\uf8ef\\n\\uf8f0\\n(W)11\\n(W)12\\n· · ·\\n(W)1m\\n(W)21\\n(W)22\\n· · ·\\n(W)2m\\n.\\n.\\n.\\n.\\n.\\n.\\n...\\n.\\n.\\n.\\n(W)m1\\n(W)m2\\n· · ·\\n(W)mm\\n\\uf8f9\\n\\uf8fa\\n\\uf8fa\\n\\uf8fa\\n\\uf8fa\\n\\uf8fa\\n\\uf8fb\\n\\uf8ee\\n\\uf8ef\\n\\uf8ef\\n\\uf8ef\\n\\uf8ef\\n\\uf8ef\\n\\uf8f0\\n1\\n√d1\\n1\\n√d2\\n...\\n1\\n√dm\\n\\uf8f9\\n\\uf8fa\\n\\uf8fa\\n\\uf8fa\\n\\uf8fa\\n\\uf8fa\\n\\uf8fb\\n由以上推导可以看出(S)ij =\\n1\\n√\\ndidj (W)ij, 即第1 个等号; 而\\nFF⊤=\\n\\uf8ee\\n\\uf8ef\\n\\uf8ef\\n\\uf8ef\\n\\uf8ef\\n\\uf8ef\\n\\uf8f0', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 174, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='1\\n√\\ndidj (W)ij, 即第1 个等号; 而\\nFF⊤=\\n\\uf8ee\\n\\uf8ef\\n\\uf8ef\\n\\uf8ef\\n\\uf8ef\\n\\uf8ef\\n\\uf8f0\\nF1\\nF2\\n.\\n.\\n.\\nFm\\n\\uf8f9\\n\\uf8fa\\n\\uf8fa\\n\\uf8fa\\n\\uf8fa\\n\\uf8fa\\n\\uf8fb\\nh\\nF⊤\\n1\\nF⊤\\n2\\n· · ·\\nF⊤\\nm\\ni\\n=\\n\\uf8ee\\n\\uf8ef\\n\\uf8ef\\n\\uf8ef\\n\\uf8ef\\n\\uf8ef\\n\\uf8f0\\nF1F⊤\\n1\\nF1F⊤\\n2\\n. . .\\nF1F⊤\\nm\\nF2F⊤\\n1\\nF2F⊤\\n2\\n· · ·\\nF2F⊤\\nm\\n.\\n.\\n.\\n.\\n.\\n.\\n...\\n.\\n.\\n.\\nFmF⊤\\n1\\nFmF⊤\\n2\\n· · ·\\nFmF⊤\\nm\\n\\uf8f9\\n\\uf8fa\\n\\uf8fa\\n\\uf8fa\\n\\uf8fa\\n\\uf8fa\\n\\uf8fb\\n若令A = S ◦FF⊤, 其中\\uffff表示Hadmard 积, 即矩阵S 与矩阵FF⊤元素对应相乘（参见百度百科哈\\n达玛积), 因此\\nm\\nX\\ni,j=1\\n(S)ijFiF⊤\\nj =\\nm\\nX\\ni,j=1\\n(A)ij\\n→_→\\n配套视频教程：https://www.bilibili.com/video/BV1Mh411e7VU\\n←_←', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 174, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='→_→\\n欢迎去各大电商平台选购纸质版南瓜书《机器学习公式详解第2 版》\\n←_←\\n可以验证, 上式的矩阵A = S ◦FF⊤元素之和Pm\\ni,j=1(A)ij 等于tr\\n\\x00S⊤FF⊤\\x01\\n, 这是因为\\ntr\\n\\uf8eb\\n\\uf8ec\\n\\uf8ec\\n\\uf8ec\\n\\uf8ec\\n\\uf8ec\\n\\uf8ed\\n\\uf8ee\\n\\uf8ef\\n\\uf8ef\\n\\uf8ef\\n\\uf8ef\\n\\uf8ef\\n\\uf8f0\\n(S)11\\n(S)12\\n· · ·\\n(S)1m\\n(S)21\\n(S)22\\n· · ·\\n(S)2m\\n.\\n.\\n.\\n.\\n.\\n.\\n...\\n.\\n.\\n.\\n(S)m1\\n(S)m2\\n· · ·\\n(S)mm\\n\\uf8f9\\n\\uf8fa\\n\\uf8fa\\n\\uf8fa\\n\\uf8fa\\n\\uf8fa\\n\\uf8fb\\n⊤\\n·\\n\\uf8ee\\n\\uf8ef\\n\\uf8ef\\n\\uf8ef\\n\\uf8ef\\n\\uf8ef\\n\\uf8f0\\nF1F⊤\\n1\\nF1F⊤\\n2\\n· · ·\\nF1F⊤\\nm\\nF2F⊤\\n1\\nF2F⊤\\n2\\n· · ·\\nF2F⊤\\nm\\n.\\n.\\n.\\n.\\n.\\n.\\n...\\n.\\n.\\n.\\nFmF⊤\\n1\\nFmF⊤\\n2\\n· · ·\\nFmF⊤\\nm\\n\\uf8f9\\n\\uf8fa\\n\\uf8fa\\n\\uf8fa\\n\\uf8fa\\n\\uf8fa\\n\\uf8fb\\n\\uf8f6\\n\\uf8f7\\n\\uf8f7\\n\\uf8f7\\n\\uf8f7\\n\\uf8f7\\n\\uf8f8\\n=\\n\\uf8ee\\n\\uf8ef\\n\\uf8ef\\n\\uf8ef\\n\\uf8ef\\n\\uf8ef\\n\\uf8f0\\n(S)11\\n(S)21\\n.\\n.\\n.\\n(S)m1\\n\\uf8f9\\n\\uf8fa\\n\\uf8fa\\n\\uf8fa\\n\\uf8fa\\n\\uf8fa\\n\\uf8fb\\n⊤\\n·\\n\\uf8ee\\n\\uf8ef\\n\\uf8ef\\n\\uf8ef\\n\\uf8ef\\n\\uf8ef\\n\\uf8f0\\nF1F⊤\\n1\\nF2F⊤\\n1\\n.\\n.\\n.\\nFmF⊤\\n1\\n\\uf8f9\\n\\uf8fa\\n\\uf8fa\\n\\uf8fa\\n\\uf8fa\\n\\uf8fa\\n\\uf8fb', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 175, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='\\uf8ef\\n\\uf8ef\\n\\uf8ef\\n\\uf8f0\\nF1F⊤\\n1\\nF2F⊤\\n1\\n.\\n.\\n.\\nFmF⊤\\n1\\n\\uf8f9\\n\\uf8fa\\n\\uf8fa\\n\\uf8fa\\n\\uf8fa\\n\\uf8fa\\n\\uf8fb\\n+\\n\\uf8ee\\n\\uf8ef\\n\\uf8ef\\n\\uf8ef\\n\\uf8ef\\n\\uf8ef\\n\\uf8f0\\n(S)12\\n(S)22\\n.\\n.\\n.\\n(S)m2\\n\\uf8f9\\n\\uf8fa\\n\\uf8fa\\n\\uf8fa\\n\\uf8fa\\n\\uf8fa\\n\\uf8fb\\n⊤\\n·\\n\\uf8ee\\n\\uf8ef\\n\\uf8ef\\n\\uf8ef\\n\\uf8ef\\n\\uf8ef\\n\\uf8f0\\nF1F⊤\\n2\\nF2F⊤\\n2\\n.\\n.\\n.\\nFmF⊤\\n2\\n\\uf8f9\\n\\uf8fa\\n\\uf8fa\\n\\uf8fa\\n\\uf8fa\\n\\uf8fa\\n\\uf8fb\\n+ . . . +\\n\\uf8ee\\n\\uf8ef\\n\\uf8ef\\n\\uf8ef\\n\\uf8ef\\n\\uf8ef\\n\\uf8f0\\n(S)1m\\n(S)2m\\n.\\n.\\n.\\n(S)mm\\n\\uf8f9\\n\\uf8fa\\n\\uf8fa\\n\\uf8fa\\n\\uf8fa\\n\\uf8fa\\n\\uf8fb\\n⊤\\uf8ee\\n\\uf8ef\\n\\uf8ef\\n\\uf8ef\\n\\uf8ef\\n\\uf8ef\\n\\uf8f0\\nF1F⊤\\nm\\nF2F⊤\\nm\\n.\\n.\\n.\\nFmF⊤\\nm\\n\\uf8f9\\n\\uf8fa\\n\\uf8fa\\n\\uf8fa\\n\\uf8fa\\n\\uf8fa\\n\\uf8fb\\n=\\nm\\nX\\ni=1\\n(S)i1FiF⊤\\n1 +\\nm\\nX\\ni=1\\n(S)i2FiF⊤\\n2 + . . . +\\nX\\ni=1\\n(S)imFiF⊤\\nm\\n=\\nm\\nX\\ni,j=1\\n(S)ijFiF⊤\\nj\\n即第2 个等号; 易知矩阵S 是对称阵\\n\\x00S⊤= S\\n\\x01\\n, 即得第3 个等号。又由于内积FiF⊤\\nj 是一个数(即大\\n小为1 × 1 的矩阵), 因此其转置等于本身,\\nFiF⊤\\nj =\\n\\x00FiF⊤\\nj\\n\\x01⊤=\\n\\x00F⊤\\nj', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 175, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='FiF⊤\\nj =\\n\\x00FiF⊤\\nj\\n\\x01⊤=\\n\\x00F⊤\\nj\\n\\x01⊤(Fi)⊤= FjF⊤\\ni\\n因此\\n1\\np\\ndidj\\nFiF⊤\\nj =\\n1\\np\\ndjdi\\nFjF⊤\\ni\\n进而上面拆分的四项中的第三项和第四项相等:\\nm\\nX\\ni,j=1\\n(W)ij\\n1\\np\\ndidj\\nFiF⊤\\nj =\\nm\\nX\\ni,j=1\\n(W)ij\\n1\\np\\ndjdi\\nFjF⊤\\ni\\n综上所述(以上拆分的四项中前两项相等、后两项相等, 正好抵消系数1\\n2 ):\\n1\\n2\\n\\uf8eb\\n\\uf8ed\\nm\\nX\\ni,j=1\\n(W)ij\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n1\\n√di\\nFi −\\n1\\np\\ndj\\nFj\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n2\\uf8f6\\n\\uf8f8= tr\\n\\x00FF⊤\\x01\\n−tr\\n\\x10\\nSFF⊤\\x11\\n第2 部分:\\n西瓜书中式(13.21) 的第2 部分与原文献[2] 中式(4) 的第2 部分不同:\\nQ(F) = 1\\n2\\nn\\nX\\ni,j=1\\nWij\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\nFi\\n√Dii\\n−\\nFj\\np\\nDjj\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n2\\n+ µ\\nn\\nX\\ni=1\\n∥Fi −Yi∥2 ,\\n原文献中第2 部分包含了所有样本(求和变量上限为n ), 而西瓜书只包含有标记样本, 并且第304 页', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 175, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='第二段提到“式(13.21) 右边第二项是迫使学得结果在有标记样本上的预测与真实标记尽可能相同”; 若\\n按原文献式(4) 在第二项中将末标记样本也包含进来, 由于对于末标记样本Yi = 0, 因此直观上理解是迫\\n使末标记样本学习结果尽可能接近0 , 这显然是不对的; 有关这一点作者在第24 次印刷勘误中进行了补\\n充: “考虑到有标记样本通常很少而末标记样本很多, 为缓解过拟合, 可在式(13.21) 中引入针对末标记样\\n本的L2 范数项µ Pl+u\\ni=l+1 ∥Fi∥2, 式(13.21) 加上此项之后就与原文献的式(4) 完全相同了。将第二项写为\\nF 范数形式:\\nm\\nX\\ni=1\\n∥Fi −Yi∥2 = ∥F −Y∥2\\nF\\n→_→\\n配套视频教程：https://www.bilibili.com/video/BV1Mh411e7VU\\n←_←', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 175, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='→_→\\n欢迎去各大电商平台选购纸质版南瓜书《机器学习公式详解第2 版》\\n←_←\\n综上, 式(13.21) 目标函数Q(F) = tr\\n\\x00FF⊤\\x01\\n−tr\\n\\x00SFF⊤\\x01\\n+ µ∥F −Y∥2\\nF, 求导:\\n∂Q(F)\\n∂F\\n= ∂tr\\n\\x00FF⊤\\x01\\n∂F\\n−∂tr\\n\\x00SFF⊤\\x01\\n∂F\\n+ µ∂∥F −Y∥2\\nF\\n∂F\\n= 2F −2SF + 2µ(F −Y)\\n令µ = 1−α\\nα , 并令∂Q(F)\\n∂F\\n= 2F −2SF + 2 1−α\\nα (F −Y) = 0, 移项化简即可得式(13.20), 即式(13.20) 是正则\\n化框架式(13.21) 的解。\\n13.5\\n基于分歧的方法\\n“西瓜书”的伟大之处在于巧妙地融入了很多机器学习的研究分支, 而非仅简单介绍经典的机器学习\\n算法。比如本节处于半监督学习章节范围内, 巧妙地将机器学习的研究热点之一多视图学习[3](multi-view\\nlearning) 融入进来, 类似地还有本章第一节将主动学习融入进来, 在第10 章第一节将k 近邻算法融入进\\n来, 在最后一节巧妙地将度量学习(metric learning) 融入进来等等。', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 176, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='来, 在最后一节巧妙地将度量学习(metric learning) 融入进来等等。\\n协同训练是多视图学习代表性算法之一, 本章叙述简单易懂。\\n13.5.1\\n图13.6 的解释\\n第2 行表示从样本集Du 中去除缓冲池样本Ds;\\n第4 行, 当j = 1 时', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 176, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='xj\\ni, x3−j\\ni\\n\\x0b\\n即为⟨x1\\ni , x2\\ni ⟩, 当j = 2 时', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 176, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='xj\\ni, x3−j\\ni\\n\\x0b\\n即为⟨x2\\ni , x1\\ni ⟩, 往后的3 −j 与\\n此相同; 注意本页左上角的注释: ⟨x1\\ni , x2\\ni ⟩与⟨x2\\ni , x1\\ni ⟩表示的是同一个样本, 因此第1 个视图的有标记标训\\n练集为D1\\nl = {(x1\\n1, y1) , . . . , (x1\\nl , yl)}, 第2 个视图的有标记标训练集为D2\\nl = {(x2\\n1, y1) , . . . , (x2\\nl , yl)};\\n第9 行第11 行是根据第j 个视图对缓冲池中无标记样本的分类置信度赋予伪标记, 准备交给第3−j\\n个视图使用。\\n13.6\\n半监督聚类\\n13.6.1\\n图13.7 的解释\\n注意算法第4 行到第21 行是依次对每个样本进行处理, 其中第8 行到第21 行是尝试将样本xi 到底\\n应该划入哪个族, 具体来说是按样本xi 到各均值向量的距离从小到大依次尝试, 若最小的不违背M 和C\\n中的约束, 则将样本xi 划入该簇并置is_merge=true, 此时第8 行的while 循环条件为假不再继续循环,', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 176, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='若从小到大依次尝试各簇后均违背M 和C 中的约束则第16 行的if 条件为真, 算法报错结束; 依次对每\\n个样本进行处理后第22 行到第24 行更新均值向量, 重新开始新一轮迭代, 直到均值向量均末更新。\\n13.6.2\\n图13.9 的解释\\n算法第6 行到第10 行即在聚类簇迭代更新过程中不改变种子样本的簇隶属关系；第11 行到第15 行\\n即对非种子样本进行普通的k-means 聚类过程；第16 行到第18 行更新均值向量，反复迭代，直到均值\\n向量均未更新。\\n参考文献\\n[1] Wikipedia contributors. Laplacian matrix, 2020.\\n[2] Dengyong Zhou, Olivier Bousquet, Thomas Lal, Jason Weston, and Bernhard Schölkopf. Learning\\nwith local and global consistency. Advances in neural information processing systems, 16, 2003.\\n→_→', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 176, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='→_→\\n配套视频教程：https://www.bilibili.com/video/BV1Mh411e7VU\\n←_←', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 176, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='→_→\\n欢迎去各大电商平台选购纸质版南瓜书《机器学习公式详解第2 版》\\n←_←\\n[3] Chang Xu, Dacheng Tao, and Chao Xu.\\nA survey on multi-view learning.\\narXiv preprint\\narXiv:1304.5634, 2013.\\n→_→\\n配套视频教程：https://www.bilibili.com/video/BV1Mh411e7VU\\n←_←', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 177, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='→_→\\n欢迎去各大电商平台选购纸质版南瓜书《机器学习公式详解第2 版》\\n←_←\\n第14 章\\n概率图模型\\n本章介绍概率图模型，前三节分别介绍了有向图模型之隐马尔可夫模型以及无向图模型之马尔可夫随\\n机场和条件随机场；接下来两节分别介绍精确推断和近似推断；最后一节简单介绍了话题模型的典型代表\\n隐狄利克雷分配模型(LDA)。\\n14.1\\n隐马尔可夫模型\\n本节前三段内容实际上是本章的概述，\\n从第四段才开始介绍\\n“隐马尔可夫模型”\\n。\\n马尔可夫的大名相信\\n很多人听说过，比如马尔可夫链；虽然隐马尔可夫模型与马尔可夫链并非同一人提出，但其中关键字“马\\n尔可夫”蕴含的概念是相同的，即系统下一时刻的状态仅由当前状态决定。\\n14.1.1\\n生成式模型和判别式模型\\n一般来说, 机器学习的任务是根据输入特征x 预测输出变量y; 生成式模型最终求得联合概率P(x, y),\\n而判别式模型最终求得条件概率P(y | x) 。\\n统计机器学习算法都是基于样本独立同分布(independent and identically distributed, 简称i.i.d. .)', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 178, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='的假设, 也就是说, 假设样本空间中全体样本服从一个末知的“分布”D, 我们获得的每个样本都是独立地\\n从这个分布上采样获得的。\\n对于一个样本(x, y), 联合概率P(x, y) 表示从样本空间中采样得到该样本的概率; 因为P(x, y) 表示\\n“生成”样本本身的概率, 故称之为“生成式模型”\\n。而条件概率P(y | x) 则表示已知x 的条件下输出为y\\n的概率, 即根据x “判别”y, 因此称为“判别式模型”\\n。\\n常见的对率回归、支持向量机等都属于判别式模型, 而朴素贝叶斯则属于生成式模型。\\n14.1.2\\n式(14.1) 的推导\\n由概率公式P(AB) = P(A | B) · P(B) 可得:\\nP (x1, y1, . . . , xn, yn) = P (x1, . . . , xn | y1, . . . , yn) · P (y1, . . . , yn)\\n其中, 进一步可将P (y1, . . . , yn) 做如下变换:\\nP (y1, . . . , yn) = P (yn | y1, . . . , yn−1) · P (y1, . . . , yn−1)', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 178, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='= P (yn | y1, . . . , yn−1) · P (yn−1 | y1, . . . , yn−2) · P (y1, . . . , yn−2)\\n= . . . . . .\\n= P (yn | y1, . . . , yn−1) · P (yn−1 | y1, . . . , yn−2) · . . . · P (y2 | y1) · P (y1)\\n由于状态y1, . . . , yn 构成马尔可夫链, 即yt 仅由yt−1 决定; 基于这种依赖关系, 有\\nP (yn | y1, . . . , yn−1) = P (yn | yn−1)\\nP (yn−1 | y1, . . . , yn−2) = P (yn−1 | yn−2)\\nP (yn−2 | y1, . . . , yn−3) = P (yn−2 | yn−3)\\n因此P (y1, . . . , yn) 可化简为\\nP (y1, . . . , yn) = P (yn | yn−1) · P (yn−1 | yn−2) · . . . · P (y2 | y1) · P (y1)\\n= P (y1)\\nn\\nY\\ni=2', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 178, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='= P (y1)\\nn\\nY\\ni=2\\nP (yi | yi−1)\\n→_→\\n配套视频教程：https://www.bilibili.com/video/BV1Mh411e7VU\\n←_←', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 178, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='→_→\\n欢迎去各大电商平台选购纸质版南瓜书《机器学习公式详解第2 版》\\n←_←\\n而根据“西瓜书”图14.1 表示的变量间的依赖关系: 在任一时刻, 观测变量的取值仅依赖于状态变量,\\n即xt 由yt 确定, 与其它状态变量及观测变量的取值无关。因此\\nP (x1, . . . , xn | y1, . . . , yn) = P (x1 | y1, . . . , yn) · . . . · P (xn | y1, . . . , yn)\\n= P (x1 | y1) · . . . · P (xn | yn)\\n=\\nn\\nY\\ni=1\\nP (xi | yi)\\n综上所述, 可得\\nP (x1, y1, . . . , xn, yn) = P (x1, . . . , xn | y1, . . . , yn) · P (y1, . . . , yn)\\n=\\n n\\nY\\ni=1\\nP (xi | yi)\\n!\\n·\\n \\nP (y1)\\nn\\nY\\ni=2\\nP (yi | yi−1)\\n!\\n= P (y1) P (x1 | y1)\\nn\\nY\\ni=2\\nP (yi | yi−1) P (xi | yi)\\n14.1.3', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 179, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='n\\nY\\ni=2\\nP (yi | yi−1) P (xi | yi)\\n14.1.3\\n隐马尔可夫模型的三组参数\\n状态转移概率和输出观测概率都容易理解, 简单解释一下初始状态概率。特别注意, 初始状态概率中\\nπi = P (y1 | si) , 1 ⩽i ⩽N, 这里只有y1, 因为y2 及以后的其它状态是由状态转移概率和y1 确定的, 具体\\n参见课本第321 页“给定隐马尔可夫模型λ, 它按如下过程产生观测序列{x1, x2, . . . , xn} ” 的四个步骤。\\n14.2\\n马尔可夫随机场\\n本节介绍无向图模型的著名代表之一：马尔可夫随机场。本节的部分概念（例如势函数、极大团等）\\n比较抽象，我亦无好办法，只能建议多读几遍，从心里接受这些概念就好。另外，从因果关系角度来讲，\\n首先是因为满足全局、局部或成对马尔可夫性的无向图模型称为马尔可夫随机场，所以马尔可夫随机场才\\n具有全局、局部或成对马尔可夫性。\\n14.2.1\\n式(14.2) 和式(14.3) 的解释\\n注意式(14.2) 之前的介绍是“则联合概率P(x) 定义为”, 而在式(14.3) 之前也有类似的描述。因', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 179, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='此, 可以将式(14.2) 和式(14.3) 理解为一种定义, 记住并接受这个定义就好了。实际上, 该定义是根据\\nHammersley-Clifford 定理而得, 可以具体了解一下该定理, 这里不再赘述。\\n值得一提的是, 在接下来讨论“条件独立性”时, 即式(14.4) 式(14.7) 的推导过程直接使用了该定义。\\n注意：在有了式(14.3) 的定义后, 式(14.2) 已作废, 不再使用。\\n14.2.2\\n式(14.4) 到式(14.7) 的推导\\n首先, 式(14.4) 直接使用了式(14.3) 有关联合概率的定义。\\n对于式(14.5), 第一行两个等号变形就是概率论中的知识; 第二行的变形直接使用了式(14.3) 有关联\\n合概率的定义; 第三行中, 由于ψAC (x′\\nA, xC) 与变量x′\\nB 无关, 因此可以拿到求和号P\\nx′\\nB 外面, 即\\nX\\nx′\\nA\\nX\\nx′\\nB\\nψAC (x′\\nA, xC) ψBC (x′\\nB, xC) =\\nX\\nx′\\nA\\nψAC (x′\\nA, xC)\\nX\\nx′\\nB\\nψBC (x′\\nB, xC)\\n→_→', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 179, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='X\\nx′\\nA\\nψAC (x′\\nA, xC)\\nX\\nx′\\nB\\nψBC (x′\\nB, xC)\\n→_→\\n配套视频教程：https://www.bilibili.com/video/BV1Mh411e7VU\\n←_←', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 179, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='→_→\\n欢迎去各大电商平台选购纸质版南瓜书《机器学习公式详解第2 版》\\n←_←\\n举个例子, 假设x = {x1, x2, x3} , y = {y1, y2, y3}, 则\\n3\\nX\\ni=1\\n3\\nX\\nj=1\\nxiyj = x1y1 + x1y2 + x1y3 + x2y1 + x2y2 + x2y3 + x3y1 + x3y2 + x3y3\\n= x1 × (y1 + y2 + y3) + x2 × (y1 + y2 + y3) + x3 × (y1 + y2 + y3)\\n= (x1 + x2 + x3) × (y1 + y2 + y3) =\\n \\n3\\nX\\ni=1\\nxi\\n!  \\n3\\nX\\nj=1\\nyj\\n!\\n同理可得式(14.6)。类似于式(14.6), 还可以得到P (xB | xC) =\\nψBC(xB,xC)\\n∑\\nx′\\nB ψBC(x′\\nB,xC)\\n最后, 综合可得式(14.7) 成立, 即马尔可夫随机场“条件独立性”得证。\\n14.2.3\\n马尔可夫毯(Markov blanket)\\n本节共提到三个性质, 分别是全局马尔可夫性、局部马尔可夫性和成对马尔可夫性, 三者本质上是一', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 180, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='本节共提到三个性质, 分别是全局马尔可夫性、局部马尔可夫性和成对马尔可夫性, 三者本质上是一\\n样的, 只是适用场景略有差异。\\n在“西瓜书”第325 页左上角边注提到“马尔可夫\\x00”的概念, 专门提一下这个概念主要是其名字与\\n马尔可夫链、隐马尔可夫模型、马尔可夫随机场等很像; 但实际上, 马尔可夫\\x00是一个局部的概念, 而马尔\\n可夫链、隐马尔可夫模型、马尔可夫随机场则是整体模型级别的概念。\\n对于某变量, 当它的马尔可夫\\x00 (即其所有邻接变量, 包含父变量、子变量、子变量的其他父变量等组\\n成的集合）确定时, 则该变量条件独立于其它变量, 即局部马尔可夫性。\\n14.2.4\\n势函数(potential function)\\n势函数贯穿本节，但却一直以抽象函数符号形式出现，直到本节最后才简单介绍势函数的具体形式，\\n个人感觉这为理解本节内容增加不少难度。具体来说，若已知势函数，例如以“西瓜书”图14.4 为例的和\\n取值，则可以根据式(14.3) 基于最大团势函数定义的联合概率公式解得各种可能变量值指派的联合概率，', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 180, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='进而完成一些预测工作；若势函数未知，在假定势函数的形式之后，应该就需要根据数据去学习势函数的\\n参数。\\n14.2.5\\n式(14.8) 的解释\\n此为势函数的定义式，即将势函数写作指数函数的形式。指数函数满足非负性，且便于求导，因此在\\n机器学习中具有广泛应用，例如西瓜书式(8.5) 和式(13.11)。\\n14.2.6\\n式(14.9) 的解释\\n此为定义在变量xQ 上的函数HQ (·) 的定义式，第二项考虑单节点，第一项考虑每一对节点之间的关\\n系。\\n14.3\\n条件随机场\\n条件随机场是给定一组输入随机变量x 条件下, 另一组输出随机变量y 构成的马尔可夫随机场, 即本\\n页边注中所说“条件随机场可看作给定观测值的马尔可夫随机场”, 条件随机场的“条件”应该就来源于\\n此吧, 因为需要求解的概率为条件联合概率P(y | x), 因此它是一种判别式模型, 参见“西瓜书”图14.6。\\n14.3.1\\n式(14.10) 的解释\\nP\\n\\x00yv|x, yV \\\\{v}\\n\\x01\\n= P\\n\\x00yv|x, yn(v)\\n\\x01\\n→_→\\n配套视频教程：https://www.bilibili.com/video/BV1Mh411e7VU', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 180, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='←_←', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 180, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='→_→\\n欢迎去各大电商平台选购纸质版南瓜书《机器学习公式详解第2 版》\\n←_←\\n[解析]：根据局部马尔科夫性，给定某变量的邻接变量，则该变量独立与其他变量，即该变量只与其邻接\\n变量有关，所以式(14.10) 中给定变量v 以外的所有变量与仅给定变量v 的邻接变量是等价的。\\n特别注意, 本式下方写到“则(y, x) 构成一个条件随机场”; 也就是说, 因为(y, x) 满足式(14.10), 所\\n以(y, x) 构成一个条件随机场, 类似马尔可夫随机场与马尔可夫性的因果关系。\\n14.3.2\\n式(14.11) 的解释\\n注意本式前面的话：\\n“条件概率被定义为”\\n。至于式中使用的转移特征函数和状态特征函数，一般这两\\n个函数取值为1 或0，当满足特征条件时取值为1，否则为0。\\n14.3.3\\n学习与推断\\n本节前4 段内容（标题“14.4.1 变量消去”之前）至关重要，可以看作是14.4 节和14.5 节的引言，为\\n后面这两节内容做铺垫，因此一定要反复研读几遍，因为这几段内容告诉你接下来两节要解决什么问题，\\n心中装着问题再去看书会事半功倍，否则即使推明白了公式也不知道为什么要去推这些公式。本节介绍两', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 181, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='心中装着问题再去看书会事半功倍，否则即使推明白了公式也不知道为什么要去推这些公式。本节介绍两\\n种精确推断方法，下一节则介绍两种近似推断方法。\\n14.3.4\\n式(14.14) 的推导\\n该式本身的含义很容易理解, 即为了求P (x5) 对联合分布中其他无关变量（即x1, x2, x3, x4 ）进行积\\n分(或求和) 的过程, 也就是“边际化”(marginalization)。\\n关键在于为什么从第1 个等号可以得到第2 个等号, 边注中提到“基于有向图模型所描述的条件独立\\n性”, 此即第7 章式(7.26)。这里的变换类似于式(7.27) 的推导过程, 不再赘述。\\n总之，在消去变量的过程中，在消去每一个变量时需要保证其依赖的变量已经消去，因此消去顺序应\\n该是有向概率图中的一条以目标节点为终点的拓扑序列。\\n14.3.5\\n式(14.15) 和式(14.16) 的推导\\n这里定义新符号mij (xj), 请一定理解并记住其含义。依次推导如下:\\nm12 (x2) =\\nX\\nx1\\nP (x1) P (x2 | x1) =\\nX\\nx1\\nP (x2, x1) = P (x2)\\nm23 (x3) =\\nX\\nx2', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 181, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='X\\nx1\\nP (x2, x1) = P (x2)\\nm23 (x3) =\\nX\\nx2\\nP (x3 | x2) m12 (x2) =\\nX\\nx2\\nP (x3, x2) = P (x3)\\nm43 (x3) =\\nX\\nx4\\nP (x4 | x3) m23 (x3) =\\nX\\nx4\\nP (x4, x3) = P (x3) (这里与书中不一样\\n!\\nm35 (x5) =\\nX\\nx3\\nP (x5 | x3) m43 (x3) =\\nX\\nx3\\nP (x5, x3) = P (x5)\\n注意: 这里的过程与“西瓜书”中不太一样, 但本质一样, 因为m43 (x3) = P\\nx4 P (x4 | x3) = 1 。\\n14.3.6\\n式(14.17) 的解释\\n忽略图14.7(a) 中的箭头，然后把无向图中的每条边的两个端点作为一个团将其分解为四个团因子的\\n乘积。Z 为规范化因子确保所有可能性的概率之和为1。本式就是基于极大团定义的联合概率分布，参见\\n式(14.3)。\\n→_→\\n配套视频教程：https://www.bilibili.com/video/BV1Mh411e7VU\\n←_←', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 181, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='→_→\\n欢迎去各大电商平台选购纸质版南瓜书《机器学习公式详解第2 版》\\n←_←\\n14.3.7\\n式(14.18) 的推导\\n原理同式14.15, 区别在于把条件概率替换为势函数。\\n由于势函数的定义是抽象的, 无法类似于P\\nx4 P (x4 | x3) =\\n1 去处理P\\nx4 ψ (x3, x4) 。\\n但根据边际化运算规则，可以知道：\\nm12 (x2) = P\\nx1 ψ12 (x1, x2) 只含x2 不含x1;\\nm23 (x3) = P\\nx2 ψ23 (x2, x3) m12 (x2) 只含x3 不含x2;\\nm43 (x3) = P\\nx4 ψ34 (x3, x4) m23 (x3) 只含x3 不含x4;\\nm35 (x5) = P\\nx3 ψ35 (x3, x5) m43 (x3) 只含x5 不含x3, 即最后得到P (x5) 。\\n14.3.8\\n式(14.19) 的解释\\n首先解释符号含义, k ∈n(i)\\\\j 表示k 属于除去j 之外的xi 的邻接结点, 例如n(1)\\\\2 为空集(因为', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 182, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='x1 只有邻接结点2 ), n(2)\\\\3 = {1} (因为x2 有邻接结点1 和3 )，n(4) n3 为空集(因为x4 只有邻接结\\n点3 ), n(3)\\\\5 = {2, 4} （因为x3 有邻接结点2,4 和5 )。\\n接下来, 仍然以图14.7 计算P (x5) 为例:\\nm12 (x2) =\\nX\\nx1\\nψ12 (x1, x2)\\nY\\nk∈n(1)\\\\2\\nmk1 (x1) =\\nX\\nx1\\nψ12 (x1, x2)\\nm23 (x3) =\\nX\\nx2\\nψ23 (x2, x3)\\nY\\nk∈n(2)\\\\3\\nmk2 (x2) =\\nX\\nx1\\nψ12 (x1, x2) m12 (x2)\\nm43 (x3) =\\nX\\nx4\\nψ34 (x3, x4)\\nY\\nk∈n(4)\\\\3\\nmk4 (x4) =\\nX\\nx4\\nψ34 (x3, x4)\\nm35 (x5) =\\nX\\nx3\\nψ35 (x3, x5)\\nY\\nk∈n(3)\\\\5\\nmk3 (x3) =\\nX\\nx3\\nψ35 (x3, x5) m23 (x3) m43 (x3)\\n该式表示从节点i 传递到节点j 的过程，求和号表示要考虑节点i 的所有可能取值。连乘号解释见式', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 182, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='该式表示从节点i 传递到节点j 的过程，求和号表示要考虑节点i 的所有可能取值。连乘号解释见式\\n14.20。应当注意这里连乘号的下标不包括节点j，节点i 只需要把自己知道的关于j 以外的消息告诉节点\\nj 即可。\\n14.3.9\\n式(14.20) 的解释\\n应当注意这里是正比于而不是等于，因为涉及到概率的规范化。可以这么解释，每个变量可以看作一\\n个有一些邻居的房子，每个邻居根据其自己的见闻告诉你一些事情(消息)，任何一条消息的可信度应当与\\n所有邻居都有相关性，此处这种相关性用乘积来表达。\\n14.3.10\\n式(14.22) 的推导\\n假设x 有M 种不同的取值，xi 的采样数量为mi(连续取值可以采用微积分的方法分割为离散的取\\n值)，则\\nˆ\\nf = 1\\nN\\nM\\nX\\nj=1\\nf (xj) · mj\\n=\\nM\\nX\\nj=1\\nf (xj) · mj\\nN\\n≈\\nM\\nX\\nj=1\\nf (xj) · p(xj)\\n≈\\nZ\\nf(x)p(x)dx\\n→_→\\n配套视频教程：https://www.bilibili.com/video/BV1Mh411e7VU\\n←_←', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 182, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='→_→\\n欢迎去各大电商平台选购纸质版南瓜书《机器学习公式详解第2 版》\\n←_←\\n14.3.11\\n图14.8 的解释\\n图(a) 表示信念传播算法的第1 步, 即指定一个根结点, 从所有叶结点开始向根结点传递消息, 直到根\\n结点收到所有邻接结点的消息; 图(b) 表示信念传播算法的第2 步, 即从根结点开始向叶结点传递消息, 直\\n到所有叶结点均收到消息。\\n本图并不难理解, 接下来思考如下两个问题:\\n【思考1】如何编程实现本图信念传播的过程? 这其中涉及到很多问题, 例如从叶结点x4 向根结点传\\n递消息时, 当传递到x3 时如何判断应该向x2 传递还是向x5 传递? 当然, 你可能感觉x5 是叶结点, 所以\\n肯定是向x2 传递, 那是因为这个无向图模型很简单, 如果x5 和x3 之间还有很多个结点呢?\\n【思考2】14.4.2 节开头就说到“信念传播...... 较好地解决了求解多个边际分布时的重复计算问题”,\\n但如果图模型很复杂而我本身只需要计算少量边际分布, 是否还应该使用信念传播呢? 其实计算边际分布\\n类似于第10.1 节提到的“懒惰学习”, 只有在计算边际分布时才需要计算某些“消息”', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 183, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='类似于第10.1 节提到的“懒惰学习”, 只有在计算边际分布时才需要计算某些“消息”\\n。这可能要根据实\\n际情况在变量消去和信念传播两种方法之间取舍。\\n14.4\\n近似推断\\n本节介绍两种近似推断方法：\\nMCMC 采样和变分推断。\\n提到推断，\\n一般是为了求解某个概率分布\\n（参\\n见上一节的例子）\\n，但需要特别说明的是，本节将要介绍的MCMC 采样并不是为了求解某个概率分布，而\\n是在已知某个概率分布的前提下去构造服从该分布的独立同分布的样本集合，理解这一点对于读懂14.5.1\\n节的内容非常关键，即14.5.1 节中的p(x) 是已知的；变分推断是概率图模型常用的推断方法，要尽可能\\n理解并掌握其中的细节。\\n14.4.1\\n式(14.21) 到式(14.25) 的解释\\n这五个公式都是概率论课程中的基本公式, 很容易理解; 从14.5.1 节开始到式(14.25), 实际都在为\\nMCMC 采样做铺垫, 即为什么要做MCMC 采样? 以下分三点说明:\\n(1) 若已知概率密度函数p(x), 则可通过式(14.21) 计算函数f(x) 在该概率密度函数p(x) 下的期望;', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 183, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='这个过程也可以先根据p(x) 抽取一组样本再通过式(14.22) 近似完成。\\n(2) 为什么要通过式(14.22) 近似完成呢? 这是因为“若x 不是单变量而是一个高维多元变量x, 且服\\n从一个非常复杂的分布, 则对式(14.24) 求积分通常很困难”\\n。\\n(3) “然而, 若概率密度函数p(x) 很复杂, 则构造服从p 分布的独立同分布样本也很困难”, 这时可以\\n使用MCMC 采样技术完成采样过程。\\n式(14.23) 就是在区间A 中的概率计算公式, 而式(14.24) 与式(14.21) 的区别也就在于式(14.24) 限\\n定了积分变量x 的区间(可能写成定积分形式可能更容易理解)。\\n14.4.2\\n式(14.26) 的解释\\n假设变量x 所在的空间有n 个状态(s1, s2, .., sn), 定义在该空间上的一个转移矩阵T ∈Rn×n 满足一\\n定的条件则该马尔可夫过程存在一个稳态分布π, 使得\\nπT = π\\n其中, π 是一个是一个n 维向量，代表s1, s2, .., sn 对应的概率. 反过来, 如果我们希望采样得到符合某个', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 183, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='分布π 的一系列变量x1, x2, .., xt, 应当采用哪一个转移矩阵T ∈Rn×n 呢？\\n事实上，转移矩阵只需要满足马尔可夫细致平稳条件\\nπiTij = πjTji\\n→_→\\n配套视频教程：https://www.bilibili.com/video/BV1Mh411e7VU\\n←_←', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 183, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='→_→\\n欢迎去各大电商平台选购纸质版南瓜书《机器学习公式详解第2 版》\\n←_←\\n即式(14.26)，这里采用的符号与西瓜书略有区别以便于理解. 证明如下\\nπTj· =\\nX\\ni\\nπiTij =\\nX\\ni\\nπjTji = πj\\n假设采样得到的序列为x1, x2, .., xt−1, xt，则可以使用MH 算法来使得xt−1(假设为状态si) 转移到xt(假\\n设为状态sj) 的概率满足式。\\n本式为某个时刻马尔可夫链平稳的条件, 注意式中的p (xt) 和p (xt−1) 已知, 但状态转移概率T (xt−1 | xt)\\n和T (xt | xt−1) 末知。如何构建马尔可夫链转移概率至关重要, 不同的构造方法将产生不同的MCMC 算\\n法（可以认为MCMC 算法是一个大的框架或一种思想, 即“MCMC 方法先设法构造一条马尔可夫链, 使\\n其收玫至平稳分布恰为待估计参数的后验分布, 然后通过这条马尔可夫链来产生符合后验分布的样本, 并\\n基于这些样本来进行估计”, 具体如何构建马尔可夫链有多种实现途径, 接下来介绍的MH 算法就是其中\\n一种)。\\n14.4.3\\n式(14.27) 的解释', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 184, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='一种)。\\n14.4.3\\n式(14.27) 的解释\\n若将本式xt−1 和x∗分别对应式(14.27) 的xt 和xt−1, 则本式与式(14.27) 区别仅在于状态转移概率\\nT (x∗| xt−1) 由先验概率Q (x∗| xt−1) 和被接受的概率A (x∗| xt−1) 的乘积表示。\\n14.4.4\\n式(14.28) 的推导\\n注意, 本式中的概率分布p(x) 和先验转移概率Q 均为已知, 因此可计算出接受概率。将本式代入式\\n(14.27) 可以验证本式是正确的。具体来说, 式(14.27) 等号左边将变为:\\np\\n\\x00xt−1\\x01\\nQ\\n\\x00x∗| xt−1\\x01\\nA\\n\\x00x∗| xt−1\\x01\\n=p\\n\\x00xt−1\\x01\\nQ\\n\\x00x∗| xt−1\\x01\\nmin\\n\\x12\\n1, p (x∗) Q (xt−1 | x∗)\\np (xt−1) Q (x∗| xt−1)\\n\\x13\\n= min\\n\\x12\\np\\n\\x00xt−1\\x01\\nQ\\n\\x00x∗| xt−1\\x01\\n, p\\n\\x00xt−1\\x01\\nQ\\n\\x00x∗| xt−1\\x01 p (x∗) Q (xt−1 | x∗)\\np (xt−1) Q (x∗| xt−1)\\n\\x13\\n= min\\n\\x00p\\n\\x00xt−1\\x01\\nQ\\n\\x00x∗| xt−1\\x01', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 184, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='\\x13\\n= min\\n\\x00p\\n\\x00xt−1\\x01\\nQ\\n\\x00x∗| xt−1\\x01\\n, p (x∗) Q\\n\\x00xt−1 | x∗\\x01\\x01\\n将A (xt−1 | x∗) 代入右边(符号式xt−1 和x∗调换位置), 同理可得如上结果, 即本式的接受概率形式\\n可保证式(14.27) 成立。\\n验证完毕之后可以再做一个简单的推导。其实若想要式(14.27) 成立, 简单令:\\nA\\n\\x00x∗| xt−1\\x01\\n= C · p (x∗) Q\\n\\x00xt−1 | x∗\\x01\\n(则等号右则的A (xt−1 | x∗) = C · p (xt−1) Q (x∗| xt−1) )\\n即可, 其中C 为大于零的常数, 且不能使A (x∗| xt−1) 和A (xt−1 | x∗) 大于1 （因为它们是概率)。注\\n意待解A (x∗| xt−1) 为接受概率, 在保证式(14.27) 成立的基础上, 其值应该尽可能大一些(但概率值不会\\n超过1 ), 否则在图14.9 描述的MH 算法中采样出的候选样本将会有大部分会被拒绝。所以, 常数C 尽可\\n能大一些, 那么C 最大可以为多少呢?', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 184, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='能大一些, 那么C 最大可以为多少呢?\\n对于A (x∗| xt−1) = C·p (x∗) Q (xt−1 | x∗), 易知C 最大可以取值\\n1\\np(x∗)Q(xt−1|x∗), 再大则会使A (x∗| xt−1)\\n大于1 ; 对于A (xt−1 | x∗) = C · p (xt−1) Q (x∗| xt−1), 易知C 最大可以取值\\n1\\np(xt−1)Q(x∗|xt−1); 常数C 的取\\n值需要同时满足两个约束, 因此\\nC = min\\n\\x12\\n1\\n·p (x∗) Q (xt−1 | x∗),\\n1\\np (xt−1) Q (x∗| xt−1)\\n\\x13\\n将这个常数C 的表达式代入A (x∗| xt−1) = C · p (x∗) Q (xt−1 | x∗) 即得式(14.28)。\\n→_→\\n配套视频教程：https://www.bilibili.com/video/BV1Mh411e7VU\\n←_←', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 184, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='→_→\\n欢迎去各大电商平台选购纸质版南瓜书《机器学习公式详解第2 版》\\n←_←\\n14.4.5\\n吉布斯采样与MH 算法\\n这里解释一下为什么说吉布斯采样是MH 算法的特例。\\n吉布斯采样算法如下(“西瓜书”第334 页):\\n(1) 随机或以某个次序选取某变量xi;\\n(2) 根据x 中除xi 外的变量的现有取值, 计算条件概率p (xi | x¯\\ni), 其中x¯\\ni = {x1, x2, . . . , xi−1, xi+1, . . . , xN};\\n(3) 根据p (xi | x¯\\ni) 对变量xi 采样, 用采样值代替原值。\\n对应到式(14.27) 和式(14.28) 表示的MH 采样, 候选样本x∗与t −1 时刻样本xt−1 的区别仅在于第\\ni 个变量的取值不同, 即x∗\\n¯\\ni 与xt−1\\n¯\\ni\\n相同。先给几个概率等式:\\n(1) Q (x∗| xt−1) = p\\n\\x00x∗\\ni | xt−1\\n¯\\ni\\n\\x01\\n；\\n(2) Q (xt−1 | x∗) = p\\n\\x00xt−1\\ni\\n| x∗\\n¯\\ni\\n\\x01\\n；\\n(3) p (x∗) = p\\n\\x00x∗\\ni , x∗\\n¯\\ni\\n\\x01\\n= p\\n\\x00x∗', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 185, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='¯\\ni\\n\\x01\\n；\\n(3) p (x∗) = p\\n\\x00x∗\\ni , x∗\\n¯\\ni\\n\\x01\\n= p\\n\\x00x∗\\ni | x∗\\n¯\\ni\\n\\x01\\np\\n\\x00x∗\\n¯\\ni\\n\\x01\\n；\\n(4) p (xt−1) = p\\n\\x00xt−1\\ni\\n, xt−1\\n¯\\ni\\n\\x01\\n= p\\n\\x00xt−1\\ni\\n| xt−1\\n¯\\ni\\n\\x01\\np\\n\\x00xt−1\\n¯\\ni\\n\\x01\\n。\\n其中等式(1) 是由于吉布斯采样中“根据p (xi | xi) 对变量xi 采样”(参见以上第(3) 步), 即用户给\\n定的先验概率为p (xi | x¯\\ni), 同理得等式(2); 等式(3) 就是将联合概率p (x∗) 换了种形式, 然写成了条件概\\n率和先验概率乘积, 同理得等式(4)。\\n对于式(14.28) 来说(注意: x∗\\n¯\\ni = xt−1\\n¯\\ni\\n)\\np (x∗) Q (xt−1 | x∗)\\np (xt−1) Q (x∗| xt−1) =\\np (x∗\\ni | x∗\\ni ) p (x∗\\ni ) p\\n\\x00xt−1\\ni\\n| x∗\\n¯\\ni\\n\\x01\\np\\n\\x00xt−1\\ni\\n| xt−1\\n¯\\ni\\n\\x01\\np\\n\\x00xt−1\\n¯\\ni\\n\\x01\\np\\n\\x00x∗\\ni | xt−1\\n¯\\ni\\n\\x01 = 1', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 185, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='¯\\ni\\n\\x01\\np\\n\\x00xt−1\\n¯\\ni\\n\\x01\\np\\n\\x00x∗\\ni | xt−1\\n¯\\ni\\n\\x01 = 1\\n即在吉布斯采样中接受概率恒等于1 , 也就是说吉布斯采样是接受概率为1 的MH 采样。\\n该推导参考了PRML\\n[1] 第544 页。\\n14.4.6\\n式(14.29) 的解释\\n连乘号是因为N 个变量的生成过程相互独立。求和号是因为每个变量的生成过程需要考虑中间隐变\\n量的所有可能性，类似于边际分布的计算方式。\\n14.4.7\\n式(14.30) 的解释\\n对式(14.29) 取对数。本式就是求对数后, 原来的连乘变为了连加, 即性质ln(ab) = ln a + ln b 。\\n接下来提到“图14.10 所对应的推断和学习任务主要是由观察到的变量x 来估计隐变量Z 和分布参\\n数变量Θ, 即求解p(z | x, Θ) 和Θ ”, 这里可以对应式(3.26) 来这样不严谨理解: Θ 对应式(3.26) 的w, b,\\n而z 对应式(3.26) 的y 。\\n14.4.8\\n式(14.31) 的解释\\n对应7.6 节EM 算法中的M 步，参见第163 页的式(7.36) 和式(7.37)。\\n14.4.9', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 185, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='14.4.9\\n式(14.32) 到式(14.34) 的推导\\n从式(14.31) 到式(14.32) 之间的跳跃比较大, 接下来为了方便忽略分布参数变量Θ 。这里的主要问\\n题是后验概率p(z | x) 难于获得, 进而使用一个已知简单分布q(z) 去近似需要推导的复杂分布p(z | x), 这\\n就是变分推断的核心思想。\\n根据概率论公式p(x, z) = p(z | x)p(x), 得:\\np(x) = p(x, z)\\np(z | x)\\n→_→\\n配套视频教程：https://www.bilibili.com/video/BV1Mh411e7VU\\n←_←', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 185, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='→_→\\n欢迎去各大电商平台选购纸质版南瓜书《机器学习公式详解第2 版》\\n←_←\\n分子分母同时除以q(z), 得:\\np(x) = p(x, z)/q(z)\\np(z | x)/q(z)\\n等号两边同时取自然对数, 得:\\nln p(x) = ln p(x, z)/q(z)\\np(z | x)/q(z) = ln p(x, z)\\nq(z)\\n−ln p(z | x)\\nq(z)\\n等号两边同时乘以q(z) 并积分, 得:\\nZ\\nq(z) ln p(x)dz =\\nZ\\nq(z) ln p(x, z)\\nq(z) dz −\\nZ\\nq(z) ln p(z | x)\\nq(z)\\ndz\\n对于等号左边的积分, 由于p(x) 与变量z 无关, 因此可以当作常数拿到积分号外面:\\nZ\\nq(z) ln p(x)dz = ln p(x)\\nZ\\nq(z)dz = ln p(x)\\n其中q(z) 为一个概率分布, 所以积分等于1 。至此, 前面式子变为:\\nln p(x) =\\nZ\\nq(z) ln p(x, z)\\nq(z) dz −\\nZ\\nq(z) ln p(z | x)\\nq(z)\\ndz', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 186, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='q(z) dz −\\nZ\\nq(z) ln p(z | x)\\nq(z)\\ndz\\n此即式(14.32), 等号右边第1 项即式(14.33) 称为Evidence Lower Bound (ELBO), 等号右边第2 项\\n即式(14.34) 为KL 散度\\n（参见附录C.3）\\n。\\n我们的目标是用分布q(z) 去近似后验概率p(z | x), 而KL 散度\\n用于度量两个概率分布之间的差异, 其中KL 散度越小表示两个分布差异越小, 因此可以最小化式(14.34):\\nmin\\nq(z) KL(q(z)∥p(z | x))\\n但这并没有什么意义, 因为p(z | x) 末知。注意, 式(14.32) 恒等于常数ln p(x), 因此最小化式(14.34)\\n等价于最大化式(14.33) 的ELBO。在本节接下来的推导中, 就是通过最大化式(14.33) 来求解p(z | x) 的\\n近似q(z) 。\\n14.4.10\\n式(14.35) 的解释\\n在“西瓜书”14.5.2 节开篇提到, “变分推断通过使用已知简单分布来逼近需推断的复杂分布”, 这里', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 186, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='我们使用q(z) 去近似后验分布p(z | x) 。而本式进一步假设复杂的多变量z 可拆解为一系列相互独立的\\n多变量zi, 进而有q(z) = QM\\ni=1 qi (zi), 以便于后面简化求解。\\n14.4.11\\n式(14.36) 的推导\\n将式(14.35) 代入式(14.33), 得:\\nL(q) =\\nZ\\nq(z) ln p(x, z)\\nq(z) dz =\\nZ\\nq(z){ln p(x, z) −ln q(z)}dz\\n=\\nZ\\nM\\nY\\ni=1\\nqi (zi)\\n(\\nln p(x, z) −ln\\nM\\nY\\ni=1\\nqi (zi)\\n)\\ndz\\n=\\nZ\\nM\\nY\\ni=1\\nqi (zi) ln p(x, z)dz −\\nZ\\nM\\nY\\ni=1\\nqi (zi) ln\\nM\\nY\\ni=1\\nqi (zi) dz ≜L1(q) −L2(q)\\n接下来推导中大量使用交换积分号次序, 记积分项为Q(x, z), 则上式可变形为:\\nL(q) =\\nZ\\nQ(x, z)dz =\\nZ\\n· · ·\\nZ\\nQ(x, z)dz1 dz2 · · · dzM\\n根据积分相关知识, 在满足某种条件下, 积分号的次序可以任意交换。\\n→_→', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 186, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='根据积分相关知识, 在满足某种条件下, 积分号的次序可以任意交换。\\n→_→\\n配套视频教程：https://www.bilibili.com/video/BV1Mh411e7VU\\n←_←', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 186, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='→_→\\n欢迎去各大电商平台选购纸质版南瓜书《机器学习公式详解第2 版》\\n←_←\\n对于第1 项L1(q), 交换积分号次序, 得:\\nL1(q) =\\nZ\\nM\\nY\\ni=1\\nqi (zi) ln p(x, z)dz =\\nZ\\nqj\\n(Z\\nln p(x, z)\\nM\\nY\\ni̸=j\\n(qi (zi) dzi)\\n)\\ndzj\\n令ln ˜\\np (x, zj) =\\nR\\nln p(x, z) QM\\ni̸=j (qi (zi) dzi) （这里与式(14.37) 略有不同, 具体参见接下来一条的解\\n释), 代入, 得:\\nL1(q) =\\nZ\\nqj ln ˜\\np (x, zj) dzj\\n对于第2 项L2(q) :\\nL2(q) =\\nZ\\nM\\nY\\ni=1\\nqi (zi) ln\\nM\\nY\\ni=1\\nqi (zi) dz =\\nZ\\nM\\nY\\ni=1\\nqi (zi)\\nM\\nX\\ni=1\\nln qi (zi) dz\\n=\\nM\\nX\\ni=1\\nZ\\nM\\nY\\ni=1\\nqi (zi) ln qi (zi) dz =\\nM\\nX\\ni1=1\\nZ\\nM\\nY\\ni2=1\\nqi2 (zi2) ln qi1 (zi1) dz', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 187, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='M\\nX\\ni1=1\\nZ\\nM\\nY\\ni2=1\\nqi2 (zi2) ln qi1 (zi1) dz\\n解释一下第2 行的第2 个等号后的结果, 这是因为课本在这里符号表示并不严谨, 求和变量和连乘\\n变量不能同时使用i, 这里求和变量和连乘变量分布使用i1 和i2 表示。对于求和号内的积分项，考虑当\\ni1 = j 时:\\nZ\\nM\\nY\\ni2=1\\nqi2 (zi2) ln qj (zj) dz =\\nZ\\nqj (zj)\\nY\\ni2̸=j\\nqi2 (zi2) ln qj (zj) dz\\n=\\nZ\\nqj (zj) ln qj (zj)\\n(Z Y\\ni2̸=j\\nqi2 (zi2)\\nY\\ni2̸=j\\ndzi2\\n)\\ndzj\\n注意到\\nR Q\\ni2̸=j qi2 (zi2) Q\\ni2̸=j dzi2 = 1, 为了直观说明这个结论, 假设这里只有q1 (z1), q2 (z2) 和\\nq3 (z3), 即:\\nZZZ\\nq1 (z1) q2 (z2) q3 (z3) dz1 dz2 dz3 =\\nZ\\nq1 (z1)\\nZ\\nq2 (z2)\\nZ\\nq3 (z3) dz3 dz2 dz1\\n对于概率分布, 我们有\\nR\\nq1 (z1) dz1 =', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 187, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='Z\\nq3 (z3) dz3 dz2 dz1\\n对于概率分布, 我们有\\nR\\nq1 (z1) dz1 =\\nR\\nq2 (z2) dz2 =\\nR\\nq3 (z3) dz3 = 1, 代入即得。因此:\\nZ\\nM\\nY\\ni2=1\\nqi2 (zi2) ln qj (zj) dz =\\nZ\\nqj (zj) ln qj (zj) dzj\\n进而第2 项可化简为:\\nL2(q) =\\nM\\nX\\ni1=1\\nZ\\nqi1 (zi1) ln qi1 (zi1) dzi1\\n=\\nZ\\nqj (zj) ln qj (zj) dzj +\\nM\\nX\\ni1̸=j\\nZ\\nqi1 (zi1) ln qi1 (zi1) dzi1\\n由于这里只关注qj （即固定qi̸=j ）, 因此第2 项进一步表示为第j 项加上一个常数:\\nL2(q) =\\nZ\\nqj (zj) ln qj (zj) dzj + const\\n综上所述, 可得式(14.36) 的形式。\\n14.4.12\\n式(14.37) 到式(14.38) 的解释\\n首先解释式(14.38), 该式等号右侧就是式(14.36) 第2 个等号后面花括号中的内容, 之所以这里写成\\n了期望的形式, 这是将Q', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 187, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='了期望的形式, 这是将Q\\ni̸=j qi 看作为一个概率分布, 则该式表示函数ln p(x, z) 在概率分布Q\\ni̸=j qi 下的\\n期望, 类似于式(14.21) 和式(14.24)。\\n→_→\\n配套视频教程：https://www.bilibili.com/video/BV1Mh411e7VU\\n←_←', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 187, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='→_→\\n欢迎去各大电商平台选购纸质版南瓜书《机器学习公式详解第2 版》\\n←_←\\n然后解释式(14.37), 该式就是一个定义, 即令等号右侧的项为ln ˜\\np (x, zj), 但该式却包含一个常数项\\nconst, 当然这并没有什么问题, 并不影响式(14.36) 本身。具体来说, 将本项反代回式(14.36) 第二个等号\\n右侧第1 项, 即:\\nZ\\nqj\\n(Z\\nln p(x, z)\\nM\\nY\\ni̸=j\\n(qi (zi) dzi)\\n)\\ndzj =\\nZ\\nqjEi̸=j[ln p(x, z)]dzj\\n=\\nZ\\nqj (ln ˜\\np (x, zj) −const ) dzj\\n=\\nZ\\nqj ln ˜\\np (x, zj) dzj −\\nZ\\nqj constd zj\\n=\\nZ\\nqj ln ˜\\np (x, zj) dzj −const\\n注意, 加或减一个常数const 实际等价, 只需const 定义时添个符号即可。将这个const 与式(14.36)\\n第2 个等号后面的const 合并（注意二者表示不同的值), 即式(14.36) 第3 个等号后面的const。\\n14.4.13\\n式(14.39) 的解释', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 188, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='14.4.13\\n式(14.39) 的解释\\n对于式(14.36), 可继续变形为:\\nL(q) =\\nZ\\nqj ln ˜\\np (x, zj) dzj −\\nZ\\nqj ln qj dzj + const\\n=\\nZ\\nqj ln ˜\\np (x, zj)\\nqj\\ndzj + const\\n= −KL (qj∥˜\\np (x, zj)) + const\\n注意, 在前面关于“式(14.32) 式(14.34) 的推导”中提到, 我们的目标是用分布q(z) 去近似后验概率\\np(z | x), 而KL 散度则用于度量两个概率分布之间的差异, 其中KL 散度越小表示两个分布差异越小, 因\\n此可以最小化式(14.34), 但这并没有什么意义, 因为p(z | x) 末知。又因为式(14.32) 恒等于常数ln p(x),\\n因此最小化式(14.34) 等价于最大化式(14.33)。刚刚又得到式(14.33) 等于−KL (qj∥˜\\np (x, zj)) + const, 因\\n此最大化式(14.33) 等价于最小化这里的KL 散度, 因此可知当qj = ˜\\np (x, zj) 时这个KL 散度最小, 即式', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 188, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='p (x, zj) 时这个KL 散度最小, 即式\\n(14.33) 最大, 也就是分布q(z) 与后验概率p(z | x) 最相似。\\n而根据式(14.37) 有ln ˜\\np (x, zj) = Ei̸=j[ln p(x, z)]+const, 再结合qj = ˜\\np (x, zj), 可知ln qj = Ei̸=j[ln p(x, z)]+\\nconst, 即本式。\\n14.4.14\\n式(14.40) 的解释\\n对式(14.39) 两边同时取exp(·) 操作, 得\\nq∗\\nj (zj) = exp (Ei̸=j[ln p(x, z)] + const )\\n= exp (Ei̸=j[ln p(x, z)]) · exp( const )\\n两边同时取积分\\nR\\n(·)dzj 操作, 由于q∗\\nj (zj) 为概率分布, 所以\\nR\\nq∗\\nj (zj) dzj = 1, 因此有\\n1 =\\nZ\\nexp (Ei̸=j[ln p(x, z)]) · exp( const )dzj\\n= exp( const )\\nZ\\nexp (Ei̸=j[ln p(x, z)]) dzj\\n这里就是将常数拿到了积分号外面, 因此:', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 188, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='exp (Ei̸=j[ln p(x, z)]) dzj\\n这里就是将常数拿到了积分号外面, 因此:\\nexp( const ) =\\n1\\nR\\nexp (Ei̸=j[ln p(x, z)]) dzj\\n→_→\\n配套视频教程：https://www.bilibili.com/video/BV1Mh411e7VU\\n←_←', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 188, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='→_→\\n欢迎去各大电商平台选购纸质版南瓜书《机器学习公式详解第2 版》\\n←_←\\n代入刚开始的表达式, 可得本式:\\nq∗\\nj (zj) = exp (Ei̸=j[ln p(x, z)]) · exp( const )\\n=\\nexp (Ei̸=j[ln p(x, z)])\\nR\\nexp (Ei̸=j[ln p(x, z)]) dzj\\n实际上, 本式的分母为归一化因子, 以保证q∗\\nj (zj) 为概率分布。\\n14.5\\n话题模型\\n本节介绍话题模型的概念及其典型代表：隐狄利克雷分配模型（LDA）\\n。\\n概括来说，给定一组文档，话题模型可以告诉我们这组文档谈论了哪些话题，以及每篇文档与哪些话\\n题有关。举个例子，社会中出现了一个热点事件，为了大致了解网民的思想动态，于是抓取了一组比较典\\n型的网页（博客、评论等）\\n；每个网页就是一篇文档，我们通过分析这组网页，可以大致了解到网民都从什\\n么角度关注这件事情（每个角度可视为一个主题，其中LDA 模型中主题个数需要人工指定）\\n，并大致知道\\n每个网页都涉及哪些角度；这里学得的主题类似于聚类（参见第9 章）中所得的簇（没有标记）\\n，每个主', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 189, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='每个网页都涉及哪些角度；这里学得的主题类似于聚类（参见第9 章）中所得的簇（没有标记）\\n，每个主\\n题最终由一个词频向量表示（即本节）\\n，通过分析该主题下的高频词，就可对其有大致的了解。\\n14.5.1\\n式(14.41) 的解释\\np(W , z, β, θ|α, η) =\\nT\\nY\\nt=1\\np(θt|α)\\nK\\nY\\nk=1\\np(βk|η)(\\nN\\nY\\nn=1\\nP(wt,n|zt,n, βk)P(zt,n|θt))\\n此式表示LDA 模型下根据参数α, η 生成文档W 的概率。其中z, β, θ 是生成过程的中间变量。具体的生\\n成步骤可见概率图14.12，图中的箭头和式14.41 中的条件概率中的因果项目一一对应。这里共有三个连\\n乘符号，表示三个相互独立的概率关系。第一个连乘表示T 个文档每个文档的话题分布都是相互独立的。\\n第二个连乘表示K 个话题每个话题下单词的分布是相互独立的。最后一个连乘号表示每篇文档中的所有\\n单词的生成是相互独立的。\\n14.5.2\\n式(14.42) 的解释\\n本式就是狄利克雷分布的定义式, 参见“西瓜书”附录C1.6。\\n14.5.3\\n式(14.43) 的解释', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 189, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='14.5.3\\n式(14.43) 的解释\\n本式为对数似然, 其中p (wt | α, η) =\\nRRR\\np (wt, z, β, Θ | α, η) dzdβdΘ, 即通过边际化p (wt, z, β, Θ | α, η)\\n而得。\\n由于T 篇文档相互独立, 所以p(W, z, β, Θ | α, η) = QT\\nt=1 p (wt, z, β, Θ | α, η), 求对数似然后连乘变\\n为了连加, 即得本式。参见7.2 极大似然估计。\\n14.5.4\\n式(14.44) 的解释\\n本式就是联合概率、先验概率、条件概率之间的关系, 换种表示方法可能更易理解:\\npα,η(z, β, Θ | W) = pα,η(W, z, β, Θ)\\npα,η(W)\\n参考文献\\n[1] Christopher M Bishop and Nasser M Nasrabadi. Pattern recognition and machine learning, volume 4.\\nSpringer, 2006.\\n→_→\\n配套视频教程：https://www.bilibili.com/video/BV1Mh411e7VU', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 189, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='←_←', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 189, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='→_→\\n欢迎去各大电商平台选购纸质版南瓜书《机器学习公式详解第2 版》\\n←_←\\n第15 章\\n规则学习\\n规则学习是“符号主义学习”的代表性方法，用来从训练数据中学到一组能对未见示例进行判别的规\\n则，形如“如果A 或B，并且C 的条件下，D 满足”这样的形式。因为这种学习方法更加贴合人类从数\\n据中学到经验的描述，具有非常良好的可解释性，是最早开始研究机器学习的技术之一。\\n15.1\\n剪枝优化\\n15.1.1\\n式(15.2) 和式(15.3) 的解释\\n似然率统计量LRS 定义为：\\nLRS = 2 ·\\n\\uf8eb\\n\\uf8edˆ\\nm+ log2\\n\\x10\\nˆ\\nm+\\nˆ\\nm++ ˆ\\nm−\\n\\x11\\n\\x10\\nm+\\nm++m−\\n\\x11 + ˆ\\nm−log2\\n\\x10\\nˆ\\nm−\\nˆ\\nm++ ˆ\\nm−\\n\\x11\\n\\x10\\nm−\\nm++m−\\n\\x11\\n\\uf8f6\\n\\uf8f8\\n同时，根据对数函数的定义，我们可以对式(15.3) 进行化简：\\nF−Gain = ˆ\\nm+ ×\\n\\x12\\nlog2\\nˆ\\nm+\\nˆ\\nm+ + ˆ\\nm−\\n−log2\\nm+\\nm+ + m−\\n\\x13\\n= ˆ\\nm+\\n \\nlog2\\nˆ\\nm+\\nˆ\\nm++ ˆ\\nm−\\nm+\\nm++m−\\n!', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 190, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='\\x13\\n= ˆ\\nm+\\n \\nlog2\\nˆ\\nm+\\nˆ\\nm++ ˆ\\nm−\\nm+\\nm++m−\\n!\\n可以观察到F_Gain 即为式(15.2) 中LRS 求和项中的第一项。这里“西瓜书”中做了详细的解释，FOIL\\n仅考虑正例的信息量，由于关系数据中正例数旺旺远少于反例数，因此通常对正例应该赋予更多的关注。\\n15.2\\n归纳逻辑程序设计\\n15.2.1\\n式(15.6) 的解释\\n定义析合范式的删除操作符为“−”\\n，表示在A 和B 的析合式中删除成分B，得到成分A。\\n15.2.2\\n式(15.7) 的推导\\nC = A ∨B，把A = C1 −{L} 和L = C2 −{¬L} 带入即得。\\n15.2.3\\n式(15.9) 的推导\\n根据式(15.7) C = (C1 −{L}) ∨(C2 −{¬L}) 和析合范式的删除操作，等式两边同时删除析合项\\nC2 −{¬L} 有：\\nC −(C1 −{L}) = C2 −{¬L}\\n再次运用析合范式删除操作符的逆定义，等式两边同时加上析合项{¬L} 有：\\nC2 = (C −(C1 −{L})) ∨{¬L}\\n15.2.4\\n式(15.10) 的解释', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 190, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='C2 = (C −(C1 −{L})) ∨{¬L}\\n15.2.4\\n式(15.10) 的解释\\n该式是吸收(absorption) 操作的定义。注意作者在文章中所用的符号定义，用X\\nY 表示X 蕴含Y ，X\\n的子句或是Y 的归结项，或是Y 中某个子句的等价项。所谓吸收，是指替换部分逻辑子句（大写字母）\\n，\\n生成一个新的逻辑文字（小写字母）用于定义这些被替换的逻辑子句。在式(15.10) 中，逻辑子句A 被逻\\n辑文字q 替换。\\n→_→\\n配套视频教程：https://www.bilibili.com/video/BV1Mh411e7VU\\n←_←', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 190, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='→_→\\n欢迎去各大电商平台选购纸质版南瓜书《机器学习公式详解第2 版》\\n←_←\\n15.2.5\\n式(15.11) 的解释\\n该式是辨识(identification) 操作的定义。辨识操作依据已知的逻辑文字，构造新的逻辑子句和文字的\\n关系。在式(15.11) 中，已知p ←A ∧B 和p ←A ∧q，构造的新逻辑文字为q ←B。\\n15.2.6\\n式(15.12) 的解释\\n该式是内构(intra-construction) 操作的定义。内构操作找到关于同一逻辑文字中的共同逻辑子句部\\n分，并且提取其中不同的部分作为新的逻辑文字。在式(15.12) 中，逻辑文字p ←A ∧B 和p ←A ∧C 的\\n共同部分为p ←A ∧q，其中新逻辑文字q ←B\\nq ←C。\\n15.2.7\\n式(15.13) 的解释\\n该式是互构(inter-construction) 操作的定义。互构操作找到不同逻辑文字中的共同逻辑子句部分，并\\n定义新的逻辑文字已描述这个共同的逻辑子句。在式(15.13) 中，逻辑文字p ←A ∧B 和q ←A ∧C 的', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 191, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='共同逻辑子句A 提取出来，并用逻辑文字定义为r ←A。逻辑文字p 和q 的定义也用r 做相应的替换得\\n到p ←r ∧B 与q ←r ∧C。\\n15.2.8\\n式(15.16) 的推导\\nθ1 为作者笔误，由15.9\\nC2 = (C −(C1 −{L1})) ∨{L2}\\n因为L2 = (¬L1θ1)θ−1\\n2 ，替换得证。\\n→_→\\n配套视频教程：https://www.bilibili.com/video/BV1Mh411e7VU\\n←_←', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 191, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='→_→\\n欢迎去各大电商平台选购纸质版南瓜书《机器学习公式详解第2 版》\\n←_←\\n第16 章\\n强化学习\\n强化学习作为机器学习的子领域，其本身拥有一套完整的理论体系，以及诸多经典和最新前沿算\\n法，“西瓜书”该章内容仅可作为综述查阅，若想深究建议查阅其他相关书籍（例如《Easy RL：强化\\n学习教程》\\n[1]）进行系统性学习。\\n16.1\\n任务与奖赏\\n本节理解强化学习的定义和相关术语的含义即可。\\n16.2\\nK-摇臂赌博机\\n16.2.1\\n式(16.2) 和式(16.3) 的推导\\nQn(k) = 1\\nn\\nn\\nX\\ni=1\\nvi\\n= 1\\nn\\n n−1\\nX\\ni=1\\nvi + vn\\n!\\n= 1\\nn ((n −1) × Qn−1(k) + vn)\\n= Qn−1(k) + 1\\nn (vn −Qn−1(k))\\n16.2.2\\n式(16.4) 的解释\\nP(k) =\\ne\\nQ(k)\\nτ\\nPK\\ni=1 e\\nQ(i)\\nτ\\n∝e\\nQ(k)\\nτ\\n∝Q(k)\\nτ\\n∝1\\nτ\\n如果τ 很大，\\n所有动作几乎以等概率选择\\n（探索）\\n；\\n如果τ 很小，\\nQ 值大的动作更容易被选中\\n（利用）\\n。\\n16.3\\n有模型学习\\n16.3.1', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 192, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='；\\n如果τ 很小，\\nQ 值大的动作更容易被选中\\n（利用）\\n。\\n16.3\\n有模型学习\\n16.3.1\\n式(16.7) 的解释\\n因为\\nπ(x, a) = P(action = a|state = x)\\n表示在状态x 下选择动作a 的概率，又因为动作事件之间两两互斥且和为动作空间，由全概率展开公式\\nP(A) =\\n∞\\nX\\ni=1\\nP(Bi)P(A | Bi)\\n可得\\nEπ[ 1\\nT r1 + T −1\\nT\\n1\\nT −1\\nT\\nX\\nt=2\\nrt | x0 = x]\\n=\\nX\\na∈A\\nπ(x, a)\\nX\\nx′∈X\\nP a\\nx→x′( 1\\nT Ra\\nx→x′ + T −1\\nT\\nEπ[\\n1\\nT −1\\nT −1\\nX\\nt=1\\nrt | x0 = x′])\\n其中\\nr1 = π(x, a)P a\\nx→x′Ra\\nx→x′\\n最后一个等式用到了递归形式。\\n→_→\\n配套视频教程：https://www.bilibili.com/video/BV1Mh411e7VU\\n←_←', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 192, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='→_→\\n欢迎去各大电商平台选购纸质版南瓜书《机器学习公式详解第2 版》\\n←_←\\nBellman 等式定义了当前状态与未来状态之间的关系，表示当前状态的价值函数可以通过下个状态的\\n价值函数来计算。\\n16.3.2\\n式(16.8) 的推导\\nV π\\nγ (x) = Eπ[\\n∞\\nX\\nt=0\\nγtrt+1 | x0 = x]\\n= Eπ[r1 +\\n∞\\nX\\nt=1\\nγtrt+1 | x0 = x]\\n= Eπ[r1 + γ\\n∞\\nX\\nt=1\\nγt−1rt+1 | x0 = x]\\n=\\nX\\na∈A\\nπ(x, a)\\nX\\nx′∈X\\nP a\\nx→x′(Ra\\nx→x′ + γEπ[\\n∞\\nX\\nt=0\\nγtrt+1 | x0 = x′])\\n=\\nX\\na∈A\\nπ(x, a)\\nX\\nx′∈X\\nP a\\nx→x′(Ra\\nx→x′ + γV π\\nγ (x′))\\n16.3.3\\n式(16.10) 的推导\\n参见式(16.7) 和式(16.8) 的推导\\n16.3.4\\n式(16.14) 的解释\\n为了获得最优的状态值函数V ，这里取了两层最优，分别是采用最优策略π∗和选取使得状态动作值\\n函数Q 最大的动作maxa∈A。\\n16.3.5', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 193, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='函数Q 最大的动作maxa∈A。\\n16.3.5\\n式(16.15) 的解释\\n最优Bellman 等式表明：最佳策略下的一个状态的价值必须等于在这个状态下采取最好动作得到的\\n累积奖赏值的期望。\\n16.3.6\\n式(16.16) 的推导\\nV π(x) ⩽Qπ (x, π′(x))\\n=\\nX\\nx′∈X\\nP π′(x)\\nx→x′\\n\\x10\\nRπ′(x)\\nx→x′ + γV π (x′)\\n\\x11\\n⩽\\nX\\nx′∈X\\nP π′(x)\\nx→x′\\n\\x10\\nRπ′(x)\\nx→x′ + γQπ (x′, π′ (x′))\\n\\x11\\n=\\nX\\nx′∈X\\nP π′(x)\\nx→x′\\n \\nRπ′(x)\\nx→x′ +\\nX\\nx′′∈X\\nP π′(x′)\\nx′→x′′\\n\\x10\\nγRπ′(x′)\\nx′→x′′ + γ2V π (x′′)\\n\\x11!\\n⩽\\nX\\nx′∈X\\nP π′(x)\\nx→x′\\n \\nRπ′(x)\\nx→x′ +\\nX\\nx′′∈X\\nP π′(x′)\\nx′→x′′\\n\\x10\\nγRπ′(x′)\\nx′→x′′ + γ2Qπ (x′′, π′ (x′′))\\n\\x11!\\n⩽· · ·\\n⩽\\nX\\nx′∈X\\nP π′(x)\\nx→x′\\n \\nRπ′(x)\\nx→x′ +\\nX', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 193, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='\\x11!\\n⩽· · ·\\n⩽\\nX\\nx′∈X\\nP π′(x)\\nx→x′\\n \\nRπ′(x)\\nx→x′ +\\nX\\nx′′∈X\\nP π′(x′)\\nx′→x′′\\n \\nγRπ′(x′)\\nx′→x′′ +\\nX\\nx′′∈X\\nP π′(x′′)\\nx′′→x′′′\\n\\x10\\nγ2Rπ′(x′′)\\nx′′→x′′′ + · · ·\\n\\x11!!\\n= V π′(x)\\n→_→\\n配套视频教程：https://www.bilibili.com/video/BV1Mh411e7VU\\n←_←', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 193, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='→_→\\n欢迎去各大电商平台选购纸质版南瓜书《机器学习公式详解第2 版》\\n←_←\\n其中，使用了动作改变条件\\nQπ(x, π′(x)) ⩾V π(x)\\n以及状态-动作值函数\\nQπ(x′, π′(x′)) =\\nX\\nx′∈X\\nP π′(x′)\\nx′→x′(Rπ′(x′)\\nx′→x′ + γV π(x′))\\n于是，当前状态的最优值函数为\\nV ∗(x) = V π′(x) ⩾V π(x)\\n16.4\\n免模型学习\\n16.4.1\\n式(16.20) 的解释\\n如果ϵk = 1\\nk，并且其值随k 增大而主角趋于零，则ϵ−贪心是在无限的探索中的极限贪心（Greedy\\nin the Limit with Infinite Exploration，简称GLIE）\\n。\\n16.4.2\\n式(16.23) 的解释\\np(x)\\nq(x) 称为重要性权重（Importance Weight）\\n，其用于修正两个分布的差异。\\n16.4.3\\n式(16.31) 的推导\\n对比公式16.29\\nQπ\\nt+1(x, a) = Qπ\\nt (x, a) +\\n1\\nt + 1(rt+1 −Qπ\\nt (x, a))\\n以及由\\n1\\nt + 1 = α', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 194, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='1\\nt + 1(rt+1 −Qπ\\nt (x, a))\\n以及由\\n1\\nt + 1 = α\\n可知，若下式成立，则公式16.31 成立\\nrt+1 = Ra\\nx→x′ + γQπ\\nt (x′, a′)\\n而rt+1 表示t + 1 步的奖赏，即状态x 变化到x′ 的奖赏加上前面t 步奖赏总和Qπ\\nt (x′, a′) 的γ 折扣，因\\n此这个式子成立。\\n16.5\\n值函数近似\\n16.5.1\\n式(16.33) 的解释\\n古代汉语中“平方”称为“二乘”\\n，此处的最小二乘误差也就是均方误差。\\n16.5.2\\n式(16.34) 的推导\\n−∂Eθ\\n∂θ = −\\n∂Ex∼π\\nh\\n(V π(x) −Vθ(x))2i\\n∂θ\\n将V π(x) −Vθ(x) 看成一个整体，根据链式法则（chain rule）可知\\n−\\n∂Ex∼π\\nh\\n(V π(x) −Vθ(x))2i\\n∂θ\\n= Ex∼π\\n\\x14\\n2 (V π(x) −Vθ(x)) ∂Vθ(x)\\n∂θ\\n\\x15\\n→_→\\n配套视频教程：https://www.bilibili.com/video/BV1Mh411e7VU\\n←_←', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 194, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='→_→\\n欢迎去各大电商平台选购纸质版南瓜书《机器学习公式详解第2 版》\\n←_←\\nVθ(x) 是一个标量，θ 是一个向量，∂Vθ(x)\\n∂θ\\n属于矩阵微积分中的标量对向量求偏导，因此\\n∂Vθ(x)\\n∂θ\\n= ∂θTx\\n∂θ\\n=\\n\"\\n∂θTx\\n∂θ1\\n, ∂θTx\\n∂θ2\\n, · · · , ∂θTx\\n∂θn\\n#T\\n= [x1, x2, · · · , xm]T\\n= x\\n故\\n−∂Eθ\\n∂θ = Ex∼π\\n\\x14\\n2 (V π(x) −Vθ(x)) ∂Vθ(x)\\n∂θ\\n\\x15\\n= Ex∼π [2 (V π(x) −Vθ(x)) x]\\n参考文献\\n[1] 王琦，杨毅远，江季. Easy RL：强化学习教程. 人民邮电出版社, 2022.\\n→_→\\n配套视频教程：https://www.bilibili.com/video/BV1Mh411e7VU\\n←_←', metadata={'source': '../../resources/data/pumpkin_book.pdf', 'file_path': '../../resources/data/pumpkin_book.pdf', 'page': 195, 'total_pages': 196, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20231117152045-00'00'\", 'modDate': '', 'trapped': ''})]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "split_docs = text_splitter.split_documents(pdf_pages)\n",
    "print(f\"切分后的文件数量：{len(split_docs)}\")\n",
    "split_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "切分后的字符数（可以用来大致评估 token 数）：308791\n"
     ]
    }
   ],
   "source": [
    "print(f\"切分后的字符数（可以用来大致评估 token 数）：{sum([len(doc.page_content) for doc in split_docs])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "注：如何对文档进行分割，其实是数据处理中最核心的一步，其往往决定了检索系统的下限。但是，如何选择分割方式，往往具有很强的业务相关性——针对不同的业务、不同的源数据，往往需要设定个性化的文档分割方式。因此，在本章，我们仅简单根据 chunk_size 对文档进行分割。对于有兴趣进一步探索的读者，欢迎阅读我们第三部分的项目示例来参考已有的项目是如何进行文档分割的。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm_universe_2.x",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
